{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsZk/cc0vZOUpF2ezSQByW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym pygame optree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1WIVy1mqLIMj",
        "outputId": "93aee7e2-58a0-404a-951e-c2bd5ace1d9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0 keras==2.12.0 keras-rl2==1.0.5 torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UQ4W3LVnPVAV",
        "outputId": "a1e8cf5a-a325-4631-ac4c-f9331fa7aba5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.11.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.26)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.44.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.7.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3==2.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QLxU3NNDVds7",
        "outputId": "0fabedc9-b21e-445b-e0e2-cbecc01c177c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3==2.3.2 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3==2.3.2)\n",
            "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.3.2) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.3.2) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.3.2) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.3.2) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.3.2) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3==2.3.2) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3==2.3.2) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3==2.3.2) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3==2.3.2) (12.6.20)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.3.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.3.2) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.3.2) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.2) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3==2.3.2) (1.3.0)\n",
            "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "Installing collected packages: gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.0.0a2\n",
            "    Uninstalling gymnasium-1.0.0a2:\n",
            "      Successfully uninstalled gymnasium-1.0.0a2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shimmy 2.0.0 requires gymnasium>=1.0.0a1, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==3.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "DEwBJ5S17Pxa",
        "outputId": "482dc4ce-e4d7-43c3-ae22-63b0cd09f975"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py==3.8.0\n",
            "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.8.0) (1.23.5)\n",
            "Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.11.0\n",
            "    Uninstalling h5py-3.11.0:\n",
            "      Successfully uninstalled h5py-3.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              },
              "id": "797d82e0f49c4016901d198fe920886f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Script"
      ],
      "metadata": {
        "id": "ZkJFbCp7eHvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class ClaimProcessorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(ClaimProcessorEnv, self).__init__()\n",
        "\n",
        "        # Define the action and observation space\n",
        "        self.action_space = spaces.Discrete(8) # 8 possible actions (one for each step in the process)\n",
        "        self.observation_space = spaces.Discrete(8) # 8 states representing each step in the process\n",
        "\n",
        "        # Initial state\n",
        "        self.state = 0\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        self.steps_beyond_done = None\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        reward = 0\n",
        "\n",
        "        # Define the correct sequence of steps\n",
        "        correct_sequence = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "        # Check if action is correct\n",
        "        if action == correct_sequence[self.state]:\n",
        "            reward = 1\n",
        "            self.state += 1\n",
        "            if self.state == len(correct_sequence):\n",
        "                done = True\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "vB0oQeyyJ2aF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test environment"
      ],
      "metadata": {
        "id": "Be-zmhotKBTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from claim_processor_env import ClaimProcessorEnv\n",
        "\n",
        "# Create environment\n",
        "env = ClaimProcessorEnv()\n",
        "\n",
        "# Test the environment\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()  # Random action for testing\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "51lRlSUpKGlu",
        "outputId": "70e7bad1-cc3d-4844-8e01-4af8f8c2fe30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 1, Reward: 1, Done: False\n",
            "State: 1, Reward: -1, Done: False\n",
            "State: 1, Reward: -1, Done: False\n",
            "State: 2, Reward: 1, Done: False\n",
            "State: 2, Reward: -1, Done: False\n",
            "State: 2, Reward: -1, Done: False\n",
            "State: 2, Reward: -1, Done: False\n",
            "State: 3, Reward: 1, Done: False\n",
            "State: 4, Reward: 1, Done: False\n",
            "State: 4, Reward: -1, Done: False\n",
            "State: 4, Reward: -1, Done: False\n",
            "State: 4, Reward: -1, Done: False\n",
            "State: 4, Reward: -1, Done: False\n",
            "State: 4, Reward: -1, Done: False\n",
            "State: 4, Reward: -1, Done: False\n",
            "State: 5, Reward: 1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 5, Reward: -1, Done: False\n",
            "State: 6, Reward: 1, Done: False\n",
            "State: 6, Reward: -1, Done: False\n",
            "State: 6, Reward: -1, Done: False\n",
            "State: 6, Reward: -1, Done: False\n",
            "State: 6, Reward: -1, Done: False\n",
            "State: 6, Reward: -1, Done: False\n",
            "State: 7, Reward: 1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 7, Reward: -1, Done: False\n",
            "State: 8, Reward: 1, Done: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train file"
      ],
      "metadata": {
        "id": "V1bZmJqKKR6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from claim_processor_env import ClaimProcessorEnv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Custom Epsilon Decay Callback\n",
        "from rl.callbacks import Callback\n",
        "\n",
        "class CustomEpsilonDecayCallback(Callback):\n",
        "    def __init__(self, initial_eps=1.0, final_eps=0.1, decay_factor=0.995):\n",
        "        self.initial_eps = initial_eps\n",
        "        self.final_eps = final_eps\n",
        "        self.decay_factor = decay_factor\n",
        "        self.epsilon = initial_eps\n",
        "\n",
        "    def on_step_end(self, episode_step, logs={}):\n",
        "        self.epsilon = max(self.final_eps, self.epsilon * self.decay_factor)\n",
        "        self.model.policy.eps = self.epsilon\n",
        "        return True\n",
        "\n",
        "# Create environment\n",
        "env = ClaimProcessorEnv()\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Build a simple model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(24))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(24))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# Configure and compile the agent\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy(eps=1.0)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "\n",
        "# Create an optimizer instance and pass it to compile\n",
        "optimizer = Adam(learning_rate=1e-3)\n",
        "dqn.compile(optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# Train the agent with custom epsilon decay callback\n",
        "epsilon_decay_callback = CustomEpsilonDecayCallback()\n",
        "dqn.fit(env, nb_steps=100000, visualize=False, verbose=2, callbacks=[epsilon_decay_callback])\n",
        "\n",
        "# Save the trained policy\n",
        "dqn.save_weights('dqn_claim_processor_weights.h5f', overwrite=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5775xU3GTSf",
        "outputId": "7f8a5a53-6ac4-4792-8c83-82a2b578ce51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "    63/100000: episode: 1, duration: 0.197s, episode steps:  63, steps per second: 320, episode reward: -47.000, mean reward: -0.746 [-1.000,  1.000], mean action: 3.143 [0.000, 7.000],  loss: --, mae: --, mean_q: --\n",
            "   423/100000: episode: 2, duration: 0.340s, episode steps: 360, steps per second: 1057, episode reward: -344.000, mean reward: -0.956 [-1.000,  1.000], mean action: 1.858 [0.000, 7.000],  loss: --, mae: --, mean_q: --\n",
            "   843/100000: episode: 3, duration: 0.434s, episode steps: 420, steps per second: 967, episode reward: -404.000, mean reward: -0.962 [-1.000,  1.000], mean action: 1.221 [0.000, 7.000],  loss: --, mae: --, mean_q: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " 56270/100000: episode: 5195, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.864774, mean_q: 4.300598\n",
            " 56280/100000: episode: 5196, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000019, mae: 3.037975, mean_q: 4.492248\n",
            " 56288/100000: episode: 5197, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.883024, mean_q: 4.315826\n",
            " 56297/100000: episode: 5198, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000012, mae: 3.218909, mean_q: 4.707564\n",
            " 56306/100000: episode: 5199, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000009, mae: 2.794639, mean_q: 4.213135\n",
            " 56316/100000: episode: 5200, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000005, mae: 2.951463, mean_q: 4.395010\n",
            " 56326/100000: episode: 5201, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000011, mae: 2.971847, mean_q: 4.420634\n",
            " 56335/100000: episode: 5202, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000009, mae: 2.997374, mean_q: 4.454298\n",
            " 56344/100000: episode: 5203, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 3.171963, mean_q: 4.649586\n",
            " 56353/100000: episode: 5204, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000009, mae: 2.831757, mean_q: 4.255033\n",
            " 56361/100000: episode: 5205, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.966855, mean_q: 4.414080\n",
            " 56370/100000: episode: 5206, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000030, mae: 3.061582, mean_q: 4.520635\n",
            " 56378/100000: episode: 5207, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.921083, mean_q: 4.357575\n",
            " 56386/100000: episode: 5208, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.923764, mean_q: 4.359179\n",
            " 56397/100000: episode: 5209, duration: 0.116s, episode steps:  11, steps per second:  95, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000023, mae: 3.015107, mean_q: 4.471533\n",
            " 56406/100000: episode: 5210, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000032, mae: 2.911239, mean_q: 4.351903\n",
            " 56414/100000: episode: 5211, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.943566, mean_q: 4.384482\n",
            " 56422/100000: episode: 5212, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 3.054900, mean_q: 4.518571\n",
            " 56433/100000: episode: 5213, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.727 [0.000, 7.000],  loss: 0.000051, mae: 3.147704, mean_q: 4.629539\n",
            " 56443/100000: episode: 5214, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000090, mae: 2.986238, mean_q: 4.436035\n",
            " 56451/100000: episode: 5215, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000103, mae: 3.021599, mean_q: 4.476431\n",
            " 56459/100000: episode: 5216, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 2.728935, mean_q: 4.134338\n",
            " 56468/100000: episode: 5217, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000043, mae: 2.958583, mean_q: 4.402891\n",
            " 56476/100000: episode: 5218, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 3.009359, mean_q: 4.460948\n",
            " 56487/100000: episode: 5219, duration: 0.121s, episode steps:  11, steps per second:  91, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000057, mae: 2.950535, mean_q: 4.389551\n",
            " 56498/100000: episode: 5220, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000046, mae: 2.922999, mean_q: 4.364743\n",
            " 56509/100000: episode: 5221, duration: 0.119s, episode steps:  11, steps per second:  92, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.091 [0.000, 7.000],  loss: 0.000027, mae: 3.073332, mean_q: 4.540607\n",
            " 56517/100000: episode: 5222, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 3.050316, mean_q: 4.512463\n",
            " 56528/100000: episode: 5223, duration: 0.119s, episode steps:  11, steps per second:  92, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000036, mae: 2.842921, mean_q: 4.266475\n",
            " 56537/100000: episode: 5224, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000015, mae: 2.795161, mean_q: 4.212993\n",
            " 56547/100000: episode: 5225, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000012, mae: 3.056648, mean_q: 4.517926\n",
            " 56555/100000: episode: 5226, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.828300, mean_q: 4.256820\n",
            " 56565/100000: episode: 5227, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000037, mae: 2.784225, mean_q: 4.204833\n",
            " 56574/100000: episode: 5228, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000014, mae: 3.034388, mean_q: 4.499021\n",
            " 56582/100000: episode: 5229, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.013927, mean_q: 4.465588\n",
            " 56591/100000: episode: 5230, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.804657, mean_q: 4.228207\n",
            " 56600/100000: episode: 5231, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000009, mae: 2.940311, mean_q: 4.378121\n",
            " 56610/100000: episode: 5232, duration: 0.105s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.942920, mean_q: 4.383482\n",
            " 56620/100000: episode: 5233, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000019, mae: 2.975841, mean_q: 4.424046\n",
            " 56629/100000: episode: 5234, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000015, mae: 2.886113, mean_q: 4.316892\n",
            " 56639/100000: episode: 5235, duration: 0.090s, episode steps:  10, steps per second: 112, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000012, mae: 2.962069, mean_q: 4.408104\n",
            " 56647/100000: episode: 5236, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.057351, mean_q: 4.518039\n",
            " 56656/100000: episode: 5237, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000044, mae: 3.003686, mean_q: 4.458302\n",
            " 56664/100000: episode: 5238, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.890865, mean_q: 4.323359\n",
            " 56674/100000: episode: 5239, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000025, mae: 3.071401, mean_q: 4.537105\n",
            " 56684/100000: episode: 5240, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000022, mae: 3.011412, mean_q: 4.466374\n",
            " 56692/100000: episode: 5241, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.813038, mean_q: 4.237416\n",
            " 56704/100000: episode: 5242, duration: 0.141s, episode steps:  12, steps per second:  85, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 2.833 [0.000, 7.000],  loss: 0.000020, mae: 3.010666, mean_q: 4.466035\n",
            " 56712/100000: episode: 5243, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.898749, mean_q: 4.335890\n",
            " 56720/100000: episode: 5244, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.873831, mean_q: 4.305873\n",
            " 56728/100000: episode: 5245, duration: 0.094s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.708237, mean_q: 4.111987\n",
            " 56737/100000: episode: 5246, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 3.106541, mean_q: 4.576870\n",
            " 56745/100000: episode: 5247, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.936664, mean_q: 4.383373\n",
            " 56753/100000: episode: 5248, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.911143, mean_q: 4.348589\n",
            " 56762/100000: episode: 5249, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000040, mae: 2.797378, mean_q: 4.217161\n",
            " 56770/100000: episode: 5250, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 3.130901, mean_q: 4.604162\n",
            " 56780/100000: episode: 5251, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000017, mae: 2.869573, mean_q: 4.306243\n",
            " 56789/100000: episode: 5252, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000025, mae: 2.966122, mean_q: 4.411124\n",
            " 56797/100000: episode: 5253, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 2.885355, mean_q: 4.311356\n",
            " 56806/100000: episode: 5254, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000088, mae: 3.003656, mean_q: 4.453698\n",
            " 56814/100000: episode: 5255, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000076, mae: 2.789459, mean_q: 4.209692\n",
            " 56823/100000: episode: 5256, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000068, mae: 2.913728, mean_q: 4.353315\n",
            " 56832/100000: episode: 5257, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000031, mae: 2.823057, mean_q: 4.250521\n",
            " 56841/100000: episode: 5258, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000039, mae: 2.929018, mean_q: 4.367158\n",
            " 56850/100000: episode: 5259, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000063, mae: 2.908031, mean_q: 4.337732\n",
            " 56858/100000: episode: 5260, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.875473, mean_q: 4.305726\n",
            " 56866/100000: episode: 5261, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.834780, mean_q: 4.260567\n",
            " 56874/100000: episode: 5262, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.923963, mean_q: 4.363513\n",
            " 56882/100000: episode: 5263, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000134, mae: 3.063538, mean_q: 4.520425\n",
            " 56890/100000: episode: 5264, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000096, mae: 2.859439, mean_q: 4.286238\n",
            " 56898/100000: episode: 5265, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 3.017235, mean_q: 4.468124\n",
            " 56907/100000: episode: 5266, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000042, mae: 3.117794, mean_q: 4.586961\n",
            " 56915/100000: episode: 5267, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.018850, mean_q: 4.471078\n",
            " 56924/100000: episode: 5268, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 3.182662, mean_q: 4.661734\n",
            " 56933/100000: episode: 5269, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000048, mae: 2.995694, mean_q: 4.445508\n",
            " 56942/100000: episode: 5270, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000019, mae: 2.972459, mean_q: 4.418674\n",
            " 56950/100000: episode: 5271, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 3.076944, mean_q: 4.536595\n",
            " 56958/100000: episode: 5272, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 3.083044, mean_q: 4.551271\n",
            " 56968/100000: episode: 5273, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000031, mae: 2.822817, mean_q: 4.250873\n",
            " 56977/100000: episode: 5274, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000017, mae: 2.840554, mean_q: 4.266733\n",
            " 56985/100000: episode: 5275, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.070622, mean_q: 4.534039\n",
            " 56996/100000: episode: 5276, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.545 [0.000, 7.000],  loss: 0.000067, mae: 2.962413, mean_q: 4.404088\n",
            " 57004/100000: episode: 5277, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.811061, mean_q: 4.236241\n",
            " 57013/100000: episode: 5278, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000018, mae: 3.104666, mean_q: 4.569406\n",
            " 57022/100000: episode: 5279, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000036, mae: 2.927431, mean_q: 4.366858\n",
            " 57031/100000: episode: 5280, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000069, mae: 3.230677, mean_q: 4.720748\n",
            " 57039/100000: episode: 5281, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 2.838323, mean_q: 4.263702\n",
            " 57048/100000: episode: 5282, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000030, mae: 3.006782, mean_q: 4.464655\n",
            " 57057/100000: episode: 5283, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000072, mae: 2.974570, mean_q: 4.422843\n",
            " 57065/100000: episode: 5284, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.960975, mean_q: 4.391790\n",
            " 57073/100000: episode: 5285, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000289, mae: 2.988038, mean_q: 4.422604\n",
            " 57081/100000: episode: 5286, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000206, mae: 2.921001, mean_q: 4.353045\n",
            " 57089/100000: episode: 5287, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000379, mae: 3.219560, mean_q: 4.714828\n",
            " 57097/100000: episode: 5288, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000554, mae: 2.949977, mean_q: 4.398268\n",
            " 57106/100000: episode: 5289, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000280, mae: 2.988996, mean_q: 4.430609\n",
            " 57117/100000: episode: 5290, duration: 0.124s, episode steps:  11, steps per second:  88, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000205, mae: 2.773035, mean_q: 4.190696\n",
            " 57129/100000: episode: 5291, duration: 0.134s, episode steps:  12, steps per second:  90, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 2.500 [0.000, 7.000],  loss: 0.000496, mae: 2.872579, mean_q: 4.305286\n",
            " 57139/100000: episode: 5292, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000505, mae: 3.135651, mean_q: 4.592112\n",
            " 57149/100000: episode: 5293, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000702, mae: 2.861790, mean_q: 4.283469\n",
            " 57158/100000: episode: 5294, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000334, mae: 2.922157, mean_q: 4.372017\n",
            " 57167/100000: episode: 5295, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000253, mae: 2.966648, mean_q: 4.416907\n",
            " 57176/100000: episode: 5296, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000175, mae: 2.856987, mean_q: 4.270615\n",
            " 57188/100000: episode: 5297, duration: 0.131s, episode steps:  12, steps per second:  91, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000197, mae: 2.854236, mean_q: 4.259542\n",
            " 57198/100000: episode: 5298, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000110, mae: 2.869360, mean_q: 4.290073\n",
            " 57208/100000: episode: 5299, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000098, mae: 2.876360, mean_q: 4.299177\n",
            " 57217/100000: episode: 5300, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000149, mae: 2.989370, mean_q: 4.443136\n",
            " 57225/100000: episode: 5301, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000120, mae: 3.107856, mean_q: 4.576491\n",
            " 57234/100000: episode: 5302, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000144, mae: 2.961019, mean_q: 4.401103\n",
            " 57243/100000: episode: 5303, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000098, mae: 3.105999, mean_q: 4.573039\n",
            " 57251/100000: episode: 5304, duration: 0.126s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.976604, mean_q: 4.426036\n",
            " 57260/100000: episode: 5305, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000100, mae: 2.856303, mean_q: 4.284318\n",
            " 57268/100000: episode: 5306, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.038030, mean_q: 4.497043\n",
            " 57276/100000: episode: 5307, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.050663, mean_q: 4.511043\n",
            " 57285/100000: episode: 5308, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000026, mae: 2.941451, mean_q: 4.384489\n",
            " 57293/100000: episode: 5309, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.967380, mean_q: 4.417405\n",
            " 57302/100000: episode: 5310, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000022, mae: 3.161083, mean_q: 4.637448\n",
            " 57310/100000: episode: 5311, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.137855, mean_q: 4.616431\n",
            " 57319/100000: episode: 5312, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000007, mae: 3.073372, mean_q: 4.540437\n",
            " 57327/100000: episode: 5313, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.879312, mean_q: 4.309135\n",
            " 57336/100000: episode: 5314, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000015, mae: 2.710538, mean_q: 4.122544\n",
            " 57344/100000: episode: 5315, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.767974, mean_q: 4.184462\n",
            " 57352/100000: episode: 5316, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.964695, mean_q: 4.416866\n",
            " 57361/100000: episode: 5317, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 2.964735, mean_q: 4.408614\n",
            " 57369/100000: episode: 5318, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.846850, mean_q: 4.276484\n",
            " 57377/100000: episode: 5319, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.964487, mean_q: 4.417018\n",
            " 57385/100000: episode: 5320, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.914811, mean_q: 4.356783\n",
            " 57394/100000: episode: 5321, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000008, mae: 2.795703, mean_q: 4.219822\n",
            " 57405/100000: episode: 5322, duration: 0.171s, episode steps:  11, steps per second:  64, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000008, mae: 3.067742, mean_q: 4.529727\n",
            " 57413/100000: episode: 5323, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.011176, mean_q: 4.463648\n",
            " 57421/100000: episode: 5324, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.037267, mean_q: 4.496692\n",
            " 57431/100000: episode: 5325, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.006226, mean_q: 4.455297\n",
            " 57439/100000: episode: 5326, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.797732, mean_q: 4.223620\n",
            " 57448/100000: episode: 5327, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000033, mae: 2.936152, mean_q: 4.376081\n",
            " 57457/100000: episode: 5328, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.928403, mean_q: 4.375344\n",
            " 57466/100000: episode: 5329, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000020, mae: 3.040193, mean_q: 4.497722\n",
            " 57474/100000: episode: 5330, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 3.013329, mean_q: 4.461561\n",
            " 57482/100000: episode: 5331, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.995018, mean_q: 4.457673\n",
            " 57490/100000: episode: 5332, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 2.789305, mean_q: 4.210290\n",
            " 57498/100000: episode: 5333, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 2.972969, mean_q: 4.413093\n",
            " 57506/100000: episode: 5334, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 2.774634, mean_q: 4.193945\n",
            " 57514/100000: episode: 5335, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.985855, mean_q: 4.442747\n",
            " 57523/100000: episode: 5336, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000041, mae: 2.981618, mean_q: 4.434059\n",
            " 57534/100000: episode: 5337, duration: 0.121s, episode steps:  11, steps per second:  91, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000039, mae: 2.992159, mean_q: 4.440892\n",
            " 57543/100000: episode: 5338, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000032, mae: 3.039108, mean_q: 4.502060\n",
            " 57552/100000: episode: 5339, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000024, mae: 2.933608, mean_q: 4.376513\n",
            " 57560/100000: episode: 5340, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.932269, mean_q: 4.376546\n",
            " 57569/100000: episode: 5341, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000043, mae: 2.991312, mean_q: 4.443330\n",
            " 57577/100000: episode: 5342, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.934641, mean_q: 4.380345\n",
            " 57586/100000: episode: 5343, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000034, mae: 3.034342, mean_q: 4.499367\n",
            " 57595/100000: episode: 5344, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000023, mae: 2.851619, mean_q: 4.283612\n",
            " 57603/100000: episode: 5345, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.821710, mean_q: 4.248932\n",
            " 57611/100000: episode: 5346, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.865036, mean_q: 4.291711\n",
            " 57620/100000: episode: 5347, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000026, mae: 2.889546, mean_q: 4.326345\n",
            " 57631/100000: episode: 5348, duration: 0.119s, episode steps:  11, steps per second:  93, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000018, mae: 2.974939, mean_q: 4.418613\n",
            " 57639/100000: episode: 5349, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 3.008347, mean_q: 4.465290\n",
            " 57648/100000: episode: 5350, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000022, mae: 3.015585, mean_q: 4.475742\n",
            " 57657/100000: episode: 5351, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000021, mae: 3.001571, mean_q: 4.460003\n",
            " 57666/100000: episode: 5352, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000040, mae: 2.749859, mean_q: 4.162843\n",
            " 57675/100000: episode: 5353, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000037, mae: 2.727187, mean_q: 4.134771\n",
            " 57684/100000: episode: 5354, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000018, mae: 3.212399, mean_q: 4.701840\n",
            " 57693/100000: episode: 5355, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000020, mae: 2.884593, mean_q: 4.314735\n",
            " 57702/100000: episode: 5356, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000061, mae: 3.034377, mean_q: 4.485281\n",
            " 57711/100000: episode: 5357, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000049, mae: 3.000381, mean_q: 4.453154\n",
            " 57719/100000: episode: 5358, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000125, mae: 2.925308, mean_q: 4.370807\n",
            " 57728/100000: episode: 5359, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000091, mae: 3.126300, mean_q: 4.589087\n",
            " 57737/100000: episode: 5360, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000096, mae: 2.882315, mean_q: 4.305429\n",
            " 57746/100000: episode: 5361, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000080, mae: 2.798990, mean_q: 4.209748\n",
            " 57754/100000: episode: 5362, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000063, mae: 3.139898, mean_q: 4.608722\n",
            " 57762/100000: episode: 5363, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 3.148406, mean_q: 4.630440\n",
            " 57771/100000: episode: 5364, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000077, mae: 2.910404, mean_q: 4.356261\n",
            " 57781/100000: episode: 5365, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000045, mae: 3.205657, mean_q: 4.689670\n",
            " 57789/100000: episode: 5366, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.785558, mean_q: 4.201900\n",
            " 57798/100000: episode: 5367, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000028, mae: 2.828612, mean_q: 4.250736\n",
            " 57808/100000: episode: 5368, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000027, mae: 2.922973, mean_q: 4.362294\n",
            " 57817/100000: episode: 5369, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000030, mae: 3.150451, mean_q: 4.628518\n",
            " 57825/100000: episode: 5370, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.033001, mean_q: 4.480168\n",
            " 57834/100000: episode: 5371, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000020, mae: 3.003078, mean_q: 4.462124\n",
            " 57842/100000: episode: 5372, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.032464, mean_q: 4.496737\n",
            " 57850/100000: episode: 5373, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 2.915369, mean_q: 4.350687\n",
            " 57858/100000: episode: 5374, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 3.068995, mean_q: 4.525151\n",
            " 57868/100000: episode: 5375, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000061, mae: 2.911522, mean_q: 4.350222\n",
            " 57877/100000: episode: 5376, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000102, mae: 2.695147, mean_q: 4.099921\n",
            " 57885/100000: episode: 5377, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 2.796223, mean_q: 4.211223\n",
            " 57893/100000: episode: 5378, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000105, mae: 3.095707, mean_q: 4.554984\n",
            " 57902/100000: episode: 5379, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000035, mae: 2.915845, mean_q: 4.358504\n",
            " 57912/100000: episode: 5380, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000025, mae: 3.096181, mean_q: 4.571122\n",
            " 57920/100000: episode: 5381, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 3.014672, mean_q: 4.476454\n",
            " 57930/100000: episode: 5382, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000023, mae: 2.940742, mean_q: 4.379369\n",
            " 57938/100000: episode: 5383, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.107968, mean_q: 4.574800\n",
            " 57946/100000: episode: 5384, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.836174, mean_q: 4.266589\n",
            " 57954/100000: episode: 5385, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.920259, mean_q: 4.362699\n",
            " 57962/100000: episode: 5386, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.933532, mean_q: 4.379985\n",
            " 57971/100000: episode: 5387, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.946397, mean_q: 4.391745\n",
            " 57980/100000: episode: 5388, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000013, mae: 2.877753, mean_q: 4.308387\n",
            " 57989/100000: episode: 5389, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 2.948587, mean_q: 4.393727\n",
            " 57997/100000: episode: 5390, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.840630, mean_q: 4.268847\n",
            " 58007/100000: episode: 5391, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000005, mae: 2.835276, mean_q: 4.263396\n",
            " 58018/100000: episode: 5392, duration: 0.107s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000003, mae: 2.941526, mean_q: 4.391512\n",
            " 58028/100000: episode: 5393, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 2.906363, mean_q: 4.345682\n",
            " 58037/100000: episode: 5394, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000009, mae: 2.954953, mean_q: 4.395778\n",
            " 58046/100000: episode: 5395, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000011, mae: 2.923563, mean_q: 4.374094\n",
            " 58055/100000: episode: 5396, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000037, mae: 3.037122, mean_q: 4.494710\n",
            " 58064/100000: episode: 5397, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000032, mae: 3.029100, mean_q: 4.490998\n",
            " 58072/100000: episode: 5398, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.764728, mean_q: 4.190220\n",
            " 58082/100000: episode: 5399, duration: 0.148s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000017, mae: 2.923691, mean_q: 4.369914\n",
            " 58091/100000: episode: 5400, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000027, mae: 2.861957, mean_q: 4.293893\n",
            " 58099/100000: episode: 5401, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000076, mae: 3.000118, mean_q: 4.452470\n",
            " 58108/100000: episode: 5402, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000161, mae: 2.924727, mean_q: 4.353386\n",
            " 58118/100000: episode: 5403, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000314, mae: 2.854913, mean_q: 4.288400\n",
            " 58127/100000: episode: 5404, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000209, mae: 3.236075, mean_q: 4.723172\n",
            " 58137/100000: episode: 5405, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000265, mae: 2.965098, mean_q: 4.408034\n",
            " 58147/100000: episode: 5406, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000268, mae: 2.857784, mean_q: 4.279075\n",
            " 58155/100000: episode: 5407, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000173, mae: 2.991955, mean_q: 4.442604\n",
            " 58163/100000: episode: 5408, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000150, mae: 3.037122, mean_q: 4.489527\n",
            " 58173/100000: episode: 5409, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 3.174305, mean_q: 4.656116\n",
            " 58181/100000: episode: 5410, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000091, mae: 2.931439, mean_q: 4.367331\n",
            " 58189/100000: episode: 5411, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000113, mae: 2.705079, mean_q: 4.113133\n",
            " 58197/100000: episode: 5412, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000067, mae: 3.136034, mean_q: 4.608900\n",
            " 58207/100000: episode: 5413, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000082, mae: 2.815566, mean_q: 4.238357\n",
            " 58215/100000: episode: 5414, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000095, mae: 2.995856, mean_q: 4.444494\n",
            " 58223/100000: episode: 5415, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 2.987117, mean_q: 4.443954\n",
            " 58232/100000: episode: 5416, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000056, mae: 2.861673, mean_q: 4.291584\n",
            " 58240/100000: episode: 5417, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.755127, mean_q: 4.168334\n",
            " 58248/100000: episode: 5418, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.963595, mean_q: 4.413543\n",
            " 58256/100000: episode: 5419, duration: 0.101s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 2.901611, mean_q: 4.336391\n",
            " 58264/100000: episode: 5420, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 3.040562, mean_q: 4.506571\n",
            " 58272/100000: episode: 5421, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 2.947241, mean_q: 4.387099\n",
            " 58282/100000: episode: 5422, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.900075, mean_q: 4.339404\n",
            " 58290/100000: episode: 5423, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.998175, mean_q: 4.451294\n",
            " 58303/100000: episode: 5424, duration: 0.136s, episode steps:  13, steps per second:  96, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 2.846 [0.000, 7.000],  loss: 0.000052, mae: 3.121867, mean_q: 4.593541\n",
            " 58313/100000: episode: 5425, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000061, mae: 2.932833, mean_q: 4.376894\n",
            " 58323/100000: episode: 5426, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000069, mae: 2.867148, mean_q: 4.292900\n",
            " 58332/100000: episode: 5427, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000040, mae: 2.938082, mean_q: 4.380609\n",
            " 58340/100000: episode: 5428, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.102077, mean_q: 4.571706\n",
            " 58349/100000: episode: 5429, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000018, mae: 3.166252, mean_q: 4.644582\n",
            " 58357/100000: episode: 5430, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.748139, mean_q: 4.163047\n",
            " 58365/100000: episode: 5431, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.005241, mean_q: 4.465000\n",
            " 58377/100000: episode: 5432, duration: 0.174s, episode steps:  12, steps per second:  69, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.833 [0.000, 7.000],  loss: 0.000052, mae: 3.025541, mean_q: 4.482185\n",
            " 58385/100000: episode: 5433, duration: 0.139s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 3.042380, mean_q: 4.507087\n",
            " 58393/100000: episode: 5434, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.986340, mean_q: 4.442740\n",
            " 58402/100000: episode: 5435, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000022, mae: 2.917548, mean_q: 4.359325\n",
            " 58410/100000: episode: 5436, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.788198, mean_q: 4.205942\n",
            " 58418/100000: episode: 5437, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.911072, mean_q: 4.350343\n",
            " 58426/100000: episode: 5438, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 2.977213, mean_q: 4.431480\n",
            " 58434/100000: episode: 5439, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.915879, mean_q: 4.355713\n",
            " 58443/100000: episode: 5440, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000041, mae: 2.938039, mean_q: 4.371819\n",
            " 58455/100000: episode: 5441, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.417 [0.000, 7.000],  loss: 0.000031, mae: 2.758597, mean_q: 4.174634\n",
            " 58463/100000: episode: 5442, duration: 0.168s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000074, mae: 2.928770, mean_q: 4.371011\n",
            " 58471/100000: episode: 5443, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000252, mae: 2.824136, mean_q: 4.250157\n",
            " 58479/100000: episode: 5444, duration: 0.120s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000086, mae: 2.904132, mean_q: 4.340643\n",
            " 58488/100000: episode: 5445, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000088, mae: 2.834778, mean_q: 4.263213\n",
            " 58496/100000: episode: 5446, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 3.043013, mean_q: 4.509059\n",
            " 58504/100000: episode: 5447, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.941276, mean_q: 4.386342\n",
            " 58513/100000: episode: 5448, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000050, mae: 3.004250, mean_q: 4.459877\n",
            " 58521/100000: episode: 5449, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 3.039712, mean_q: 4.504561\n",
            " 58530/100000: episode: 5450, duration: 0.191s, episode steps:   9, steps per second:  47, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000056, mae: 2.824754, mean_q: 4.247330\n",
            " 58538/100000: episode: 5451, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000091, mae: 2.970379, mean_q: 4.421601\n",
            " 58547/100000: episode: 5452, duration: 0.185s, episode steps:   9, steps per second:  49, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000088, mae: 2.930055, mean_q: 4.371043\n",
            " 58556/100000: episode: 5453, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000139, mae: 2.759971, mean_q: 4.176882\n",
            " 58566/100000: episode: 5454, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000100, mae: 2.761423, mean_q: 4.180743\n",
            " 58574/100000: episode: 5455, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.833390, mean_q: 4.256778\n",
            " 58583/100000: episode: 5456, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000041, mae: 3.074485, mean_q: 4.536613\n",
            " 58592/100000: episode: 5457, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000058, mae: 2.930403, mean_q: 4.377615\n",
            " 58605/100000: episode: 5458, duration: 0.152s, episode steps:  13, steps per second:  86, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 4.077 [0.000, 7.000],  loss: 0.000056, mae: 2.917451, mean_q: 4.356751\n",
            " 58615/100000: episode: 5459, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000044, mae: 2.959095, mean_q: 4.406374\n",
            " 58624/100000: episode: 5460, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000051, mae: 2.806201, mean_q: 4.227281\n",
            " 58632/100000: episode: 5461, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 3.009832, mean_q: 4.468344\n",
            " 58640/100000: episode: 5462, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000101, mae: 2.986791, mean_q: 4.432047\n",
            " 58649/100000: episode: 5463, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000038, mae: 3.055741, mean_q: 4.518312\n",
            " 58657/100000: episode: 5464, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.879443, mean_q: 4.317716\n",
            " 58665/100000: episode: 5465, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.011783, mean_q: 4.466072\n",
            " 58674/100000: episode: 5466, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000014, mae: 2.937211, mean_q: 4.379511\n",
            " 58682/100000: episode: 5467, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.826191, mean_q: 4.249968\n",
            " 58690/100000: episode: 5468, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.789598, mean_q: 4.209995\n",
            " 58698/100000: episode: 5469, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.871950, mean_q: 4.307006\n",
            " 58707/100000: episode: 5470, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000014, mae: 2.808874, mean_q: 4.240543\n",
            " 58715/100000: episode: 5471, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.069632, mean_q: 4.532431\n",
            " 58724/100000: episode: 5472, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.821727, mean_q: 4.246147\n",
            " 58734/100000: episode: 5473, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000005, mae: 3.207831, mean_q: 4.695004\n",
            " 58743/100000: episode: 5474, duration: 0.101s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 3.069385, mean_q: 4.534395\n",
            " 58751/100000: episode: 5475, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.802579, mean_q: 4.228075\n",
            " 58759/100000: episode: 5476, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.095725, mean_q: 4.564724\n",
            " 58767/100000: episode: 5477, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.170609, mean_q: 4.650917\n",
            " 58776/100000: episode: 5478, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 3.063450, mean_q: 4.526110\n",
            " 58784/100000: episode: 5479, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.866718, mean_q: 4.304996\n",
            " 58793/100000: episode: 5480, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.868231, mean_q: 4.302559\n",
            " 58804/100000: episode: 5481, duration: 0.120s, episode steps:  11, steps per second:  92, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000002, mae: 2.825563, mean_q: 4.255975\n",
            " 58813/100000: episode: 5482, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.869859, mean_q: 4.306982\n",
            " 58821/100000: episode: 5483, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.754259, mean_q: 4.169056\n",
            " 58830/100000: episode: 5484, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 3.008147, mean_q: 4.465051\n",
            " 58838/100000: episode: 5485, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.877143, mean_q: 4.315586\n",
            " 58846/100000: episode: 5486, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.924419, mean_q: 4.367019\n",
            " 58854/100000: episode: 5487, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.763521, mean_q: 4.185326\n",
            " 58863/100000: episode: 5488, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 2.955276, mean_q: 4.401331\n",
            " 58872/100000: episode: 5489, duration: 0.090s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.990975, mean_q: 4.444216\n",
            " 58881/100000: episode: 5490, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000004, mae: 2.788826, mean_q: 4.209075\n",
            " 58890/100000: episode: 5491, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 3.125247, mean_q: 4.603421\n",
            " 58899/100000: episode: 5492, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.948763, mean_q: 4.396476\n",
            " 58907/100000: episode: 5493, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.030224, mean_q: 4.487859\n",
            " 58917/100000: episode: 5494, duration: 0.106s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000004, mae: 3.045051, mean_q: 4.508183\n",
            " 58926/100000: episode: 5495, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000009, mae: 3.138150, mean_q: 4.615074\n",
            " 58934/100000: episode: 5496, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.873605, mean_q: 4.305109\n",
            " 58943/100000: episode: 5497, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.940017, mean_q: 4.387978\n",
            " 58952/100000: episode: 5498, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 2.973150, mean_q: 4.425997\n",
            " 58961/100000: episode: 5499, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 2.857243, mean_q: 4.286630\n",
            " 58969/100000: episode: 5500, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.081588, mean_q: 4.553964\n",
            " 58979/100000: episode: 5501, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000010, mae: 2.952798, mean_q: 4.401330\n",
            " 58987/100000: episode: 5502, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.967463, mean_q: 4.420631\n",
            " 58995/100000: episode: 5503, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.936804, mean_q: 4.382668\n",
            " 59003/100000: episode: 5504, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.149684, mean_q: 4.628029\n",
            " 59011/100000: episode: 5505, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.915462, mean_q: 4.351234\n",
            " 59019/100000: episode: 5506, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.982499, mean_q: 4.438413\n",
            " 59029/100000: episode: 5507, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000011, mae: 2.969239, mean_q: 4.419380\n",
            " 59037/100000: episode: 5508, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.020612, mean_q: 4.478930\n",
            " 59047/100000: episode: 5509, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000022, mae: 2.773535, mean_q: 4.196067\n",
            " 59056/100000: episode: 5510, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000021, mae: 2.971515, mean_q: 4.418683\n",
            " 59065/100000: episode: 5511, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000046, mae: 2.903232, mean_q: 4.333696\n",
            " 59074/100000: episode: 5512, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000056, mae: 3.156585, mean_q: 4.637207\n",
            " 59082/100000: episode: 5513, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.975511, mean_q: 4.420467\n",
            " 59090/100000: episode: 5514, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000143, mae: 2.989007, mean_q: 4.429371\n",
            " 59098/100000: episode: 5515, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000115, mae: 3.191299, mean_q: 4.681111\n",
            " 59106/100000: episode: 5516, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000415, mae: 2.868185, mean_q: 4.299126\n",
            " 59115/100000: episode: 5517, duration: 0.079s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000185, mae: 3.003022, mean_q: 4.458517\n",
            " 59126/100000: episode: 5518, duration: 0.117s, episode steps:  11, steps per second:  94, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000110, mae: 2.852093, mean_q: 4.272102\n",
            " 59137/100000: episode: 5519, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.273 [0.000, 7.000],  loss: 0.000113, mae: 3.002921, mean_q: 4.448257\n",
            " 59146/100000: episode: 5520, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000200, mae: 2.813409, mean_q: 4.230675\n",
            " 59154/100000: episode: 5521, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000243, mae: 3.033770, mean_q: 4.491758\n",
            " 59163/100000: episode: 5522, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000182, mae: 2.938058, mean_q: 4.388634\n",
            " 59171/100000: episode: 5523, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000207, mae: 2.838951, mean_q: 4.260806\n",
            " 59180/100000: episode: 5524, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000222, mae: 3.105637, mean_q: 4.562418\n",
            " 59188/100000: episode: 5525, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000170, mae: 2.852894, mean_q: 4.287710\n",
            " 59196/100000: episode: 5526, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000244, mae: 3.094163, mean_q: 4.572569\n",
            " 59205/100000: episode: 5527, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000398, mae: 2.887506, mean_q: 4.310137\n",
            " 59214/100000: episode: 5528, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000219, mae: 2.963086, mean_q: 4.399710\n",
            " 59223/100000: episode: 5529, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000206, mae: 2.764658, mean_q: 4.174652\n",
            " 59232/100000: episode: 5530, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000158, mae: 3.045606, mean_q: 4.502069\n",
            " 59243/100000: episode: 5531, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000070, mae: 3.210746, mean_q: 4.698725\n",
            " 59251/100000: episode: 5532, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000112, mae: 2.908828, mean_q: 4.356545\n",
            " 59259/100000: episode: 5533, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000095, mae: 3.104293, mean_q: 4.588380\n",
            " 59268/100000: episode: 5534, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000263, mae: 2.819842, mean_q: 4.233896\n",
            " 59277/100000: episode: 5535, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000278, mae: 2.936651, mean_q: 4.364931\n",
            " 59286/100000: episode: 5536, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000195, mae: 3.015064, mean_q: 4.471552\n",
            " 59297/100000: episode: 5537, duration: 0.107s, episode steps:  11, steps per second: 103, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000121, mae: 2.949651, mean_q: 4.396837\n",
            " 59305/100000: episode: 5538, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000133, mae: 2.819192, mean_q: 4.244041\n",
            " 59314/100000: episode: 5539, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000093, mae: 2.862327, mean_q: 4.294055\n",
            " 59323/100000: episode: 5540, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000063, mae: 2.976478, mean_q: 4.431998\n",
            " 59331/100000: episode: 5541, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000098, mae: 2.900623, mean_q: 4.343065\n",
            " 59339/100000: episode: 5542, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000088, mae: 2.994683, mean_q: 4.447235\n",
            " 59347/100000: episode: 5543, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 3.059936, mean_q: 4.522673\n",
            " 59356/100000: episode: 5544, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000110, mae: 2.883427, mean_q: 4.321459\n",
            " 59365/100000: episode: 5545, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000060, mae: 3.106294, mean_q: 4.579855\n",
            " 59374/100000: episode: 5546, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000029, mae: 2.767903, mean_q: 4.182097\n",
            " 59384/100000: episode: 5547, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000021, mae: 2.918179, mean_q: 4.360663\n",
            " 59392/100000: episode: 5548, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.978550, mean_q: 4.432192\n",
            " 59401/100000: episode: 5549, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000014, mae: 2.863508, mean_q: 4.301363\n",
            " 59409/100000: episode: 5550, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.048599, mean_q: 4.514300\n",
            " 59420/100000: episode: 5551, duration: 0.126s, episode steps:  11, steps per second:  87, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000012, mae: 3.016872, mean_q: 4.475522\n",
            " 59430/100000: episode: 5552, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000005, mae: 2.963626, mean_q: 4.413059\n",
            " 59439/100000: episode: 5553, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000008, mae: 2.805394, mean_q: 4.224629\n",
            " 59449/100000: episode: 5554, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.236851, mean_q: 4.737013\n",
            " 59457/100000: episode: 5555, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.776702, mean_q: 4.200420\n",
            " 59466/100000: episode: 5556, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 3.003112, mean_q: 4.458993\n",
            " 59474/100000: episode: 5557, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.840107, mean_q: 4.271971\n",
            " 59483/100000: episode: 5558, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 2.996587, mean_q: 4.454412\n",
            " 59491/100000: episode: 5559, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.988200, mean_q: 4.444901\n",
            " 59499/100000: episode: 5560, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.922364, mean_q: 4.370128\n",
            " 59507/100000: episode: 5561, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.752927, mean_q: 4.169989\n",
            " 59515/100000: episode: 5562, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.901767, mean_q: 4.339797\n",
            " 59524/100000: episode: 5563, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 2.923677, mean_q: 4.362407\n",
            " 59534/100000: episode: 5564, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000008, mae: 2.940366, mean_q: 4.383423\n",
            " 59542/100000: episode: 5565, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.989925, mean_q: 4.443448\n",
            " 59551/100000: episode: 5566, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 2.939796, mean_q: 4.388090\n",
            " 59560/100000: episode: 5567, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000017, mae: 2.817325, mean_q: 4.245318\n",
            " 59568/100000: episode: 5568, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.165220, mean_q: 4.652129\n",
            " 59579/100000: episode: 5569, duration: 0.154s, episode steps:  11, steps per second:  72, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000015, mae: 2.979722, mean_q: 4.434783\n",
            " 59588/100000: episode: 5570, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000024, mae: 2.842227, mean_q: 4.279019\n",
            " 59596/100000: episode: 5571, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 3.028752, mean_q: 4.489718\n",
            " 59605/100000: episode: 5572, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000027, mae: 2.938057, mean_q: 4.378679\n",
            " 59613/100000: episode: 5573, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.976160, mean_q: 4.429026\n",
            " 59621/100000: episode: 5574, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 2.933380, mean_q: 4.371766\n",
            " 59629/100000: episode: 5575, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000136, mae: 3.078675, mean_q: 4.536696\n",
            " 59637/100000: episode: 5576, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000100, mae: 3.187530, mean_q: 4.672852\n",
            " 59645/100000: episode: 5577, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000099, mae: 2.759506, mean_q: 4.182978\n",
            " 59653/100000: episode: 5578, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000237, mae: 3.139219, mean_q: 4.608140\n",
            " 59661/100000: episode: 5579, duration: 0.151s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000200, mae: 2.740784, mean_q: 4.142872\n",
            " 59670/100000: episode: 5580, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000113, mae: 2.825828, mean_q: 4.258348\n",
            " 59679/100000: episode: 5581, duration: 0.184s, episode steps:   9, steps per second:  49, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000050, mae: 2.714239, mean_q: 4.131604\n",
            " 59689/100000: episode: 5582, duration: 0.149s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000025, mae: 2.930629, mean_q: 4.382463\n",
            " 59697/100000: episode: 5583, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.919405, mean_q: 4.365985\n",
            " 59706/100000: episode: 5584, duration: 0.151s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000014, mae: 3.000397, mean_q: 4.452970\n",
            " 59715/100000: episode: 5585, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000019, mae: 2.883637, mean_q: 4.318751\n",
            " 59723/100000: episode: 5586, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.854266, mean_q: 4.292380\n",
            " 59731/100000: episode: 5587, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.146300, mean_q: 4.625311\n",
            " 59739/100000: episode: 5588, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.887673, mean_q: 4.322142\n",
            " 59747/100000: episode: 5589, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 2.959208, mean_q: 4.409598\n",
            " 59755/100000: episode: 5590, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 3.007690, mean_q: 4.460925\n",
            " 59764/100000: episode: 5591, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 2.964592, mean_q: 4.414455\n",
            " 59772/100000: episode: 5592, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.023283, mean_q: 4.477653\n",
            " 59780/100000: episode: 5593, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.924763, mean_q: 4.369050\n",
            " 59789/100000: episode: 5594, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000025, mae: 3.019357, mean_q: 4.484605\n",
            " 59797/100000: episode: 5595, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.856297, mean_q: 4.295034\n",
            " 59807/100000: episode: 5596, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.027126, mean_q: 4.485562\n",
            " 59816/100000: episode: 5597, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 2.875382, mean_q: 4.314013\n",
            " 59824/100000: episode: 5598, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.912579, mean_q: 4.356460\n",
            " 59832/100000: episode: 5599, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.025692, mean_q: 4.487366\n",
            " 59841/100000: episode: 5600, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000005, mae: 2.874534, mean_q: 4.312356\n",
            " 59850/100000: episode: 5601, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 2.906801, mean_q: 4.344430\n",
            " 59858/100000: episode: 5602, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.170084, mean_q: 4.652925\n",
            " 59868/100000: episode: 5603, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000005, mae: 2.903635, mean_q: 4.348850\n",
            " 59878/100000: episode: 5604, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000004, mae: 3.162148, mean_q: 4.645180\n",
            " 59886/100000: episode: 5605, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.035026, mean_q: 4.501018\n",
            " 59895/100000: episode: 5606, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.853784, mean_q: 4.291633\n",
            " 59905/100000: episode: 5607, duration: 0.196s, episode steps:  10, steps per second:  51, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000006, mae: 3.023353, mean_q: 4.484840\n",
            " 59913/100000: episode: 5608, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.646730, mean_q: 4.050013\n",
            " 59921/100000: episode: 5609, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.777476, mean_q: 4.201785\n",
            " 59929/100000: episode: 5610, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.156103, mean_q: 4.641174\n",
            " 59937/100000: episode: 5611, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.999249, mean_q: 4.460040\n",
            " 59945/100000: episode: 5612, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.062769, mean_q: 4.531648\n",
            " 59953/100000: episode: 5613, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.847711, mean_q: 4.283594\n",
            " 59963/100000: episode: 5614, duration: 0.113s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.788788, mean_q: 4.211245\n",
            " 59971/100000: episode: 5615, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.075671, mean_q: 4.542961\n",
            " 59979/100000: episode: 5616, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.815692, mean_q: 4.242557\n",
            " 59990/100000: episode: 5617, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000020, mae: 2.895440, mean_q: 4.334808\n",
            " 59998/100000: episode: 5618, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 2.899821, mean_q: 4.345175\n",
            " 60007/100000: episode: 5619, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000031, mae: 2.816459, mean_q: 4.247777\n",
            " 60015/100000: episode: 5620, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.924621, mean_q: 4.369429\n",
            " 60023/100000: episode: 5621, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.924812, mean_q: 4.376264\n",
            " 60032/100000: episode: 5622, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000043, mae: 2.855054, mean_q: 4.299338\n",
            " 60041/100000: episode: 5623, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000036, mae: 2.912112, mean_q: 4.353750\n",
            " 60051/100000: episode: 5624, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000036, mae: 2.909039, mean_q: 4.344563\n",
            " 60060/100000: episode: 5625, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000036, mae: 2.914948, mean_q: 4.347982\n",
            " 60068/100000: episode: 5626, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 2.906129, mean_q: 4.347614\n",
            " 60076/100000: episode: 5627, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000099, mae: 3.164404, mean_q: 4.650323\n",
            " 60084/100000: episode: 5628, duration: 0.094s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000267, mae: 2.835458, mean_q: 4.255800\n",
            " 60092/100000: episode: 5629, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000480, mae: 3.000935, mean_q: 4.446630\n",
            " 60101/100000: episode: 5630, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000299, mae: 2.810775, mean_q: 4.228935\n",
            " 60110/100000: episode: 5631, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000268, mae: 3.069778, mean_q: 4.529652\n",
            " 60119/100000: episode: 5632, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000230, mae: 3.023105, mean_q: 4.479659\n",
            " 60127/100000: episode: 5633, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000153, mae: 2.709289, mean_q: 4.117921\n",
            " 60136/100000: episode: 5634, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000184, mae: 2.997533, mean_q: 4.452233\n",
            " 60145/100000: episode: 5635, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000055, mae: 3.002122, mean_q: 4.453551\n",
            " 60153/100000: episode: 5636, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.784191, mean_q: 4.195741\n",
            " 60161/100000: episode: 5637, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 2.930556, mean_q: 4.363160\n",
            " 60170/100000: episode: 5638, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000027, mae: 3.082651, mean_q: 4.546707\n",
            " 60178/100000: episode: 5639, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.842758, mean_q: 4.269485\n",
            " 60186/100000: episode: 5640, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 3.101396, mean_q: 4.565754\n",
            " 60195/100000: episode: 5641, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000024, mae: 2.870222, mean_q: 4.310585\n",
            " 60203/100000: episode: 5642, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.849016, mean_q: 4.290687\n",
            " 60211/100000: episode: 5643, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.846149, mean_q: 4.281241\n",
            " 60221/100000: episode: 5644, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000015, mae: 2.970561, mean_q: 4.421499\n",
            " 60229/100000: episode: 5645, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 3.027005, mean_q: 4.479672\n",
            " 60239/100000: episode: 5646, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000019, mae: 3.044968, mean_q: 4.508891\n",
            " 60247/100000: episode: 5647, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.989928, mean_q: 4.443653\n",
            " 60256/100000: episode: 5648, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000074, mae: 2.996056, mean_q: 4.440937\n",
            " 60266/100000: episode: 5649, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000193, mae: 2.834629, mean_q: 4.274432\n",
            " 60275/100000: episode: 5650, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000556, mae: 3.031580, mean_q: 4.469467\n",
            " 60283/100000: episode: 5651, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000201, mae: 3.040740, mean_q: 4.479724\n",
            " 60291/100000: episode: 5652, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000179, mae: 3.135975, mean_q: 4.606579\n",
            " 60300/100000: episode: 5653, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000213, mae: 3.139067, mean_q: 4.604712\n",
            " 60309/100000: episode: 5654, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000137, mae: 2.998536, mean_q: 4.450007\n",
            " 60317/100000: episode: 5655, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000086, mae: 3.039145, mean_q: 4.493449\n",
            " 60327/100000: episode: 5656, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000058, mae: 3.190778, mean_q: 4.665060\n",
            " 60335/100000: episode: 5657, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.884908, mean_q: 4.323052\n",
            " 60343/100000: episode: 5658, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.884034, mean_q: 4.325709\n",
            " 60351/100000: episode: 5659, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.930156, mean_q: 4.373598\n",
            " 60359/100000: episode: 5660, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 3.226457, mean_q: 4.716570\n",
            " 60367/100000: episode: 5661, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000100, mae: 3.040167, mean_q: 4.491302\n",
            " 60376/100000: episode: 5662, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000085, mae: 2.895818, mean_q: 4.329677\n",
            " 60385/100000: episode: 5663, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000038, mae: 2.843137, mean_q: 4.269831\n",
            " 60393/100000: episode: 5664, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000052, mae: 3.002779, mean_q: 4.459284\n",
            " 60402/100000: episode: 5665, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000045, mae: 2.742238, mean_q: 4.161238\n",
            " 60413/100000: episode: 5666, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000152, mae: 3.229616, mean_q: 4.715419\n",
            " 60423/100000: episode: 5667, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000088, mae: 3.000062, mean_q: 4.462121\n",
            " 60433/100000: episode: 5668, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000112, mae: 3.150511, mean_q: 4.633988\n",
            " 60443/100000: episode: 5669, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000085, mae: 3.043626, mean_q: 4.507912\n",
            " 60451/100000: episode: 5670, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.982340, mean_q: 4.438894\n",
            " 60460/100000: episode: 5671, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000030, mae: 2.931770, mean_q: 4.381327\n",
            " 60469/100000: episode: 5672, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000033, mae: 2.766634, mean_q: 4.187014\n",
            " 60477/100000: episode: 5673, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.037542, mean_q: 4.499076\n",
            " 60486/100000: episode: 5674, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000009, mae: 2.861995, mean_q: 4.296117\n",
            " 60495/100000: episode: 5675, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000014, mae: 2.737531, mean_q: 4.147867\n",
            " 60503/100000: episode: 5676, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.861017, mean_q: 4.297614\n",
            " 60512/100000: episode: 5677, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 2.935934, mean_q: 4.383780\n",
            " 60520/100000: episode: 5678, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.035182, mean_q: 4.498318\n",
            " 60529/100000: episode: 5679, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 2.856081, mean_q: 4.288179\n",
            " 60539/100000: episode: 5680, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000010, mae: 2.959118, mean_q: 4.410137\n",
            " 60547/100000: episode: 5681, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.000878, mean_q: 4.456646\n",
            " 60556/100000: episode: 5682, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.913498, mean_q: 4.359753\n",
            " 60565/100000: episode: 5683, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 3.044140, mean_q: 4.504807\n",
            " 60573/100000: episode: 5684, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.980663, mean_q: 4.433242\n",
            " 60581/100000: episode: 5685, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.956289, mean_q: 4.404193\n",
            " 60589/100000: episode: 5686, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.937637, mean_q: 4.380923\n",
            " 60598/100000: episode: 5687, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 2.829929, mean_q: 4.257620\n",
            " 60608/100000: episode: 5688, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000003, mae: 3.025501, mean_q: 4.487342\n",
            " 60617/100000: episode: 5689, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.996589, mean_q: 4.454360\n",
            " 60625/100000: episode: 5690, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.917280, mean_q: 4.358066\n",
            " 60633/100000: episode: 5691, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.975806, mean_q: 4.432975\n",
            " 60642/100000: episode: 5692, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.914382, mean_q: 4.360808\n",
            " 60652/100000: episode: 5693, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000002, mae: 2.910831, mean_q: 4.356163\n",
            " 60661/100000: episode: 5694, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.794820, mean_q: 4.222960\n",
            " 60669/100000: episode: 5695, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.754040, mean_q: 4.174560\n",
            " 60677/100000: episode: 5696, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.095159, mean_q: 4.570069\n",
            " 60686/100000: episode: 5697, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.843360, mean_q: 4.276882\n",
            " 60695/100000: episode: 5698, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 2.943960, mean_q: 4.395880\n",
            " 60706/100000: episode: 5699, duration: 0.156s, episode steps:  11, steps per second:  71, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000007, mae: 2.952202, mean_q: 4.403239\n",
            " 60714/100000: episode: 5700, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.804876, mean_q: 4.230779\n",
            " 60723/100000: episode: 5701, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000010, mae: 2.939195, mean_q: 4.385337\n",
            " 60731/100000: episode: 5702, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.792613, mean_q: 4.217959\n",
            " 60739/100000: episode: 5703, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.779956, mean_q: 4.203024\n",
            " 60747/100000: episode: 5704, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.890884, mean_q: 4.324625\n",
            " 60756/100000: episode: 5705, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000011, mae: 2.994181, mean_q: 4.451089\n",
            " 60764/100000: episode: 5706, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.857083, mean_q: 4.294003\n",
            " 60773/100000: episode: 5707, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000075, mae: 3.022371, mean_q: 4.478006\n",
            " 60782/100000: episode: 5708, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000078, mae: 2.781971, mean_q: 4.198730\n",
            " 60793/100000: episode: 5709, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000201, mae: 3.065923, mean_q: 4.527021\n",
            " 60801/100000: episode: 5710, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000603, mae: 2.929895, mean_q: 4.369061\n",
            " 60810/100000: episode: 5711, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000570, mae: 2.882433, mean_q: 4.309365\n",
            " 60819/100000: episode: 5712, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.001208, mae: 2.987017, mean_q: 4.417693\n",
            " 60827/100000: episode: 5713, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000890, mae: 2.842282, mean_q: 4.274606\n",
            " 60835/100000: episode: 5714, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000427, mae: 2.835956, mean_q: 4.261524\n",
            " 60844/100000: episode: 5715, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000474, mae: 3.173872, mean_q: 4.635229\n",
            " 60853/100000: episode: 5716, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000547, mae: 2.915220, mean_q: 4.346716\n",
            " 60861/100000: episode: 5717, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000662, mae: 2.993924, mean_q: 4.451507\n",
            " 60869/100000: episode: 5718, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000242, mae: 2.785220, mean_q: 4.198549\n",
            " 60877/100000: episode: 5719, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000187, mae: 3.136447, mean_q: 4.604553\n",
            " 60886/100000: episode: 5720, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000371, mae: 2.983185, mean_q: 4.429316\n",
            " 60895/100000: episode: 5721, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000233, mae: 2.837699, mean_q: 4.261906\n",
            " 60903/100000: episode: 5722, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000125, mae: 2.988503, mean_q: 4.419547\n",
            " 60911/100000: episode: 5723, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000260, mae: 2.997643, mean_q: 4.442672\n",
            " 60920/100000: episode: 5724, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000207, mae: 3.082573, mean_q: 4.559294\n",
            " 60929/100000: episode: 5725, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000265, mae: 3.078631, mean_q: 4.538973\n",
            " 60939/100000: episode: 5726, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000167, mae: 2.852606, mean_q: 4.273293\n",
            " 60947/100000: episode: 5727, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000129, mae: 3.012033, mean_q: 4.473542\n",
            " 60955/100000: episode: 5728, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000109, mae: 3.110136, mean_q: 4.579740\n",
            " 60963/100000: episode: 5729, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.921993, mean_q: 4.359859\n",
            " 60971/100000: episode: 5730, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 3.047478, mean_q: 4.505437\n",
            " 60979/100000: episode: 5731, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.975216, mean_q: 4.426504\n",
            " 60987/100000: episode: 5732, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.038625, mean_q: 4.505310\n",
            " 60995/100000: episode: 5733, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.106685, mean_q: 4.580547\n",
            " 61004/100000: episode: 5734, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000011, mae: 2.874923, mean_q: 4.314135\n",
            " 61012/100000: episode: 5735, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.672852, mean_q: 4.076674\n",
            " 61021/100000: episode: 5736, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.972025, mean_q: 4.429584\n",
            " 61029/100000: episode: 5737, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.907467, mean_q: 4.352363\n",
            " 61037/100000: episode: 5738, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.061161, mean_q: 4.528533\n",
            " 61047/100000: episode: 5739, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.977286, mean_q: 4.435303\n",
            " 61055/100000: episode: 5740, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.892541, mean_q: 4.333563\n",
            " 61064/100000: episode: 5741, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.858099, mean_q: 4.292401\n",
            " 61073/100000: episode: 5742, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.626657, mean_q: 4.029243\n",
            " 61082/100000: episode: 5743, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 2.883898, mean_q: 4.321903\n",
            " 61090/100000: episode: 5744, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.899312, mean_q: 4.338308\n",
            " 61099/100000: episode: 5745, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 3.004249, mean_q: 4.465988\n",
            " 61110/100000: episode: 5746, duration: 0.111s, episode steps:  11, steps per second:  99, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000001, mae: 2.897781, mean_q: 4.337933\n",
            " 61118/100000: episode: 5747, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.020127, mean_q: 4.479313\n",
            " 61127/100000: episode: 5748, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 2.983352, mean_q: 4.434616\n",
            " 61136/100000: episode: 5749, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.047215, mean_q: 4.517463\n",
            " 61144/100000: episode: 5750, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.255986, mean_q: 4.750289\n",
            " 61153/100000: episode: 5751, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.009494, mean_q: 4.471950\n",
            " 61161/100000: episode: 5752, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.896583, mean_q: 4.338580\n",
            " 61170/100000: episode: 5753, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000000, mae: 2.859890, mean_q: 4.299036\n",
            " 61182/100000: episode: 5754, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.250 [0.000, 7.000],  loss: 0.000001, mae: 2.967005, mean_q: 4.422311\n",
            " 61190/100000: episode: 5755, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.128292, mean_q: 4.605992\n",
            " 61198/100000: episode: 5756, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.121580, mean_q: 4.596969\n",
            " 61206/100000: episode: 5757, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.920015, mean_q: 4.365169\n",
            " 61216/100000: episode: 5758, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 3.005164, mean_q: 4.464551\n",
            " 61227/100000: episode: 5759, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000002, mae: 3.048542, mean_q: 4.514489\n",
            " 61236/100000: episode: 5760, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.880542, mean_q: 4.322966\n",
            " 61244/100000: episode: 5761, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.990984, mean_q: 4.446691\n",
            " 61252/100000: episode: 5762, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.812447, mean_q: 4.244988\n",
            " 61264/100000: episode: 5763, duration: 0.126s, episode steps:  12, steps per second:  95, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 2.917 [0.000, 7.000],  loss: 0.000001, mae: 2.798579, mean_q: 4.226597\n",
            " 61272/100000: episode: 5764, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.997356, mean_q: 4.452908\n",
            " 61280/100000: episode: 5765, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.927570, mean_q: 4.374420\n",
            " 61288/100000: episode: 5766, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.984988, mean_q: 4.439552\n",
            " 61296/100000: episode: 5767, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.755962, mean_q: 4.175184\n",
            " 61306/100000: episode: 5768, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000001, mae: 2.962825, mean_q: 4.414887\n",
            " 61314/100000: episode: 5769, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.039927, mean_q: 4.497725\n",
            " 61322/100000: episode: 5770, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.883826, mean_q: 4.323326\n",
            " 61330/100000: episode: 5771, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.911948, mean_q: 4.356924\n",
            " 61338/100000: episode: 5772, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.931284, mean_q: 4.379233\n",
            " 61346/100000: episode: 5773, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.785847, mean_q: 4.208599\n",
            " 61355/100000: episode: 5774, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000009, mae: 2.768777, mean_q: 4.188085\n",
            " 61365/100000: episode: 5775, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000004, mae: 3.022446, mean_q: 4.481070\n",
            " 61374/100000: episode: 5776, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.909417, mean_q: 4.353041\n",
            " 61383/100000: episode: 5777, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.083195, mean_q: 4.553602\n",
            " 61391/100000: episode: 5778, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.903329, mean_q: 4.344701\n",
            " 61400/100000: episode: 5779, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.897746, mean_q: 4.336827\n",
            " 61409/100000: episode: 5780, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 3.041778, mean_q: 4.504118\n",
            " 61419/100000: episode: 5781, duration: 0.123s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000003, mae: 2.846179, mean_q: 4.282026\n",
            " 61430/100000: episode: 5782, duration: 0.118s, episode steps:  11, steps per second:  93, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000004, mae: 2.904309, mean_q: 4.345664\n",
            " 61438/100000: episode: 5783, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.845822, mean_q: 4.278826\n",
            " 61446/100000: episode: 5784, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.760868, mean_q: 4.178520\n",
            " 61456/100000: episode: 5785, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000002, mae: 2.938978, mean_q: 4.387393\n",
            " 61464/100000: episode: 5786, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.984401, mean_q: 4.438865\n",
            " 61475/100000: episode: 5787, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000002, mae: 2.956126, mean_q: 4.408868\n",
            " 61483/100000: episode: 5788, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.048954, mean_q: 4.515094\n",
            " 61491/100000: episode: 5789, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.851084, mean_q: 4.284189\n",
            " 61500/100000: episode: 5790, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 3.002648, mean_q: 4.461634\n",
            " 61510/100000: episode: 5791, duration: 0.121s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000006, mae: 2.882623, mean_q: 4.326884\n",
            " 61518/100000: episode: 5792, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.039488, mean_q: 4.508136\n",
            " 61526/100000: episode: 5793, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.880774, mean_q: 4.319998\n",
            " 61534/100000: episode: 5794, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.922168, mean_q: 4.371525\n",
            " 61542/100000: episode: 5795, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.052951, mean_q: 4.518652\n",
            " 61550/100000: episode: 5796, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.839708, mean_q: 4.276527\n",
            " 61560/100000: episode: 5797, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000029, mae: 3.069613, mean_q: 4.537580\n",
            " 61568/100000: episode: 5798, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000126, mae: 3.110334, mean_q: 4.572834\n",
            " 61576/100000: episode: 5799, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 3.023494, mean_q: 4.479872\n",
            " 61584/100000: episode: 5800, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.868502, mean_q: 4.301759\n",
            " 61592/100000: episode: 5801, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.713291, mean_q: 4.118231\n",
            " 61601/100000: episode: 5802, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000013, mae: 3.080389, mean_q: 4.542149\n",
            " 61610/100000: episode: 5803, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000021, mae: 3.003675, mean_q: 4.460746\n",
            " 61621/100000: episode: 5804, duration: 0.115s, episode steps:  11, steps per second:  95, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.818 [0.000, 7.000],  loss: 0.000023, mae: 2.837259, mean_q: 4.266755\n",
            " 61629/100000: episode: 5805, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.923098, mean_q: 4.358927\n",
            " 61637/100000: episode: 5806, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.907029, mean_q: 4.345881\n",
            " 61648/100000: episode: 5807, duration: 0.124s, episode steps:  11, steps per second:  89, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000016, mae: 2.870524, mean_q: 4.309075\n",
            " 61656/100000: episode: 5808, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 3.016233, mean_q: 4.475996\n",
            " 61664/100000: episode: 5809, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000110, mae: 2.956675, mean_q: 4.405944\n",
            " 61673/100000: episode: 5810, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000047, mae: 3.019960, mean_q: 4.476379\n",
            " 61681/100000: episode: 5811, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.988711, mean_q: 4.434014\n",
            " 61689/100000: episode: 5812, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000088, mae: 3.057127, mean_q: 4.512954\n",
            " 61697/100000: episode: 5813, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000062, mae: 3.030179, mean_q: 4.490143\n",
            " 61706/100000: episode: 5814, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000063, mae: 2.885176, mean_q: 4.318920\n",
            " 61717/100000: episode: 5815, duration: 0.121s, episode steps:  11, steps per second:  91, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000042, mae: 3.073563, mean_q: 4.539478\n",
            " 61726/100000: episode: 5816, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000021, mae: 3.022753, mean_q: 4.480375\n",
            " 61734/100000: episode: 5817, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.011105, mean_q: 4.476340\n",
            " 61743/100000: episode: 5818, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000016, mae: 3.304770, mean_q: 4.807480\n",
            " 61751/100000: episode: 5819, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.799410, mean_q: 4.223284\n",
            " 61759/100000: episode: 5820, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.617633, mean_q: 4.014392\n",
            " 61767/100000: episode: 5821, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.019487, mean_q: 4.478727\n",
            " 61775/100000: episode: 5822, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.205689, mean_q: 4.685562\n",
            " 61783/100000: episode: 5823, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.950331, mean_q: 4.402015\n",
            " 61791/100000: episode: 5824, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.154081, mean_q: 4.645471\n",
            " 61799/100000: episode: 5825, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 2.994833, mean_q: 4.449978\n",
            " 61807/100000: episode: 5826, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000081, mae: 2.785588, mean_q: 4.202595\n",
            " 61815/100000: episode: 5827, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.814813, mean_q: 4.243730\n",
            " 61823/100000: episode: 5828, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 2.994938, mean_q: 4.447784\n",
            " 61831/100000: episode: 5829, duration: 0.126s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.995263, mean_q: 4.450439\n",
            " 61839/100000: episode: 5830, duration: 0.164s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.933770, mean_q: 4.376251\n",
            " 61848/100000: episode: 5831, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000064, mae: 2.855384, mean_q: 4.283502\n",
            " 61857/100000: episode: 5832, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000099, mae: 2.831351, mean_q: 4.257607\n",
            " 61866/100000: episode: 5833, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000040, mae: 2.936114, mean_q: 4.382746\n",
            " 61874/100000: episode: 5834, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.909024, mean_q: 4.353313\n",
            " 61882/100000: episode: 5835, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.931798, mean_q: 4.373560\n",
            " 61890/100000: episode: 5836, duration: 0.163s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 2.964214, mean_q: 4.402984\n",
            " 61898/100000: episode: 5837, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 3.005353, mean_q: 4.463626\n",
            " 61907/100000: episode: 5838, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000088, mae: 2.998659, mean_q: 4.454260\n",
            " 61917/100000: episode: 5839, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 2.966262, mean_q: 4.418151\n",
            " 61926/100000: episode: 5840, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000043, mae: 3.011517, mean_q: 4.463084\n",
            " 61934/100000: episode: 5841, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 3.044764, mean_q: 4.509569\n",
            " 61942/100000: episode: 5842, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.792384, mean_q: 4.229183\n",
            " 61952/100000: episode: 5843, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000068, mae: 3.084818, mean_q: 4.555164\n",
            " 61960/100000: episode: 5844, duration: 0.171s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.805638, mean_q: 4.232965\n",
            " 61969/100000: episode: 5845, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000087, mae: 2.774962, mean_q: 4.184602\n",
            " 61977/100000: episode: 5846, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 2.978646, mean_q: 4.431256\n",
            " 61988/100000: episode: 5847, duration: 0.114s, episode steps:  11, steps per second:  97, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000191, mae: 2.985909, mean_q: 4.434666\n",
            " 61997/100000: episode: 5848, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000490, mae: 2.887523, mean_q: 4.305190\n",
            " 62005/100000: episode: 5849, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000529, mae: 2.923982, mean_q: 4.369074\n",
            " 62014/100000: episode: 5850, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000433, mae: 2.843521, mean_q: 4.282017\n",
            " 62022/100000: episode: 5851, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000320, mae: 3.102550, mean_q: 4.559283\n",
            " 62031/100000: episode: 5852, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000403, mae: 3.169441, mean_q: 4.625278\n",
            " 62040/100000: episode: 5853, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000423, mae: 3.100088, mean_q: 4.563024\n",
            " 62048/100000: episode: 5854, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000193, mae: 2.833194, mean_q: 4.272934\n",
            " 62056/100000: episode: 5855, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000094, mae: 2.833840, mean_q: 4.268131\n",
            " 62064/100000: episode: 5856, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.941193, mean_q: 4.382919\n",
            " 62074/100000: episode: 5857, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000096, mae: 3.080703, mean_q: 4.543147\n",
            " 62083/100000: episode: 5858, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000107, mae: 2.910306, mean_q: 4.349115\n",
            " 62091/100000: episode: 5859, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 2.904740, mean_q: 4.341730\n",
            " 62099/100000: episode: 5860, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000193, mae: 3.098529, mean_q: 4.567870\n",
            " 62109/100000: episode: 5861, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000336, mae: 2.962460, mean_q: 4.390437\n",
            " 62117/100000: episode: 5862, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000221, mae: 2.774183, mean_q: 4.192872\n",
            " 62126/100000: episode: 5863, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000214, mae: 2.911853, mean_q: 4.348793\n",
            " 62134/100000: episode: 5864, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000166, mae: 2.878541, mean_q: 4.303515\n",
            " 62142/100000: episode: 5865, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000162, mae: 2.980619, mean_q: 4.432627\n",
            " 62150/100000: episode: 5866, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 2.968973, mean_q: 4.414882\n",
            " 62160/100000: episode: 5867, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000112, mae: 2.872294, mean_q: 4.298303\n",
            " 62168/100000: episode: 5868, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 2.952629, mean_q: 4.401110\n",
            " 62176/100000: episode: 5869, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000085, mae: 3.041615, mean_q: 4.505047\n",
            " 62186/100000: episode: 5870, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000122, mae: 2.929760, mean_q: 4.364127\n",
            " 62195/100000: episode: 5871, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000035, mae: 2.914738, mean_q: 4.352183\n",
            " 62204/100000: episode: 5872, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000082, mae: 2.901503, mean_q: 4.335448\n",
            " 62213/100000: episode: 5873, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000086, mae: 2.922718, mean_q: 4.357973\n",
            " 62221/100000: episode: 5874, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000135, mae: 2.912251, mean_q: 4.347592\n",
            " 62229/100000: episode: 5875, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000241, mae: 3.079064, mean_q: 4.543513\n",
            " 62237/100000: episode: 5876, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 2.885050, mean_q: 4.316807\n",
            " 62245/100000: episode: 5877, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000114, mae: 2.917368, mean_q: 4.359752\n",
            " 62253/100000: episode: 5878, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 3.076725, mean_q: 4.539358\n",
            " 62262/100000: episode: 5879, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000059, mae: 2.815827, mean_q: 4.240444\n",
            " 62273/100000: episode: 5880, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000046, mae: 2.852567, mean_q: 4.284505\n",
            " 62281/100000: episode: 5881, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.857083, mean_q: 4.294551\n",
            " 62289/100000: episode: 5882, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.137717, mean_q: 4.614845\n",
            " 62298/100000: episode: 5883, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000019, mae: 2.929145, mean_q: 4.373560\n",
            " 62306/100000: episode: 5884, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.031044, mean_q: 4.490543\n",
            " 62316/100000: episode: 5885, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000011, mae: 2.813164, mean_q: 4.234153\n",
            " 62325/100000: episode: 5886, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 3.050439, mean_q: 4.512491\n",
            " 62336/100000: episode: 5887, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.818 [0.000, 7.000],  loss: 0.000004, mae: 2.988526, mean_q: 4.445293\n",
            " 62344/100000: episode: 5888, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.806058, mean_q: 4.228468\n",
            " 62353/100000: episode: 5889, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.935660, mean_q: 4.384843\n",
            " 62362/100000: episode: 5890, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.805098, mean_q: 4.229162\n",
            " 62371/100000: episode: 5891, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 2.794784, mean_q: 4.216745\n",
            " 62379/100000: episode: 5892, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.754973, mean_q: 4.175156\n",
            " 62388/100000: episode: 5893, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 3.047194, mean_q: 4.511006\n",
            " 62396/100000: episode: 5894, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.660912, mean_q: 4.064140\n",
            " 62404/100000: episode: 5895, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.919473, mean_q: 4.367244\n",
            " 62412/100000: episode: 5896, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.952950, mean_q: 4.401954\n",
            " 62421/100000: episode: 5897, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 3.064085, mean_q: 4.534414\n",
            " 62429/100000: episode: 5898, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.073903, mean_q: 4.539058\n",
            " 62437/100000: episode: 5899, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.857367, mean_q: 4.292121\n",
            " 62447/100000: episode: 5900, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000003, mae: 2.670804, mean_q: 4.079675\n",
            " 62456/100000: episode: 5901, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 3.139209, mean_q: 4.619135\n",
            " 62465/100000: episode: 5902, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.156025, mean_q: 4.635381\n",
            " 62473/100000: episode: 5903, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.807984, mean_q: 4.231403\n",
            " 62481/100000: episode: 5904, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.883647, mean_q: 4.320408\n",
            " 62491/100000: episode: 5905, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000003, mae: 2.963925, mean_q: 4.419025\n",
            " 62501/100000: episode: 5906, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000003, mae: 2.729688, mean_q: 4.141036\n",
            " 62511/100000: episode: 5907, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.993804, mean_q: 4.448350\n",
            " 62520/100000: episode: 5908, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 2.974671, mean_q: 4.428252\n",
            " 62529/100000: episode: 5909, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 2.821023, mean_q: 4.254620\n",
            " 62538/100000: episode: 5910, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000006, mae: 2.956431, mean_q: 4.406282\n",
            " 62547/100000: episode: 5911, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 2.811209, mean_q: 4.239561\n",
            " 62555/100000: episode: 5912, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.121969, mean_q: 4.595887\n",
            " 62566/100000: episode: 5913, duration: 0.099s, episode steps:  11, steps per second: 111, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000005, mae: 3.081707, mean_q: 4.550016\n",
            " 62574/100000: episode: 5914, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.866396, mean_q: 4.301037\n",
            " 62582/100000: episode: 5915, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.913838, mean_q: 4.355561\n",
            " 62591/100000: episode: 5916, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.764616, mean_q: 4.183363\n",
            " 62601/100000: episode: 5917, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000006, mae: 2.971548, mean_q: 4.427239\n",
            " 62610/100000: episode: 5918, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000016, mae: 3.088925, mean_q: 4.558292\n",
            " 62619/100000: episode: 5919, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000008, mae: 2.928636, mean_q: 4.374690\n",
            " 62629/100000: episode: 5920, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000006, mae: 2.866265, mean_q: 4.301139\n",
            " 62637/100000: episode: 5921, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.844902, mean_q: 4.281427\n",
            " 62645/100000: episode: 5922, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.761520, mean_q: 4.177451\n",
            " 62654/100000: episode: 5923, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000010, mae: 2.973538, mean_q: 4.427152\n",
            " 62663/100000: episode: 5924, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 2.933019, mean_q: 4.378395\n",
            " 62673/100000: episode: 5925, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000009, mae: 3.076997, mean_q: 4.552263\n",
            " 62682/100000: episode: 5926, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000012, mae: 2.726678, mean_q: 4.144831\n",
            " 62690/100000: episode: 5927, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.910648, mean_q: 4.355251\n",
            " 62699/100000: episode: 5928, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.923264, mean_q: 4.365977\n",
            " 62709/100000: episode: 5929, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.973235, mean_q: 4.423665\n",
            " 62718/100000: episode: 5930, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000011, mae: 2.889625, mean_q: 4.328770\n",
            " 62727/100000: episode: 5931, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000021, mae: 2.768916, mean_q: 4.185737\n",
            " 62736/100000: episode: 5932, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 2.874393, mean_q: 4.315279\n",
            " 62744/100000: episode: 5933, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.905900, mean_q: 4.351282\n",
            " 62752/100000: episode: 5934, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.012700, mean_q: 4.466149\n",
            " 62761/100000: episode: 5935, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000022, mae: 2.820754, mean_q: 4.255989\n",
            " 62771/100000: episode: 5936, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000057, mae: 2.767399, mean_q: 4.184463\n",
            " 62779/100000: episode: 5937, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000143, mae: 2.956615, mean_q: 4.396121\n",
            " 62787/100000: episode: 5938, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 2.966230, mean_q: 4.422531\n",
            " 62795/100000: episode: 5939, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000095, mae: 3.045688, mean_q: 4.507756\n",
            " 62803/100000: episode: 5940, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 2.969244, mean_q: 4.425526\n",
            " 62812/100000: episode: 5941, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000031, mae: 2.973730, mean_q: 4.421739\n",
            " 62823/100000: episode: 5942, duration: 0.116s, episode steps:  11, steps per second:  95, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000057, mae: 2.949168, mean_q: 4.393559\n",
            " 62831/100000: episode: 5943, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.920405, mean_q: 4.367544\n",
            " 62840/100000: episode: 5944, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000032, mae: 2.831177, mean_q: 4.265138\n",
            " 62850/100000: episode: 5945, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000041, mae: 2.792762, mean_q: 4.212357\n",
            " 62858/100000: episode: 5946, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000063, mae: 3.084741, mean_q: 4.556696\n",
            " 62868/100000: episode: 5947, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000063, mae: 2.873095, mean_q: 4.304036\n",
            " 62876/100000: episode: 5948, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 2.942609, mean_q: 4.393732\n",
            " 62885/100000: episode: 5949, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000025, mae: 2.828120, mean_q: 4.257122\n",
            " 62894/100000: episode: 5950, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000018, mae: 2.889524, mean_q: 4.324136\n",
            " 62903/100000: episode: 5951, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000019, mae: 2.898947, mean_q: 4.341611\n",
            " 62912/100000: episode: 5952, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000019, mae: 2.883825, mean_q: 4.320871\n",
            " 62920/100000: episode: 5953, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.004838, mean_q: 4.463846\n",
            " 62929/100000: episode: 5954, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000008, mae: 2.898930, mean_q: 4.338593\n",
            " 62938/100000: episode: 5955, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 2.990418, mean_q: 4.444512\n",
            " 62946/100000: episode: 5956, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.610620, mean_q: 4.006178\n",
            " 62954/100000: episode: 5957, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.986298, mean_q: 4.442913\n",
            " 62962/100000: episode: 5958, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.018217, mean_q: 4.484372\n",
            " 62970/100000: episode: 5959, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.848760, mean_q: 4.281712\n",
            " 62978/100000: episode: 5960, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 3.197433, mean_q: 4.681865\n",
            " 62986/100000: episode: 5961, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.005497, mean_q: 4.461397\n",
            " 62994/100000: episode: 5962, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.894582, mean_q: 4.329202\n",
            " 63002/100000: episode: 5963, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 3.062469, mean_q: 4.527677\n",
            " 63011/100000: episode: 5964, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000041, mae: 2.979814, mean_q: 4.425686\n",
            " 63019/100000: episode: 5965, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.091454, mean_q: 4.561595\n",
            " 63028/100000: episode: 5966, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000036, mae: 2.794812, mean_q: 4.213381\n",
            " 63037/100000: episode: 5967, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000043, mae: 2.767514, mean_q: 4.182513\n",
            " 63045/100000: episode: 5968, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.994074, mean_q: 4.445093\n",
            " 63053/100000: episode: 5969, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.897877, mean_q: 4.343595\n",
            " 63062/100000: episode: 5970, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000038, mae: 3.085562, mean_q: 4.553092\n",
            " 63070/100000: episode: 5971, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.889720, mean_q: 4.323759\n",
            " 63078/100000: episode: 5972, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000102, mae: 2.956899, mean_q: 4.405037\n",
            " 63088/100000: episode: 5973, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000109, mae: 2.874998, mean_q: 4.313182\n",
            " 63096/100000: episode: 5974, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 2.969415, mean_q: 4.428087\n",
            " 63106/100000: episode: 5975, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000071, mae: 3.058324, mean_q: 4.523424\n",
            " 63116/100000: episode: 5976, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000061, mae: 2.997646, mean_q: 4.452434\n",
            " 63126/100000: episode: 5977, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000041, mae: 2.885814, mean_q: 4.326481\n",
            " 63136/100000: episode: 5978, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000034, mae: 2.991045, mean_q: 4.455852\n",
            " 63144/100000: episode: 5979, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.858839, mean_q: 4.287466\n",
            " 63153/100000: episode: 5980, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000019, mae: 2.983783, mean_q: 4.428062\n",
            " 63163/100000: episode: 5981, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000017, mae: 3.105163, mean_q: 4.574785\n",
            " 63173/100000: episode: 5982, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000026, mae: 3.226142, mean_q: 4.720478\n",
            " 63183/100000: episode: 5983, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.825620, mean_q: 4.252570\n",
            " 63192/100000: episode: 5984, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000042, mae: 2.700903, mean_q: 4.108169\n",
            " 63201/100000: episode: 5985, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000029, mae: 2.850732, mean_q: 4.281766\n",
            " 63210/100000: episode: 5986, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000032, mae: 2.893118, mean_q: 4.333483\n",
            " 63218/100000: episode: 5987, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.775440, mean_q: 4.194182\n",
            " 63226/100000: episode: 5988, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000111, mae: 2.889774, mean_q: 4.326908\n",
            " 63236/100000: episode: 5989, duration: 0.096s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000393, mae: 2.793176, mean_q: 4.214836\n",
            " 63245/100000: episode: 5990, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000572, mae: 3.036946, mean_q: 4.484734\n",
            " 63254/100000: episode: 5991, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000351, mae: 3.062979, mean_q: 4.505621\n",
            " 63262/100000: episode: 5992, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000214, mae: 2.984484, mean_q: 4.429089\n",
            " 63271/100000: episode: 5993, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000210, mae: 2.982865, mean_q: 4.424227\n",
            " 63281/100000: episode: 5994, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000161, mae: 2.995783, mean_q: 4.441163\n",
            " 63290/100000: episode: 5995, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000361, mae: 3.019845, mean_q: 4.469546\n",
            " 63298/100000: episode: 5996, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000249, mae: 2.804278, mean_q: 4.232785\n",
            " 63306/100000: episode: 5997, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000178, mae: 2.752103, mean_q: 4.161978\n",
            " 63315/100000: episode: 5998, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000319, mae: 3.040900, mean_q: 4.494031\n",
            " 63323/100000: episode: 5999, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000293, mae: 3.012104, mean_q: 4.463930\n",
            " 63332/100000: episode: 6000, duration: 0.090s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000260, mae: 3.049350, mean_q: 4.506456\n",
            " 63340/100000: episode: 6001, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000309, mae: 3.043141, mean_q: 4.511595\n",
            " 63348/100000: episode: 6002, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000354, mae: 2.801549, mean_q: 4.212410\n",
            " 63356/100000: episode: 6003, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000258, mae: 2.979169, mean_q: 4.421728\n",
            " 63365/100000: episode: 6004, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000267, mae: 3.073407, mean_q: 4.534796\n",
            " 63373/100000: episode: 6005, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000148, mae: 2.986983, mean_q: 4.428855\n",
            " 63382/100000: episode: 6006, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000863, mae: 2.993476, mean_q: 4.415136\n",
            " 63390/100000: episode: 6007, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000369, mae: 3.011102, mean_q: 4.452210\n",
            " 63398/100000: episode: 6008, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000940, mae: 3.014315, mean_q: 4.458586\n",
            " 63406/100000: episode: 6009, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000320, mae: 3.015343, mean_q: 4.477905\n",
            " 63414/100000: episode: 6010, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000315, mae: 2.736134, mean_q: 4.148480\n",
            " 63424/100000: episode: 6011, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000140, mae: 3.054742, mean_q: 4.517411\n",
            " 63432/100000: episode: 6012, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.941119, mean_q: 4.389665\n",
            " 63440/100000: episode: 6013, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 3.194632, mean_q: 4.679019\n",
            " 63449/100000: episode: 6014, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000053, mae: 3.021936, mean_q: 4.489955\n",
            " 63458/100000: episode: 6015, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000046, mae: 3.026169, mean_q: 4.486520\n",
            " 63466/100000: episode: 6016, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.764222, mean_q: 4.177290\n",
            " 63474/100000: episode: 6017, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.776268, mean_q: 4.195398\n",
            " 63482/100000: episode: 6018, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.101677, mean_q: 4.567286\n",
            " 63491/100000: episode: 6019, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000036, mae: 2.969121, mean_q: 4.418032\n",
            " 63501/100000: episode: 6020, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000039, mae: 2.890714, mean_q: 4.329047\n",
            " 63510/100000: episode: 6021, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000029, mae: 2.942124, mean_q: 4.388056\n",
            " 63518/100000: episode: 6022, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.908452, mean_q: 4.350127\n",
            " 63526/100000: episode: 6023, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.865482, mean_q: 4.303432\n",
            " 63534/100000: episode: 6024, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.784747, mean_q: 4.208430\n",
            " 63542/100000: episode: 6025, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.059934, mean_q: 4.528643\n",
            " 63551/100000: episode: 6026, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.984141, mean_q: 4.439607\n",
            " 63560/100000: episode: 6027, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.969326, mean_q: 4.423688\n",
            " 63569/100000: episode: 6028, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.922422, mean_q: 4.365687\n",
            " 63578/100000: episode: 6029, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.945029, mean_q: 4.397566\n",
            " 63587/100000: episode: 6030, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.723867, mean_q: 4.138945\n",
            " 63596/100000: episode: 6031, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.172761, mean_q: 4.658775\n",
            " 63604/100000: episode: 6032, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.942989, mean_q: 4.390645\n",
            " 63612/100000: episode: 6033, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.031999, mean_q: 4.494199\n",
            " 63620/100000: episode: 6034, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.880769, mean_q: 4.319818\n",
            " 63631/100000: episode: 6035, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000002, mae: 3.045171, mean_q: 4.509027\n",
            " 63640/100000: episode: 6036, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.723899, mean_q: 4.133749\n",
            " 63648/100000: episode: 6037, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.971911, mean_q: 4.426920\n",
            " 63658/100000: episode: 6038, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000001, mae: 2.988625, mean_q: 4.441471\n",
            " 63666/100000: episode: 6039, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.831179, mean_q: 4.265907\n",
            " 63674/100000: episode: 6040, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.836741, mean_q: 4.271378\n",
            " 63683/100000: episode: 6041, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000000, mae: 2.742520, mean_q: 4.163097\n",
            " 63692/100000: episode: 6042, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000000, mae: 2.822051, mean_q: 4.254467\n",
            " 63702/100000: episode: 6043, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 2.860782, mean_q: 4.294655\n",
            " 63712/100000: episode: 6044, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000001, mae: 3.048586, mean_q: 4.510786\n",
            " 63720/100000: episode: 6045, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.079145, mean_q: 4.549727\n",
            " 63728/100000: episode: 6046, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.888175, mean_q: 4.334152\n",
            " 63736/100000: episode: 6047, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.815624, mean_q: 4.243176\n",
            " 63745/100000: episode: 6048, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000000, mae: 3.100667, mean_q: 4.573884\n",
            " 63753/100000: episode: 6049, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.044105, mean_q: 4.506594\n",
            " 63762/100000: episode: 6050, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 2.844716, mean_q: 4.277486\n",
            " 63772/100000: episode: 6051, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 2.810061, mean_q: 4.238603\n",
            " 63781/100000: episode: 6052, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 3.028919, mean_q: 4.486007\n",
            " 63790/100000: episode: 6053, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.004203, mean_q: 4.456717\n",
            " 63799/100000: episode: 6054, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000000, mae: 2.776469, mean_q: 4.203083\n",
            " 63807/100000: episode: 6055, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.977176, mean_q: 4.431536\n",
            " 63815/100000: episode: 6056, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.798169, mean_q: 4.224734\n",
            " 63824/100000: episode: 6057, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000000, mae: 2.995175, mean_q: 4.454659\n",
            " 63832/100000: episode: 6058, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.080925, mean_q: 4.548316\n",
            " 63840/100000: episode: 6059, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.676762, mean_q: 4.088276\n",
            " 63848/100000: episode: 6060, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.194673, mean_q: 4.682032\n",
            " 63856/100000: episode: 6061, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.843632, mean_q: 4.277474\n",
            " 63865/100000: episode: 6062, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 2.908571, mean_q: 4.353744\n",
            " 63873/100000: episode: 6063, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.931538, mean_q: 4.376822\n",
            " 63881/100000: episode: 6064, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.074795, mean_q: 4.545512\n",
            " 63890/100000: episode: 6065, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.966824, mean_q: 4.418836\n",
            " 63900/100000: episode: 6066, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.802057, mean_q: 4.228385\n",
            " 63908/100000: episode: 6067, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.082773, mean_q: 4.554430\n",
            " 63918/100000: episode: 6068, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000000, mae: 2.965685, mean_q: 4.417144\n",
            " 63927/100000: episode: 6069, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000000, mae: 2.966778, mean_q: 4.424665\n",
            " 63936/100000: episode: 6070, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000000, mae: 2.931752, mean_q: 4.379828\n",
            " 63945/100000: episode: 6071, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000000, mae: 3.102864, mean_q: 4.578024\n",
            " 63953/100000: episode: 6072, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.971138, mean_q: 4.425522\n",
            " 63962/100000: episode: 6073, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000000, mae: 2.867066, mean_q: 4.308352\n",
            " 63971/100000: episode: 6074, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000000, mae: 2.661400, mean_q: 4.065858\n",
            " 63979/100000: episode: 6075, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.838270, mean_q: 4.274155\n",
            " 63987/100000: episode: 6076, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.156883, mean_q: 4.640676\n",
            " 63995/100000: episode: 6077, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.864853, mean_q: 4.300991\n",
            " 64004/100000: episode: 6078, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000000, mae: 2.878053, mean_q: 4.317953\n",
            " 64013/100000: episode: 6079, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000000, mae: 2.810199, mean_q: 4.237629\n",
            " 64022/100000: episode: 6080, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000000, mae: 2.737509, mean_q: 4.154803\n",
            " 64030/100000: episode: 6081, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.835471, mean_q: 4.266579\n",
            " 64040/100000: episode: 6082, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000001, mae: 2.955872, mean_q: 4.404068\n",
            " 64048/100000: episode: 6083, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.083500, mean_q: 4.554511\n",
            " 64057/100000: episode: 6084, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.886495, mean_q: 4.326975\n",
            " 64066/100000: episode: 6085, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.949826, mean_q: 4.400031\n",
            " 64074/100000: episode: 6086, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.816908, mean_q: 4.244110\n",
            " 64083/100000: episode: 6087, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.833271, mean_q: 4.260927\n",
            " 64092/100000: episode: 6088, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.932389, mean_q: 4.379140\n",
            " 64101/100000: episode: 6089, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.979497, mean_q: 4.434970\n",
            " 64109/100000: episode: 6090, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.746469, mean_q: 4.170417\n",
            " 64117/100000: episode: 6091, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.020073, mean_q: 4.479023\n",
            " 64125/100000: episode: 6092, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.045953, mean_q: 4.508811\n",
            " 64134/100000: episode: 6093, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.009932, mean_q: 4.470738\n",
            " 64144/100000: episode: 6094, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.931665, mean_q: 4.380083\n",
            " 64152/100000: episode: 6095, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.805818, mean_q: 4.232269\n",
            " 64161/100000: episode: 6096, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.722174, mean_q: 4.139803\n",
            " 64169/100000: episode: 6097, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.922713, mean_q: 4.368965\n",
            " 64177/100000: episode: 6098, duration: 0.172s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.943985, mean_q: 4.395955\n",
            " 64185/100000: episode: 6099, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.069535, mean_q: 4.532351\n",
            " 64194/100000: episode: 6100, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.958821, mean_q: 4.409060\n",
            " 64203/100000: episode: 6101, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.018111, mean_q: 4.474887\n",
            " 64213/100000: episode: 6102, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 2.942233, mean_q: 4.386849\n",
            " 64222/100000: episode: 6103, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.943036, mean_q: 4.393034\n",
            " 64232/100000: episode: 6104, duration: 0.149s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000003, mae: 2.859716, mean_q: 4.295331\n",
            " 64240/100000: episode: 6105, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.059698, mean_q: 4.530421\n",
            " 64248/100000: episode: 6106, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.240178, mean_q: 4.731388\n",
            " 64256/100000: episode: 6107, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.669784, mean_q: 4.078304\n",
            " 64264/100000: episode: 6108, duration: 0.153s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.138704, mean_q: 4.623426\n",
            " 64273/100000: episode: 6109, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.039654, mean_q: 4.501517\n",
            " 64284/100000: episode: 6110, duration: 0.202s, episode steps:  11, steps per second:  55, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000004, mae: 2.937237, mean_q: 4.383374\n",
            " 64292/100000: episode: 6111, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.027527, mean_q: 4.490163\n",
            " 64301/100000: episode: 6112, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 2.948949, mean_q: 4.399169\n",
            " 64310/100000: episode: 6113, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000007, mae: 3.027397, mean_q: 4.491648\n",
            " 64319/100000: episode: 6114, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.919764, mean_q: 4.361099\n",
            " 64327/100000: episode: 6115, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.852126, mean_q: 4.284305\n",
            " 64337/100000: episode: 6116, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000022, mae: 2.801373, mean_q: 4.222028\n",
            " 64346/100000: episode: 6117, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000007, mae: 3.071903, mean_q: 4.541013\n",
            " 64354/100000: episode: 6118, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.963958, mean_q: 4.412560\n",
            " 64362/100000: episode: 6119, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.702867, mean_q: 4.120575\n",
            " 64370/100000: episode: 6120, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.702485, mean_q: 4.110947\n",
            " 64379/100000: episode: 6121, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000021, mae: 2.905206, mean_q: 4.341915\n",
            " 64388/100000: episode: 6122, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000032, mae: 2.741310, mean_q: 4.158182\n",
            " 64396/100000: episode: 6123, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.175103, mean_q: 4.656647\n",
            " 64404/100000: episode: 6124, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.940639, mean_q: 4.384832\n",
            " 64414/100000: episode: 6125, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000042, mae: 3.012286, mean_q: 4.465312\n",
            " 64423/100000: episode: 6126, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000066, mae: 2.889480, mean_q: 4.333757\n",
            " 64431/100000: episode: 6127, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 3.025685, mean_q: 4.488195\n",
            " 64439/100000: episode: 6128, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.838458, mean_q: 4.256101\n",
            " 64450/100000: episode: 6129, duration: 0.111s, episode steps:  11, steps per second:  99, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000085, mae: 2.932582, mean_q: 4.379648\n",
            " 64458/100000: episode: 6130, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000130, mae: 2.927630, mean_q: 4.366139\n",
            " 64467/100000: episode: 6131, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000153, mae: 2.957533, mean_q: 4.402696\n",
            " 64475/100000: episode: 6132, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000126, mae: 2.996780, mean_q: 4.430985\n",
            " 64484/100000: episode: 6133, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000417, mae: 2.909281, mean_q: 4.324074\n",
            " 64492/100000: episode: 6134, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000469, mae: 3.082959, mean_q: 4.550740\n",
            " 64501/100000: episode: 6135, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000549, mae: 2.895731, mean_q: 4.341811\n",
            " 64509/100000: episode: 6136, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000450, mae: 3.027676, mean_q: 4.482330\n",
            " 64517/100000: episode: 6137, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000331, mae: 3.096114, mean_q: 4.564765\n",
            " 64525/100000: episode: 6138, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000205, mae: 2.989810, mean_q: 4.443278\n",
            " 64533/100000: episode: 6139, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000112, mae: 3.057584, mean_q: 4.510188\n",
            " 64542/100000: episode: 6140, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000255, mae: 2.879411, mean_q: 4.309597\n",
            " 64550/100000: episode: 6141, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000333, mae: 3.023331, mean_q: 4.499283\n",
            " 64561/100000: episode: 6142, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000587, mae: 3.091339, mean_q: 4.566870\n",
            " 64572/100000: episode: 6143, duration: 0.105s, episode steps:  11, steps per second: 105, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.001259, mae: 2.906446, mean_q: 4.337814\n",
            " 64581/100000: episode: 6144, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.001047, mae: 3.071566, mean_q: 4.531870\n",
            " 64589/100000: episode: 6145, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000484, mae: 3.055880, mean_q: 4.503019\n",
            " 64598/100000: episode: 6146, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000336, mae: 2.854236, mean_q: 4.275299\n",
            " 64606/100000: episode: 6147, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000170, mae: 3.068111, mean_q: 4.535199\n",
            " 64616/100000: episode: 6148, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000121, mae: 2.938337, mean_q: 4.386457\n",
            " 64624/100000: episode: 6149, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000086, mae: 2.872262, mean_q: 4.300642\n",
            " 64632/100000: episode: 6150, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 3.048635, mean_q: 4.503682\n",
            " 64641/100000: episode: 6151, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000078, mae: 2.841568, mean_q: 4.263720\n",
            " 64649/100000: episode: 6152, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 2.993258, mean_q: 4.447606\n",
            " 64659/100000: episode: 6153, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.864346, mean_q: 4.304034\n",
            " 64668/100000: episode: 6154, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000040, mae: 2.712481, mean_q: 4.123028\n",
            " 64677/100000: episode: 6155, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000024, mae: 3.007324, mean_q: 4.464680\n",
            " 64686/100000: episode: 6156, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000033, mae: 3.104608, mean_q: 4.575201\n",
            " 64694/100000: episode: 6157, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.004303, mean_q: 4.461035\n",
            " 64703/100000: episode: 6158, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000011, mae: 3.319890, mean_q: 4.820365\n",
            " 64711/100000: episode: 6159, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.925224, mean_q: 4.372108\n",
            " 64720/100000: episode: 6160, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 3.064007, mean_q: 4.535102\n",
            " 64732/100000: episode: 6161, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.583 [0.000, 7.000],  loss: 0.000011, mae: 3.122035, mean_q: 4.596857\n",
            " 64743/100000: episode: 6162, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000002, mae: 2.881083, mean_q: 4.325626\n",
            " 64751/100000: episode: 6163, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.945661, mean_q: 4.398128\n",
            " 64759/100000: episode: 6164, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.939125, mean_q: 4.382904\n",
            " 64768/100000: episode: 6165, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000009, mae: 3.122650, mean_q: 4.591463\n",
            " 64777/100000: episode: 6166, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 2.933547, mean_q: 4.377375\n",
            " 64785/100000: episode: 6167, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.096972, mean_q: 4.567195\n",
            " 64793/100000: episode: 6168, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.768790, mean_q: 4.195587\n",
            " 64802/100000: episode: 6169, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000009, mae: 2.879969, mean_q: 4.324688\n",
            " 64810/100000: episode: 6170, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.950889, mean_q: 4.398108\n",
            " 64818/100000: episode: 6171, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.899729, mean_q: 4.339885\n",
            " 64828/100000: episode: 6172, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000011, mae: 2.952034, mean_q: 4.397161\n",
            " 64836/100000: episode: 6173, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.842926, mean_q: 4.277114\n",
            " 64844/100000: episode: 6174, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.917630, mean_q: 4.363894\n",
            " 64853/100000: episode: 6175, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 2.968142, mean_q: 4.423695\n",
            " 64861/100000: episode: 6176, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.054409, mean_q: 4.517875\n",
            " 64869/100000: episode: 6177, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.890115, mean_q: 4.323511\n",
            " 64877/100000: episode: 6178, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.980418, mean_q: 4.432639\n",
            " 64885/100000: episode: 6179, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.876955, mean_q: 4.311813\n",
            " 64893/100000: episode: 6180, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.087609, mean_q: 4.557173\n",
            " 64902/100000: episode: 6181, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000004, mae: 2.717506, mean_q: 4.131418\n",
            " 64910/100000: episode: 6182, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.823694, mean_q: 4.249510\n",
            " 64920/100000: episode: 6183, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000002, mae: 2.852047, mean_q: 4.283934\n",
            " 64928/100000: episode: 6184, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.930106, mean_q: 4.375693\n",
            " 64936/100000: episode: 6185, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.086660, mean_q: 4.557669\n",
            " 64944/100000: episode: 6186, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.004891, mean_q: 4.461699\n",
            " 64953/100000: episode: 6187, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 3.077729, mean_q: 4.546594\n",
            " 64965/100000: episode: 6188, duration: 0.128s, episode steps:  12, steps per second:  93, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 2.917 [0.000, 7.000],  loss: 0.000002, mae: 2.860838, mean_q: 4.292425\n",
            " 64973/100000: episode: 6189, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.826673, mean_q: 4.250754\n",
            " 64982/100000: episode: 6190, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 3.031161, mean_q: 4.486555\n",
            " 64990/100000: episode: 6191, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.850838, mean_q: 4.289805\n",
            " 64998/100000: episode: 6192, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.066855, mean_q: 4.530826\n",
            " 65006/100000: episode: 6193, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.926463, mean_q: 4.371960\n",
            " 65014/100000: episode: 6194, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.893270, mean_q: 4.332751\n",
            " 65022/100000: episode: 6195, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.173357, mean_q: 4.657441\n",
            " 65030/100000: episode: 6196, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.992200, mean_q: 4.450921\n",
            " 65038/100000: episode: 6197, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.870411, mean_q: 4.310553\n",
            " 65046/100000: episode: 6198, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.839166, mean_q: 4.266747\n",
            " 65056/100000: episode: 6199, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000001, mae: 3.184116, mean_q: 4.671611\n",
            " 65064/100000: episode: 6200, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.069171, mean_q: 4.540741\n",
            " 65073/100000: episode: 6201, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.063168, mean_q: 4.530989\n",
            " 65082/100000: episode: 6202, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.742888, mean_q: 4.158074\n",
            " 65091/100000: episode: 6203, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.999603, mean_q: 4.451497\n",
            " 65100/100000: episode: 6204, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 2.982471, mean_q: 4.435739\n",
            " 65109/100000: episode: 6205, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000006, mae: 3.136168, mean_q: 4.612978\n",
            " 65119/100000: episode: 6206, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000019, mae: 2.867283, mean_q: 4.303796\n",
            " 65127/100000: episode: 6207, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.822398, mean_q: 4.249055\n",
            " 65136/100000: episode: 6208, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 3.004906, mean_q: 4.463788\n",
            " 65144/100000: episode: 6209, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.088115, mean_q: 4.562273\n",
            " 65153/100000: episode: 6210, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000006, mae: 2.989545, mean_q: 4.443612\n",
            " 65162/100000: episode: 6211, duration: 0.090s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 3.046950, mean_q: 4.510180\n",
            " 65170/100000: episode: 6212, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.911250, mean_q: 4.357187\n",
            " 65178/100000: episode: 6213, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.086866, mean_q: 4.555385\n",
            " 65186/100000: episode: 6214, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.913850, mean_q: 4.357872\n",
            " 65197/100000: episode: 6215, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000003, mae: 2.909265, mean_q: 4.349646\n",
            " 65205/100000: episode: 6216, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.689625, mean_q: 4.100421\n",
            " 65213/100000: episode: 6217, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.964136, mean_q: 4.415660\n",
            " 65221/100000: episode: 6218, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.866576, mean_q: 4.299546\n",
            " 65229/100000: episode: 6219, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.790211, mean_q: 4.211703\n",
            " 65238/100000: episode: 6220, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 2.969356, mean_q: 4.420821\n",
            " 65246/100000: episode: 6221, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.997226, mean_q: 4.456246\n",
            " 65255/100000: episode: 6222, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.907096, mean_q: 4.348564\n",
            " 65263/100000: episode: 6223, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.904609, mean_q: 4.339695\n",
            " 65271/100000: episode: 6224, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.846910, mean_q: 4.272260\n",
            " 65279/100000: episode: 6225, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.994603, mean_q: 4.453414\n",
            " 65287/100000: episode: 6226, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.104055, mean_q: 4.578569\n",
            " 65296/100000: episode: 6227, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.947724, mean_q: 4.397171\n",
            " 65305/100000: episode: 6228, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.087664, mean_q: 4.557586\n",
            " 65316/100000: episode: 6229, duration: 0.261s, episode steps:  11, steps per second:  42, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000001, mae: 2.812661, mean_q: 4.237938\n",
            " 65325/100000: episode: 6230, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.210249, mean_q: 4.703537\n",
            " 65333/100000: episode: 6231, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.936106, mean_q: 4.382486\n",
            " 65341/100000: episode: 6232, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.862957, mean_q: 4.301289\n",
            " 65350/100000: episode: 6233, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.920918, mean_q: 4.369596\n",
            " 65360/100000: episode: 6234, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 2.994313, mean_q: 4.453854\n",
            " 65368/100000: episode: 6235, duration: 0.183s, episode steps:   8, steps per second:  44, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.896185, mean_q: 4.334864\n",
            " 65376/100000: episode: 6236, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.942280, mean_q: 4.393013\n",
            " 65385/100000: episode: 6237, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.913259, mean_q: 4.354968\n",
            " 65395/100000: episode: 6238, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000001, mae: 2.994859, mean_q: 4.450675\n",
            " 65403/100000: episode: 6239, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.965808, mean_q: 4.415820\n",
            " 65412/100000: episode: 6240, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.853461, mean_q: 4.286312\n",
            " 65420/100000: episode: 6241, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.170164, mean_q: 4.655172\n",
            " 65428/100000: episode: 6242, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.978691, mean_q: 4.426874\n",
            " 65439/100000: episode: 6243, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000001, mae: 2.967821, mean_q: 4.422793\n",
            " 65448/100000: episode: 6244, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.990106, mean_q: 4.449022\n",
            " 65456/100000: episode: 6245, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.830380, mean_q: 4.257147\n",
            " 65465/100000: episode: 6246, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000000, mae: 2.817641, mean_q: 4.245667\n",
            " 65474/100000: episode: 6247, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 3.157637, mean_q: 4.639387\n",
            " 65482/100000: episode: 6248, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.968585, mean_q: 4.418380\n",
            " 65492/100000: episode: 6249, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000001, mae: 3.052283, mean_q: 4.513359\n",
            " 65501/100000: episode: 6250, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 2.758711, mean_q: 4.175468\n",
            " 65509/100000: episode: 6251, duration: 0.087s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.054958, mean_q: 4.519361\n",
            " 65518/100000: episode: 6252, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 3.008605, mean_q: 4.467446\n",
            " 65526/100000: episode: 6253, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.023857, mean_q: 4.480301\n",
            " 65534/100000: episode: 6254, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.797432, mean_q: 4.226374\n",
            " 65542/100000: episode: 6255, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.937490, mean_q: 4.382262\n",
            " 65551/100000: episode: 6256, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.043874, mean_q: 4.507002\n",
            " 65560/100000: episode: 6257, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.962701, mean_q: 4.417293\n",
            " 65568/100000: episode: 6258, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.842876, mean_q: 4.277884\n",
            " 65576/100000: episode: 6259, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.957744, mean_q: 4.411071\n",
            " 65585/100000: episode: 6260, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 3.061667, mean_q: 4.527970\n",
            " 65593/100000: episode: 6261, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.011272, mean_q: 4.469069\n",
            " 65601/100000: episode: 6262, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.006757, mean_q: 4.465706\n",
            " 65609/100000: episode: 6263, duration: 0.086s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.896905, mean_q: 4.340168\n",
            " 65617/100000: episode: 6264, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.090505, mean_q: 4.560458\n",
            " 65625/100000: episode: 6265, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.928520, mean_q: 4.370544\n",
            " 65634/100000: episode: 6266, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.908509, mean_q: 4.350383\n",
            " 65643/100000: episode: 6267, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000010, mae: 2.823352, mean_q: 4.254502\n",
            " 65651/100000: episode: 6268, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.000978, mean_q: 4.456453\n",
            " 65660/100000: episode: 6269, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000010, mae: 2.823243, mean_q: 4.252307\n",
            " 65669/100000: episode: 6270, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000012, mae: 2.740919, mean_q: 4.159309\n",
            " 65678/100000: episode: 6271, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000012, mae: 2.822659, mean_q: 4.248034\n",
            " 65687/100000: episode: 6272, duration: 0.099s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000014, mae: 2.956119, mean_q: 4.399753\n",
            " 65696/100000: episode: 6273, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000023, mae: 3.001766, mean_q: 4.460020\n",
            " 65705/100000: episode: 6274, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000044, mae: 2.976823, mean_q: 4.440439\n",
            " 65714/100000: episode: 6275, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000134, mae: 3.049176, mean_q: 4.514962\n",
            " 65723/100000: episode: 6276, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000164, mae: 2.889651, mean_q: 4.315563\n",
            " 65732/100000: episode: 6277, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000193, mae: 2.931933, mean_q: 4.363786\n",
            " 65740/100000: episode: 6278, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000185, mae: 3.129212, mean_q: 4.601505\n",
            " 65749/100000: episode: 6279, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000367, mae: 3.067140, mean_q: 4.529314\n",
            " 65757/100000: episode: 6280, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000429, mae: 3.049628, mean_q: 4.502669\n",
            " 65767/100000: episode: 6281, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000540, mae: 3.025890, mean_q: 4.480155\n",
            " 65777/100000: episode: 6282, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000373, mae: 2.916862, mean_q: 4.351762\n",
            " 65786/100000: episode: 6283, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000283, mae: 2.944131, mean_q: 4.389670\n",
            " 65795/100000: episode: 6284, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000140, mae: 2.820360, mean_q: 4.247115\n",
            " 65805/100000: episode: 6285, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000147, mae: 3.039469, mean_q: 4.488597\n",
            " 65815/100000: episode: 6286, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000317, mae: 2.962807, mean_q: 4.401628\n",
            " 65823/100000: episode: 6287, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000254, mae: 2.871405, mean_q: 4.301193\n",
            " 65831/100000: episode: 6288, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000202, mae: 2.910466, mean_q: 4.351752\n",
            " 65839/100000: episode: 6289, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000214, mae: 2.875819, mean_q: 4.306772\n",
            " 65847/100000: episode: 6290, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000207, mae: 3.015183, mean_q: 4.469657\n",
            " 65855/100000: episode: 6291, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000124, mae: 2.993139, mean_q: 4.444484\n",
            " 65863/100000: episode: 6292, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 3.000495, mean_q: 4.461820\n",
            " 65871/100000: episode: 6293, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 3.026457, mean_q: 4.488063\n",
            " 65880/100000: episode: 6294, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000066, mae: 2.963265, mean_q: 4.404734\n",
            " 65888/100000: episode: 6295, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.016538, mean_q: 4.471226\n",
            " 65897/100000: episode: 6296, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000019, mae: 2.880721, mean_q: 4.319268\n",
            " 65906/100000: episode: 6297, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000020, mae: 2.984291, mean_q: 4.437801\n",
            " 65915/100000: episode: 6298, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000027, mae: 2.831682, mean_q: 4.259118\n",
            " 65923/100000: episode: 6299, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.059263, mean_q: 4.524323\n",
            " 65931/100000: episode: 6300, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.991866, mean_q: 4.451357\n",
            " 65939/100000: episode: 6301, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.966472, mean_q: 4.421947\n",
            " 65948/100000: episode: 6302, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 2.903960, mean_q: 4.345804\n",
            " 65957/100000: episode: 6303, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000013, mae: 2.934385, mean_q: 4.385173\n",
            " 65966/100000: episode: 6304, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.960626, mean_q: 4.415693\n",
            " 65975/100000: episode: 6305, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000020, mae: 3.055826, mean_q: 4.520327\n",
            " 65984/100000: episode: 6306, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 2.722840, mean_q: 4.132142\n",
            " 65992/100000: episode: 6307, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.215956, mean_q: 4.704123\n",
            " 66000/100000: episode: 6308, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.914951, mean_q: 4.363132\n",
            " 66008/100000: episode: 6309, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.908329, mean_q: 4.351293\n",
            " 66016/100000: episode: 6310, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.907579, mean_q: 4.346293\n",
            " 66024/100000: episode: 6311, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.701976, mean_q: 4.112081\n",
            " 66033/100000: episode: 6312, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 2.593199, mean_q: 3.983453\n",
            " 66041/100000: episode: 6313, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.027160, mean_q: 4.485056\n",
            " 66050/100000: episode: 6314, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.833637, mean_q: 4.260500\n",
            " 66059/100000: episode: 6315, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.093833, mean_q: 4.563623\n",
            " 66068/100000: episode: 6316, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000007, mae: 2.816343, mean_q: 4.247891\n",
            " 66076/100000: episode: 6317, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.807556, mean_q: 4.233597\n",
            " 66084/100000: episode: 6318, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.900799, mean_q: 4.342442\n",
            " 66092/100000: episode: 6319, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.058043, mean_q: 4.524912\n",
            " 66103/100000: episode: 6320, duration: 0.154s, episode steps:  11, steps per second:  71, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.909 [0.000, 7.000],  loss: 0.000007, mae: 3.012164, mean_q: 4.472480\n",
            " 66112/100000: episode: 6321, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 3.055116, mean_q: 4.523065\n",
            " 66122/100000: episode: 6322, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000005, mae: 2.987763, mean_q: 4.436855\n",
            " 66130/100000: episode: 6323, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.898626, mean_q: 4.340693\n",
            " 66138/100000: episode: 6324, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.875894, mean_q: 4.313374\n",
            " 66147/100000: episode: 6325, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000015, mae: 2.859178, mean_q: 4.293325\n",
            " 66155/100000: episode: 6326, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.083909, mean_q: 4.551102\n",
            " 66165/100000: episode: 6327, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000009, mae: 2.859013, mean_q: 4.293718\n",
            " 66173/100000: episode: 6328, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.023599, mean_q: 4.482589\n",
            " 66181/100000: episode: 6329, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.133015, mean_q: 4.611551\n",
            " 66190/100000: episode: 6330, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 3.006047, mean_q: 4.459721\n",
            " 66198/100000: episode: 6331, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.076971, mean_q: 4.549541\n",
            " 66210/100000: episode: 6332, duration: 0.131s, episode steps:  12, steps per second:  91, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000020, mae: 3.038418, mean_q: 4.500884\n",
            " 66218/100000: episode: 6333, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.088716, mean_q: 4.552978\n",
            " 66226/100000: episode: 6334, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.918106, mean_q: 4.365067\n",
            " 66236/100000: episode: 6335, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000016, mae: 2.927721, mean_q: 4.372942\n",
            " 66246/100000: episode: 6336, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000020, mae: 2.925771, mean_q: 4.366359\n",
            " 66254/100000: episode: 6337, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.841790, mean_q: 4.278680\n",
            " 66262/100000: episode: 6338, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.894767, mean_q: 4.331893\n",
            " 66272/100000: episode: 6339, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000008, mae: 2.903165, mean_q: 4.348582\n",
            " 66280/100000: episode: 6340, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.047264, mean_q: 4.512833\n",
            " 66289/100000: episode: 6341, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000010, mae: 2.956543, mean_q: 4.407378\n",
            " 66299/100000: episode: 6342, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000009, mae: 3.069679, mean_q: 4.536892\n",
            " 66307/100000: episode: 6343, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.049559, mean_q: 4.515837\n",
            " 66317/100000: episode: 6344, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000007, mae: 2.847539, mean_q: 4.284822\n",
            " 66326/100000: episode: 6345, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.935229, mean_q: 4.385508\n",
            " 66334/100000: episode: 6346, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.917825, mean_q: 4.358230\n",
            " 66342/100000: episode: 6347, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.872192, mean_q: 4.305848\n",
            " 66351/100000: episode: 6348, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000022, mae: 2.999823, mean_q: 4.453053\n",
            " 66360/100000: episode: 6349, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 3.068735, mean_q: 4.537268\n",
            " 66368/100000: episode: 6350, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.983889, mean_q: 4.437823\n",
            " 66378/100000: episode: 6351, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000015, mae: 2.999128, mean_q: 4.452748\n",
            " 66387/100000: episode: 6352, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000012, mae: 2.998804, mean_q: 4.454256\n",
            " 66395/100000: episode: 6353, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.747811, mean_q: 4.158313\n",
            " 66403/100000: episode: 6354, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.906091, mean_q: 4.350935\n",
            " 66411/100000: episode: 6355, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.881975, mean_q: 4.324835\n",
            " 66419/100000: episode: 6356, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.142669, mean_q: 4.618719\n",
            " 66428/100000: episode: 6357, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.876022, mean_q: 4.312223\n",
            " 66436/100000: episode: 6358, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.694436, mean_q: 4.103478\n",
            " 66444/100000: episode: 6359, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.919038, mean_q: 4.361987\n",
            " 66452/100000: episode: 6360, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.926885, mean_q: 4.369525\n",
            " 66460/100000: episode: 6361, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.966477, mean_q: 4.420312\n",
            " 66469/100000: episode: 6362, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000019, mae: 3.030276, mean_q: 4.490053\n",
            " 66479/100000: episode: 6363, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000018, mae: 2.865242, mean_q: 4.302031\n",
            " 66487/100000: episode: 6364, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.807187, mean_q: 4.234399\n",
            " 66495/100000: episode: 6365, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.985316, mean_q: 4.439939\n",
            " 66503/100000: episode: 6366, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.021352, mean_q: 4.477950\n",
            " 66513/100000: episode: 6367, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000026, mae: 2.957456, mean_q: 4.404214\n",
            " 66521/100000: episode: 6368, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.970186, mean_q: 4.422083\n",
            " 66530/100000: episode: 6369, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000018, mae: 2.935511, mean_q: 4.381414\n",
            " 66538/100000: episode: 6370, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 3.163071, mean_q: 4.646738\n",
            " 66547/100000: episode: 6371, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000024, mae: 2.977207, mean_q: 4.428503\n",
            " 66556/100000: episode: 6372, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000016, mae: 2.882185, mean_q: 4.321193\n",
            " 66564/100000: episode: 6373, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.985291, mean_q: 4.442565\n",
            " 66572/100000: episode: 6374, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000060, mae: 2.914598, mean_q: 4.350009\n",
            " 66582/100000: episode: 6375, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000089, mae: 2.956970, mean_q: 4.403045\n",
            " 66591/100000: episode: 6376, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000114, mae: 2.972857, mean_q: 4.424757\n",
            " 66599/100000: episode: 6377, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000121, mae: 2.837092, mean_q: 4.258001\n",
            " 66607/100000: episode: 6378, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000145, mae: 3.162292, mean_q: 4.640125\n",
            " 66615/100000: episode: 6379, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000216, mae: 3.134397, mean_q: 4.611265\n",
            " 66624/100000: episode: 6380, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000408, mae: 2.988074, mean_q: 4.429613\n",
            " 66632/100000: episode: 6381, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000146, mae: 3.017208, mean_q: 4.460363\n",
            " 66640/100000: episode: 6382, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000265, mae: 2.952509, mean_q: 4.388398\n",
            " 66649/100000: episode: 6383, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000167, mae: 2.770559, mean_q: 4.189438\n",
            " 66657/100000: episode: 6384, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000269, mae: 3.041658, mean_q: 4.494255\n",
            " 66665/100000: episode: 6385, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000110, mae: 3.147989, mean_q: 4.623360\n",
            " 66674/100000: episode: 6386, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000228, mae: 2.820334, mean_q: 4.246285\n",
            " 66682/100000: episode: 6387, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000187, mae: 3.029999, mean_q: 4.485847\n",
            " 66690/100000: episode: 6388, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000160, mae: 3.033243, mean_q: 4.485713\n",
            " 66700/100000: episode: 6389, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000115, mae: 2.925973, mean_q: 4.367009\n",
            " 66709/100000: episode: 6390, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000102, mae: 3.067156, mean_q: 4.525149\n",
            " 66718/100000: episode: 6391, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000099, mae: 3.023408, mean_q: 4.487200\n",
            " 66726/100000: episode: 6392, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000104, mae: 2.826282, mean_q: 4.254135\n",
            " 66736/100000: episode: 6393, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000083, mae: 3.103492, mean_q: 4.570421\n",
            " 66744/100000: episode: 6394, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 3.066349, mean_q: 4.526795\n",
            " 66752/100000: episode: 6395, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.951697, mean_q: 4.406686\n",
            " 66761/100000: episode: 6396, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000052, mae: 2.911372, mean_q: 4.355316\n",
            " 66770/100000: episode: 6397, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000070, mae: 2.745656, mean_q: 4.163684\n",
            " 66781/100000: episode: 6398, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000052, mae: 2.762528, mean_q: 4.178520\n",
            " 66789/100000: episode: 6399, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000095, mae: 2.906536, mean_q: 4.342844\n",
            " 66797/100000: episode: 6400, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000129, mae: 2.959927, mean_q: 4.409838\n",
            " 66807/100000: episode: 6401, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000049, mae: 2.897922, mean_q: 4.339568\n",
            " 66817/100000: episode: 6402, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000031, mae: 2.834420, mean_q: 4.260510\n",
            " 66826/100000: episode: 6403, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000053, mae: 2.901139, mean_q: 4.340953\n",
            " 66834/100000: episode: 6404, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 3.107296, mean_q: 4.590036\n",
            " 66842/100000: episode: 6405, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 3.033825, mean_q: 4.491898\n",
            " 66851/100000: episode: 6406, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000228, mae: 2.910669, mean_q: 4.349797\n",
            " 66859/100000: episode: 6407, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000294, mae: 2.797858, mean_q: 4.211305\n",
            " 66867/100000: episode: 6408, duration: 0.101s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000122, mae: 3.040860, mean_q: 4.495751\n",
            " 66875/100000: episode: 6409, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000137, mae: 3.046910, mean_q: 4.505658\n",
            " 66884/100000: episode: 6410, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000129, mae: 2.916873, mean_q: 4.364912\n",
            " 66892/100000: episode: 6411, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000166, mae: 2.883165, mean_q: 4.307149\n",
            " 66900/100000: episode: 6412, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000246, mae: 2.949035, mean_q: 4.397832\n",
            " 66909/100000: episode: 6413, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000240, mae: 2.826701, mean_q: 4.252057\n",
            " 66917/100000: episode: 6414, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000146, mae: 3.030342, mean_q: 4.485855\n",
            " 66925/100000: episode: 6415, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000185, mae: 2.838880, mean_q: 4.260403\n",
            " 66933/100000: episode: 6416, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000264, mae: 2.839756, mean_q: 4.269804\n",
            " 66941/100000: episode: 6417, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000164, mae: 2.950275, mean_q: 4.397890\n",
            " 66949/100000: episode: 6418, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 2.985141, mean_q: 4.427049\n",
            " 66960/100000: episode: 6419, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000111, mae: 3.003563, mean_q: 4.458686\n",
            " 66969/100000: episode: 6420, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000069, mae: 2.934123, mean_q: 4.382903\n",
            " 66977/100000: episode: 6421, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.893186, mean_q: 4.328142\n",
            " 66986/100000: episode: 6422, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000010, mae: 3.028943, mean_q: 4.492383\n",
            " 66995/100000: episode: 6423, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000018, mae: 2.976810, mean_q: 4.428953\n",
            " 67003/100000: episode: 6424, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.079619, mean_q: 4.542821\n",
            " 67012/100000: episode: 6425, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000006, mae: 3.172866, mean_q: 4.653259\n",
            " 67021/100000: episode: 6426, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000014, mae: 2.695531, mean_q: 4.101738\n",
            " 67029/100000: episode: 6427, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.986482, mean_q: 4.442232\n",
            " 67040/100000: episode: 6428, duration: 0.106s, episode steps:  11, steps per second: 104, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000006, mae: 2.948480, mean_q: 4.396675\n",
            " 67050/100000: episode: 6429, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000015, mae: 3.080814, mean_q: 4.545861\n",
            " 67058/100000: episode: 6430, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.918857, mean_q: 4.359225\n",
            " 67069/100000: episode: 6431, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000014, mae: 2.884200, mean_q: 4.322807\n",
            " 67079/100000: episode: 6432, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000012, mae: 2.940037, mean_q: 4.385798\n",
            " 67087/100000: episode: 6433, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.073221, mean_q: 4.547525\n",
            " 67096/100000: episode: 6434, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 2.987122, mean_q: 4.446935\n",
            " 67104/100000: episode: 6435, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.874895, mean_q: 4.316134\n",
            " 67115/100000: episode: 6436, duration: 0.118s, episode steps:  11, steps per second:  93, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000005, mae: 3.035829, mean_q: 4.497970\n",
            " 67123/100000: episode: 6437, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.831827, mean_q: 4.261315\n",
            " 67132/100000: episode: 6438, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000009, mae: 2.781033, mean_q: 4.209028\n",
            " 67141/100000: episode: 6439, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 2.979645, mean_q: 4.435260\n",
            " 67152/100000: episode: 6440, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000007, mae: 2.979747, mean_q: 4.430494\n",
            " 67160/100000: episode: 6441, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.765652, mean_q: 4.185336\n",
            " 67169/100000: episode: 6442, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000007, mae: 2.900735, mean_q: 4.347519\n",
            " 67178/100000: episode: 6443, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 2.901937, mean_q: 4.344717\n",
            " 67186/100000: episode: 6444, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.954151, mean_q: 4.399377\n",
            " 67194/100000: episode: 6445, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.996832, mean_q: 4.449460\n",
            " 67202/100000: episode: 6446, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.952582, mean_q: 4.406644\n",
            " 67211/100000: episode: 6447, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000014, mae: 2.859424, mean_q: 4.298244\n",
            " 67220/100000: episode: 6448, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 3.083205, mean_q: 4.550442\n",
            " 67228/100000: episode: 6449, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.062672, mean_q: 4.528259\n",
            " 67237/100000: episode: 6450, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000010, mae: 2.979928, mean_q: 4.433155\n",
            " 67246/100000: episode: 6451, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000011, mae: 2.916149, mean_q: 4.359846\n",
            " 67254/100000: episode: 6452, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.053077, mean_q: 4.515922\n",
            " 67263/100000: episode: 6453, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000015, mae: 3.106776, mean_q: 4.582706\n",
            " 67273/100000: episode: 6454, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000008, mae: 3.058306, mean_q: 4.523079\n",
            " 67281/100000: episode: 6455, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.129120, mean_q: 4.605809\n",
            " 67289/100000: episode: 6456, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.817071, mean_q: 4.244895\n",
            " 67298/100000: episode: 6457, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 3.149720, mean_q: 4.633869\n",
            " 67308/100000: episode: 6458, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.139080, mean_q: 4.616746\n",
            " 67317/100000: episode: 6459, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000027, mae: 2.986834, mean_q: 4.443472\n",
            " 67327/100000: episode: 6460, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000019, mae: 3.021468, mean_q: 4.478556\n",
            " 67338/100000: episode: 6461, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000017, mae: 2.989151, mean_q: 4.442748\n",
            " 67347/100000: episode: 6462, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000011, mae: 2.956521, mean_q: 4.412131\n",
            " 67355/100000: episode: 6463, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.768955, mean_q: 4.186214\n",
            " 67364/100000: episode: 6464, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 3.009753, mean_q: 4.468074\n",
            " 67374/100000: episode: 6465, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000004, mae: 2.807551, mean_q: 4.232942\n",
            " 67385/100000: episode: 6466, duration: 0.118s, episode steps:  11, steps per second:  93, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000005, mae: 3.040918, mean_q: 4.500405\n",
            " 67393/100000: episode: 6467, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.109328, mean_q: 4.579652\n",
            " 67401/100000: episode: 6468, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.969471, mean_q: 4.420172\n",
            " 67409/100000: episode: 6469, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.990727, mean_q: 4.446363\n",
            " 67417/100000: episode: 6470, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.902546, mean_q: 4.340814\n",
            " 67425/100000: episode: 6471, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.198921, mean_q: 4.687600\n",
            " 67433/100000: episode: 6472, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.832226, mean_q: 4.264276\n",
            " 67442/100000: episode: 6473, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.993957, mean_q: 4.450255\n",
            " 67451/100000: episode: 6474, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.063334, mean_q: 4.532733\n",
            " 67459/100000: episode: 6475, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.038321, mean_q: 4.500624\n",
            " 67468/100000: episode: 6476, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.926094, mean_q: 4.369620\n",
            " 67476/100000: episode: 6477, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.873232, mean_q: 4.311040\n",
            " 67484/100000: episode: 6478, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.031642, mean_q: 4.495387\n",
            " 67493/100000: episode: 6479, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.862602, mean_q: 4.294248\n",
            " 67501/100000: episode: 6480, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.964069, mean_q: 4.414235\n",
            " 67512/100000: episode: 6481, duration: 0.211s, episode steps:  11, steps per second:  52, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000009, mae: 3.104935, mean_q: 4.580082\n",
            " 67521/100000: episode: 6482, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 3.094826, mean_q: 4.563294\n",
            " 67529/100000: episode: 6483, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.092525, mean_q: 4.568874\n",
            " 67538/100000: episode: 6484, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 2.993771, mean_q: 4.450219\n",
            " 67547/100000: episode: 6485, duration: 0.154s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000009, mae: 2.972230, mean_q: 4.424108\n",
            " 67558/100000: episode: 6486, duration: 0.169s, episode steps:  11, steps per second:  65, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000008, mae: 2.869528, mean_q: 4.306955\n",
            " 67566/100000: episode: 6487, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.897691, mean_q: 4.337276\n",
            " 67576/100000: episode: 6488, duration: 0.148s, episode steps:  10, steps per second:  68, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000007, mae: 3.063307, mean_q: 4.527675\n",
            " 67584/100000: episode: 6489, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.760221, mean_q: 4.179684\n",
            " 67594/100000: episode: 6490, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000038, mae: 2.898815, mean_q: 4.338686\n",
            " 67603/100000: episode: 6491, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000025, mae: 3.022975, mean_q: 4.483677\n",
            " 67613/100000: episode: 6492, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.092891, mean_q: 4.561166\n",
            " 67621/100000: episode: 6493, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 3.003565, mean_q: 4.460708\n",
            " 67629/100000: episode: 6494, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.956181, mean_q: 4.402685\n",
            " 67639/100000: episode: 6495, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000073, mae: 2.963647, mean_q: 4.411501\n",
            " 67647/100000: episode: 6496, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000076, mae: 2.990663, mean_q: 4.441262\n",
            " 67655/100000: episode: 6497, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000148, mae: 2.983005, mean_q: 4.431561\n",
            " 67663/100000: episode: 6498, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 3.009645, mean_q: 4.462641\n",
            " 67671/100000: episode: 6499, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 2.738873, mean_q: 4.155624\n",
            " 67681/100000: episode: 6500, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000055, mae: 2.891406, mean_q: 4.330444\n",
            " 67689/100000: episode: 6501, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 3.180789, mean_q: 4.669020\n",
            " 67699/100000: episode: 6502, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000148, mae: 3.061987, mean_q: 4.534678\n",
            " 67707/100000: episode: 6503, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000170, mae: 3.028839, mean_q: 4.491840\n",
            " 67715/100000: episode: 6504, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000143, mae: 2.971272, mean_q: 4.416811\n",
            " 67724/100000: episode: 6505, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000084, mae: 3.071023, mean_q: 4.531401\n",
            " 67734/100000: episode: 6506, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000137, mae: 2.954112, mean_q: 4.393148\n",
            " 67742/100000: episode: 6507, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000119, mae: 2.828282, mean_q: 4.239975\n",
            " 67751/100000: episode: 6508, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000179, mae: 2.821681, mean_q: 4.241515\n",
            " 67759/100000: episode: 6509, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000288, mae: 2.819487, mean_q: 4.233035\n",
            " 67768/100000: episode: 6510, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000324, mae: 2.860787, mean_q: 4.267066\n",
            " 67776/100000: episode: 6511, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000239, mae: 3.190089, mean_q: 4.664824\n",
            " 67785/100000: episode: 6512, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000539, mae: 2.963403, mean_q: 4.390853\n",
            " 67793/100000: episode: 6513, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000442, mae: 2.923286, mean_q: 4.351006\n",
            " 67802/100000: episode: 6514, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000403, mae: 2.940518, mean_q: 4.366927\n",
            " 67811/100000: episode: 6515, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000289, mae: 3.081533, mean_q: 4.536659\n",
            " 67819/100000: episode: 6516, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000134, mae: 2.890242, mean_q: 4.315581\n",
            " 67827/100000: episode: 6517, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000115, mae: 2.931178, mean_q: 4.370730\n",
            " 67837/100000: episode: 6518, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000199, mae: 3.116052, mean_q: 4.578557\n",
            " 67846/100000: episode: 6519, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000162, mae: 2.933934, mean_q: 4.355497\n",
            " 67854/100000: episode: 6520, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000117, mae: 2.828322, mean_q: 4.245675\n",
            " 67862/100000: episode: 6521, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000147, mae: 3.162893, mean_q: 4.641098\n",
            " 67870/100000: episode: 6522, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000227, mae: 2.911185, mean_q: 4.356301\n",
            " 67878/100000: episode: 6523, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000121, mae: 2.925584, mean_q: 4.363480\n",
            " 67886/100000: episode: 6524, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000122, mae: 2.905715, mean_q: 4.345189\n",
            " 67895/100000: episode: 6525, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000147, mae: 2.795733, mean_q: 4.202768\n",
            " 67903/100000: episode: 6526, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000097, mae: 2.969851, mean_q: 4.414629\n",
            " 67911/100000: episode: 6527, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 3.043077, mean_q: 4.507921\n",
            " 67920/100000: episode: 6528, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000064, mae: 2.983125, mean_q: 4.432123\n",
            " 67928/100000: episode: 6529, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000081, mae: 3.002549, mean_q: 4.451647\n",
            " 67936/100000: episode: 6530, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 3.158741, mean_q: 4.644135\n",
            " 67945/100000: episode: 6531, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000039, mae: 2.853345, mean_q: 4.289412\n",
            " 67953/100000: episode: 6532, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.013037, mean_q: 4.470809\n",
            " 67962/100000: episode: 6533, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000031, mae: 2.753814, mean_q: 4.172603\n",
            " 67971/100000: episode: 6534, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000028, mae: 2.851372, mean_q: 4.287512\n",
            " 67982/100000: episode: 6535, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000014, mae: 2.807025, mean_q: 4.232987\n",
            " 67991/100000: episode: 6536, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 2.887937, mean_q: 4.329107\n",
            " 68000/100000: episode: 6537, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.910546, mean_q: 4.353863\n",
            " 68011/100000: episode: 6538, duration: 0.103s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000016, mae: 3.003729, mean_q: 4.459646\n",
            " 68021/100000: episode: 6539, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000010, mae: 3.005232, mean_q: 4.465462\n",
            " 68029/100000: episode: 6540, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.860126, mean_q: 4.298143\n",
            " 68037/100000: episode: 6541, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.087344, mean_q: 4.561282\n",
            " 68046/100000: episode: 6542, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.939856, mean_q: 4.389064\n",
            " 68055/100000: episode: 6543, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 2.904546, mean_q: 4.343897\n",
            " 68065/100000: episode: 6544, duration: 0.146s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000009, mae: 2.855647, mean_q: 4.288249\n",
            " 68075/100000: episode: 6545, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.045706, mean_q: 4.512392\n",
            " 68083/100000: episode: 6546, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.700822, mean_q: 4.117646\n",
            " 68091/100000: episode: 6547, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.781862, mean_q: 4.209617\n",
            " 68099/100000: episode: 6548, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.772650, mean_q: 4.195851\n",
            " 68108/100000: episode: 6549, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000011, mae: 3.048404, mean_q: 4.512389\n",
            " 68116/100000: episode: 6550, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.960663, mean_q: 4.407063\n",
            " 68124/100000: episode: 6551, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.933021, mean_q: 4.379147\n",
            " 68132/100000: episode: 6552, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.875468, mean_q: 4.310388\n",
            " 68141/100000: episode: 6553, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 3.066219, mean_q: 4.533451\n",
            " 68150/100000: episode: 6554, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 2.855999, mean_q: 4.296048\n",
            " 68158/100000: episode: 6555, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.208176, mean_q: 4.695268\n",
            " 68166/100000: episode: 6556, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.872823, mean_q: 4.318221\n",
            " 68174/100000: episode: 6557, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.894846, mean_q: 4.336372\n",
            " 68182/100000: episode: 6558, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.792373, mean_q: 4.213185\n",
            " 68191/100000: episode: 6559, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000013, mae: 2.852777, mean_q: 4.284750\n",
            " 68200/100000: episode: 6560, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000006, mae: 3.052128, mean_q: 4.515081\n",
            " 68208/100000: episode: 6561, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.067322, mean_q: 4.532593\n",
            " 68217/100000: episode: 6562, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.819117, mean_q: 4.251217\n",
            " 68227/100000: episode: 6563, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000006, mae: 2.960345, mean_q: 4.410371\n",
            " 68235/100000: episode: 6564, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.817663, mean_q: 4.243996\n",
            " 68244/100000: episode: 6565, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000015, mae: 2.953808, mean_q: 4.406269\n",
            " 68252/100000: episode: 6566, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.088939, mean_q: 4.558782\n",
            " 68260/100000: episode: 6567, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.663615, mean_q: 4.070504\n",
            " 68268/100000: episode: 6568, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.101155, mean_q: 4.576041\n",
            " 68276/100000: episode: 6569, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.793667, mean_q: 4.218441\n",
            " 68285/100000: episode: 6570, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 3.069743, mean_q: 4.537808\n",
            " 68295/100000: episode: 6571, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000004, mae: 2.882165, mean_q: 4.320838\n",
            " 68304/100000: episode: 6572, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 3.050043, mean_q: 4.519173\n",
            " 68312/100000: episode: 6573, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.951294, mean_q: 4.397688\n",
            " 68320/100000: episode: 6574, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.957669, mean_q: 4.403512\n",
            " 68330/100000: episode: 6575, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.730299, mean_q: 4.141288\n",
            " 68339/100000: episode: 6576, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000037, mae: 2.921984, mean_q: 4.365533\n",
            " 68348/100000: episode: 6577, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000101, mae: 3.038936, mean_q: 4.503094\n",
            " 68356/100000: episode: 6578, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 2.983394, mean_q: 4.434847\n",
            " 68365/100000: episode: 6579, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000083, mae: 3.053783, mean_q: 4.511961\n",
            " 68374/100000: episode: 6580, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000052, mae: 2.884029, mean_q: 4.320151\n",
            " 68383/100000: episode: 6581, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000038, mae: 3.118616, mean_q: 4.588882\n",
            " 68392/100000: episode: 6582, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000020, mae: 2.826628, mean_q: 4.252935\n",
            " 68400/100000: episode: 6583, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.914268, mean_q: 4.357768\n",
            " 68408/100000: episode: 6584, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 3.103183, mean_q: 4.575235\n",
            " 68416/100000: episode: 6585, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.930705, mean_q: 4.379919\n",
            " 68424/100000: episode: 6586, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 2.897224, mean_q: 4.338590\n",
            " 68432/100000: episode: 6587, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 3.078098, mean_q: 4.542143\n",
            " 68440/100000: episode: 6588, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.944663, mean_q: 4.395508\n",
            " 68448/100000: episode: 6589, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.987829, mean_q: 4.446509\n",
            " 68456/100000: episode: 6590, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.106487, mean_q: 4.581523\n",
            " 68464/100000: episode: 6591, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.925656, mean_q: 4.373195\n",
            " 68472/100000: episode: 6592, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.882827, mean_q: 4.324926\n",
            " 68482/100000: episode: 6593, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000039, mae: 2.896888, mean_q: 4.332858\n",
            " 68490/100000: episode: 6594, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.083802, mean_q: 4.553023\n",
            " 68500/100000: episode: 6595, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000018, mae: 2.958004, mean_q: 4.407351\n",
            " 68509/100000: episode: 6596, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000028, mae: 3.090014, mean_q: 4.555918\n",
            " 68517/100000: episode: 6597, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.748721, mean_q: 4.167976\n",
            " 68526/100000: episode: 6598, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000029, mae: 3.058146, mean_q: 4.525777\n",
            " 68534/100000: episode: 6599, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.124780, mean_q: 4.598706\n",
            " 68544/100000: episode: 6600, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000023, mae: 3.111661, mean_q: 4.583022\n",
            " 68554/100000: episode: 6601, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 3.136823, mean_q: 4.611712\n",
            " 68562/100000: episode: 6602, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.809300, mean_q: 4.224878\n",
            " 68572/100000: episode: 6603, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.772279, mean_q: 4.200408\n",
            " 68582/100000: episode: 6604, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000096, mae: 3.117156, mean_q: 4.594056\n",
            " 68591/100000: episode: 6605, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000088, mae: 2.891490, mean_q: 4.316301\n",
            " 68599/100000: episode: 6606, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000138, mae: 2.927605, mean_q: 4.365913\n",
            " 68608/100000: episode: 6607, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000119, mae: 2.922154, mean_q: 4.373918\n",
            " 68619/100000: episode: 6608, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000098, mae: 2.896457, mean_q: 4.334527\n",
            " 68627/100000: episode: 6609, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000146, mae: 2.892100, mean_q: 4.334837\n",
            " 68635/100000: episode: 6610, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000392, mae: 3.162311, mean_q: 4.631358\n",
            " 68644/100000: episode: 6611, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000401, mae: 2.908232, mean_q: 4.330029\n",
            " 68652/100000: episode: 6612, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000281, mae: 3.069194, mean_q: 4.529015\n",
            " 68660/100000: episode: 6613, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 2.963304, mean_q: 4.408815\n",
            " 68668/100000: episode: 6614, duration: 0.162s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000128, mae: 2.883358, mean_q: 4.298105\n",
            " 68677/100000: episode: 6615, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000489, mae: 3.008917, mean_q: 4.452551\n",
            " 68686/100000: episode: 6616, duration: 0.175s, episode steps:   9, steps per second:  51, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000414, mae: 3.107006, mean_q: 4.581886\n",
            " 68697/100000: episode: 6617, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000355, mae: 3.057447, mean_q: 4.522983\n",
            " 68705/100000: episode: 6618, duration: 0.155s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000164, mae: 3.082783, mean_q: 4.550522\n",
            " 68714/100000: episode: 6619, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000117, mae: 3.016277, mean_q: 4.474957\n",
            " 68723/100000: episode: 6620, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000078, mae: 3.002387, mean_q: 4.461841\n",
            " 68734/100000: episode: 6621, duration: 0.160s, episode steps:  11, steps per second:  69, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000060, mae: 2.863565, mean_q: 4.298784\n",
            " 68743/100000: episode: 6622, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000057, mae: 3.008805, mean_q: 4.467064\n",
            " 68751/100000: episode: 6623, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 3.050294, mean_q: 4.512419\n",
            " 68759/100000: episode: 6624, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 3.051507, mean_q: 4.511034\n",
            " 68769/100000: episode: 6625, duration: 0.153s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000068, mae: 3.099939, mean_q: 4.576370\n",
            " 68778/100000: episode: 6626, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000044, mae: 2.918972, mean_q: 4.366883\n",
            " 68786/100000: episode: 6627, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 3.353168, mean_q: 4.867659\n",
            " 68798/100000: episode: 6628, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.917 [0.000, 7.000],  loss: 0.000031, mae: 2.823754, mean_q: 4.254080\n",
            " 68807/100000: episode: 6629, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000016, mae: 2.996564, mean_q: 4.458117\n",
            " 68817/100000: episode: 6630, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000012, mae: 2.897787, mean_q: 4.340506\n",
            " 68825/100000: episode: 6631, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.802366, mean_q: 4.227100\n",
            " 68833/100000: episode: 6632, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.074976, mean_q: 4.546961\n",
            " 68842/100000: episode: 6633, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000022, mae: 2.911359, mean_q: 4.353085\n",
            " 68850/100000: episode: 6634, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.894587, mean_q: 4.334631\n",
            " 68859/100000: episode: 6635, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000016, mae: 2.827882, mean_q: 4.257547\n",
            " 68867/100000: episode: 6636, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.072152, mean_q: 4.540092\n",
            " 68876/100000: episode: 6637, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 3.138065, mean_q: 4.620439\n",
            " 68884/100000: episode: 6638, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.055112, mean_q: 4.519601\n",
            " 68895/100000: episode: 6639, duration: 0.103s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000008, mae: 2.872278, mean_q: 4.314011\n",
            " 68903/100000: episode: 6640, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.074117, mean_q: 4.544421\n",
            " 68912/100000: episode: 6641, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000015, mae: 2.911163, mean_q: 4.350311\n",
            " 68920/100000: episode: 6642, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.910513, mean_q: 4.354263\n",
            " 68930/100000: episode: 6643, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000009, mae: 2.895909, mean_q: 4.337729\n",
            " 68938/100000: episode: 6644, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.943912, mean_q: 4.396537\n",
            " 68946/100000: episode: 6645, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.949095, mean_q: 4.397655\n",
            " 68954/100000: episode: 6646, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.791719, mean_q: 4.216188\n",
            " 68963/100000: episode: 6647, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.979941, mean_q: 4.435383\n",
            " 68972/100000: episode: 6648, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000006, mae: 2.807363, mean_q: 4.233020\n",
            " 68980/100000: episode: 6649, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.063164, mean_q: 4.525474\n",
            " 68990/100000: episode: 6650, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000010, mae: 3.106064, mean_q: 4.577887\n",
            " 69000/100000: episode: 6651, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000007, mae: 2.809020, mean_q: 4.236000\n",
            " 69010/100000: episode: 6652, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.930488, mean_q: 4.374749\n",
            " 69018/100000: episode: 6653, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.898355, mean_q: 4.337508\n",
            " 69028/100000: episode: 6654, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000004, mae: 3.111845, mean_q: 4.586202\n",
            " 69036/100000: episode: 6655, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.865865, mean_q: 4.301949\n",
            " 69044/100000: episode: 6656, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.858107, mean_q: 4.287436\n",
            " 69052/100000: episode: 6657, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.741834, mean_q: 4.160826\n",
            " 69061/100000: episode: 6658, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000008, mae: 3.005196, mean_q: 4.460872\n",
            " 69070/100000: episode: 6659, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000005, mae: 2.983073, mean_q: 4.436072\n",
            " 69078/100000: episode: 6660, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.906626, mean_q: 4.346953\n",
            " 69088/100000: episode: 6661, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000014, mae: 3.001705, mean_q: 4.455089\n",
            " 69096/100000: episode: 6662, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.043428, mean_q: 4.508798\n",
            " 69104/100000: episode: 6663, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.864571, mean_q: 4.298091\n",
            " 69112/100000: episode: 6664, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.780749, mean_q: 4.199469\n",
            " 69120/100000: episode: 6665, duration: 0.092s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.118266, mean_q: 4.594274\n",
            " 69128/100000: episode: 6666, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.894361, mean_q: 4.336235\n",
            " 69139/100000: episode: 6667, duration: 0.113s, episode steps:  11, steps per second:  98, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000007, mae: 2.982397, mean_q: 4.437364\n",
            " 69148/100000: episode: 6668, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000005, mae: 2.828767, mean_q: 4.257794\n",
            " 69157/100000: episode: 6669, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 3.136402, mean_q: 4.611681\n",
            " 69165/100000: episode: 6670, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.868411, mean_q: 4.299380\n",
            " 69174/100000: episode: 6671, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.874090, mean_q: 4.309749\n",
            " 69183/100000: episode: 6672, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000009, mae: 2.941393, mean_q: 4.390932\n",
            " 69192/100000: episode: 6673, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 2.976140, mean_q: 4.428005\n",
            " 69200/100000: episode: 6674, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.022498, mean_q: 4.478159\n",
            " 69209/100000: episode: 6675, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000017, mae: 2.883824, mean_q: 4.320761\n",
            " 69218/100000: episode: 6676, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000012, mae: 2.989933, mean_q: 4.443888\n",
            " 69226/100000: episode: 6677, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.856537, mean_q: 4.291877\n",
            " 69234/100000: episode: 6678, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.058336, mean_q: 4.523390\n",
            " 69243/100000: episode: 6679, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000010, mae: 3.007315, mean_q: 4.459621\n",
            " 69251/100000: episode: 6680, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.818890, mean_q: 4.245568\n",
            " 69259/100000: episode: 6681, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.885056, mean_q: 4.324367\n",
            " 69268/100000: episode: 6682, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000015, mae: 3.091273, mean_q: 4.564566\n",
            " 69277/100000: episode: 6683, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 3.020464, mean_q: 4.482111\n",
            " 69285/100000: episode: 6684, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.097350, mean_q: 4.568736\n",
            " 69293/100000: episode: 6685, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.104290, mean_q: 4.573518\n",
            " 69302/100000: episode: 6686, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000029, mae: 2.816833, mean_q: 4.238976\n",
            " 69310/100000: episode: 6687, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.977774, mean_q: 4.431027\n",
            " 69318/100000: episode: 6688, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.860584, mean_q: 4.297540\n",
            " 69329/100000: episode: 6689, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000004, mae: 2.911632, mean_q: 4.350785\n",
            " 69338/100000: episode: 6690, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 3.049362, mean_q: 4.509962\n",
            " 69346/100000: episode: 6691, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.879614, mean_q: 4.321098\n",
            " 69356/100000: episode: 6692, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000006, mae: 3.007405, mean_q: 4.466041\n",
            " 69364/100000: episode: 6693, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.826899, mean_q: 4.257547\n",
            " 69372/100000: episode: 6694, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.839615, mean_q: 4.273231\n",
            " 69381/100000: episode: 6695, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 3.183267, mean_q: 4.667148\n",
            " 69389/100000: episode: 6696, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.703111, mean_q: 4.112559\n",
            " 69397/100000: episode: 6697, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.905152, mean_q: 4.343000\n",
            " 69406/100000: episode: 6698, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 2.910770, mean_q: 4.357305\n",
            " 69415/100000: episode: 6699, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 2.939765, mean_q: 4.386076\n",
            " 69426/100000: episode: 6700, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.909 [0.000, 7.000],  loss: 0.000003, mae: 3.006885, mean_q: 4.468678\n",
            " 69437/100000: episode: 6701, duration: 0.127s, episode steps:  11, steps per second:  86, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000002, mae: 3.137907, mean_q: 4.615697\n",
            " 69445/100000: episode: 6702, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.847167, mean_q: 4.281052\n",
            " 69454/100000: episode: 6703, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.017660, mean_q: 4.476094\n",
            " 69462/100000: episode: 6704, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.958290, mean_q: 4.406900\n",
            " 69470/100000: episode: 6705, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.794813, mean_q: 4.219180\n",
            " 69479/100000: episode: 6706, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.121294, mean_q: 4.601258\n",
            " 69488/100000: episode: 6707, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 3.114839, mean_q: 4.586203\n",
            " 69497/100000: episode: 6708, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 3.066094, mean_q: 4.531179\n",
            " 69506/100000: episode: 6709, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.926842, mean_q: 4.370562\n",
            " 69515/100000: episode: 6710, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 3.016338, mean_q: 4.474935\n",
            " 69524/100000: episode: 6711, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 2.917834, mean_q: 4.359368\n",
            " 69533/100000: episode: 6712, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 2.934890, mean_q: 4.377510\n",
            " 69543/100000: episode: 6713, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000003, mae: 2.879199, mean_q: 4.317125\n",
            " 69552/100000: episode: 6714, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000009, mae: 2.848432, mean_q: 4.277231\n",
            " 69560/100000: episode: 6715, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.021488, mean_q: 4.473989\n",
            " 69568/100000: episode: 6716, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.925710, mean_q: 4.367712\n",
            " 69576/100000: episode: 6717, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.064281, mean_q: 4.529925\n",
            " 69584/100000: episode: 6718, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000200, mae: 2.900893, mean_q: 4.338477\n",
            " 69592/100000: episode: 6719, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000282, mae: 2.862248, mean_q: 4.294644\n",
            " 69601/100000: episode: 6720, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000222, mae: 2.992200, mean_q: 4.449899\n",
            " 69609/100000: episode: 6721, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000335, mae: 2.864943, mean_q: 4.297118\n",
            " 69617/100000: episode: 6722, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000138, mae: 2.919634, mean_q: 4.353152\n",
            " 69626/100000: episode: 6723, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000092, mae: 2.965919, mean_q: 4.415043\n",
            " 69634/100000: episode: 6724, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000076, mae: 3.032780, mean_q: 4.495083\n",
            " 69642/100000: episode: 6725, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 2.881943, mean_q: 4.322390\n",
            " 69650/100000: episode: 6726, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 3.083311, mean_q: 4.558531\n",
            " 69659/100000: episode: 6727, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000064, mae: 2.889683, mean_q: 4.328005\n",
            " 69667/100000: episode: 6728, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.830429, mean_q: 4.256887\n",
            " 69676/100000: episode: 6729, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000079, mae: 2.796245, mean_q: 4.218589\n",
            " 69684/100000: episode: 6730, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 2.876623, mean_q: 4.315163\n",
            " 69693/100000: episode: 6731, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000099, mae: 3.043381, mean_q: 4.508313\n",
            " 69701/100000: episode: 6732, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 3.074202, mean_q: 4.538080\n",
            " 69709/100000: episode: 6733, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 2.913574, mean_q: 4.352471\n",
            " 69718/100000: episode: 6734, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000048, mae: 2.960372, mean_q: 4.414416\n",
            " 69726/100000: episode: 6735, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.082797, mean_q: 4.549860\n",
            " 69734/100000: episode: 6736, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.900057, mean_q: 4.334615\n",
            " 69742/100000: episode: 6737, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.944087, mean_q: 4.387557\n",
            " 69750/100000: episode: 6738, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 2.795903, mean_q: 4.221202\n",
            " 69759/100000: episode: 6739, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000036, mae: 2.907892, mean_q: 4.337852\n",
            " 69767/100000: episode: 6740, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000052, mae: 3.013411, mean_q: 4.469126\n",
            " 69776/100000: episode: 6741, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000074, mae: 2.721302, mean_q: 4.144206\n",
            " 69784/100000: episode: 6742, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 3.044444, mean_q: 4.512051\n",
            " 69792/100000: episode: 6743, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.946075, mean_q: 4.382206\n",
            " 69801/100000: episode: 6744, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000031, mae: 2.985153, mean_q: 4.432318\n",
            " 69809/100000: episode: 6745, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 3.111588, mean_q: 4.579061\n",
            " 69818/100000: episode: 6746, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000129, mae: 2.784987, mean_q: 4.204502\n",
            " 69828/100000: episode: 6747, duration: 0.149s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000127, mae: 2.903835, mean_q: 4.346793\n",
            " 69838/100000: episode: 6748, duration: 0.187s, episode steps:  10, steps per second:  54, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000038, mae: 3.075099, mean_q: 4.544344\n",
            " 69846/100000: episode: 6749, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.980680, mean_q: 4.431983\n",
            " 69854/100000: episode: 6750, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 3.062707, mean_q: 4.523458\n",
            " 69862/100000: episode: 6751, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 3.009963, mean_q: 4.460497\n",
            " 69870/100000: episode: 6752, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.125249, mean_q: 4.605885\n",
            " 69878/100000: episode: 6753, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.782276, mean_q: 4.203692\n",
            " 69888/100000: episode: 6754, duration: 0.138s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000030, mae: 3.030250, mean_q: 4.485086\n",
            " 69896/100000: episode: 6755, duration: 0.157s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.779083, mean_q: 4.199852\n",
            " 69906/100000: episode: 6756, duration: 0.171s, episode steps:  10, steps per second:  58, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000050, mae: 3.126658, mean_q: 4.608656\n",
            " 69915/100000: episode: 6757, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000042, mae: 3.019819, mean_q: 4.480592\n",
            " 69923/100000: episode: 6758, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.828936, mean_q: 4.252247\n",
            " 69933/100000: episode: 6759, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000142, mae: 2.871709, mean_q: 4.304847\n",
            " 69943/100000: episode: 6760, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000109, mae: 2.907831, mean_q: 4.339942\n",
            " 69952/100000: episode: 6761, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000047, mae: 3.136858, mean_q: 4.605692\n",
            " 69960/100000: episode: 6762, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 2.987028, mean_q: 4.431176\n",
            " 69968/100000: episode: 6763, duration: 0.101s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000306, mae: 2.939979, mean_q: 4.382662\n",
            " 69976/100000: episode: 6764, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000294, mae: 2.937261, mean_q: 4.379566\n",
            " 69984/100000: episode: 6765, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000448, mae: 2.799923, mean_q: 4.205964\n",
            " 69993/100000: episode: 6766, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000829, mae: 2.784723, mean_q: 4.190173\n",
            " 70003/100000: episode: 6767, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000632, mae: 3.000127, mean_q: 4.450098\n",
            " 70011/100000: episode: 6768, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000283, mae: 2.875340, mean_q: 4.313696\n",
            " 70020/100000: episode: 6769, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000247, mae: 3.034083, mean_q: 4.505410\n",
            " 70028/100000: episode: 6770, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000289, mae: 2.937972, mean_q: 4.383413\n",
            " 70036/100000: episode: 6771, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000257, mae: 2.834405, mean_q: 4.266243\n",
            " 70044/100000: episode: 6772, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000212, mae: 2.858948, mean_q: 4.294568\n",
            " 70053/100000: episode: 6773, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000153, mae: 3.060557, mean_q: 4.524511\n",
            " 70061/100000: episode: 6774, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000167, mae: 2.826166, mean_q: 4.248656\n",
            " 70069/100000: episode: 6775, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000136, mae: 2.988072, mean_q: 4.434276\n",
            " 70079/100000: episode: 6776, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000115, mae: 2.983497, mean_q: 4.437612\n",
            " 70088/100000: episode: 6777, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000183, mae: 2.929471, mean_q: 4.375894\n",
            " 70099/100000: episode: 6778, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000257, mae: 2.901373, mean_q: 4.331243\n",
            " 70108/100000: episode: 6779, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000094, mae: 2.731516, mean_q: 4.144789\n",
            " 70118/100000: episode: 6780, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.932572, mean_q: 4.367652\n",
            " 70126/100000: episode: 6781, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.980132, mean_q: 4.427854\n",
            " 70134/100000: episode: 6782, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.755351, mean_q: 4.175370\n",
            " 70142/100000: episode: 6783, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.064001, mean_q: 4.531149\n",
            " 70150/100000: episode: 6784, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.908355, mean_q: 4.347925\n",
            " 70158/100000: episode: 6785, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.635641, mean_q: 4.038710\n",
            " 70166/100000: episode: 6786, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.840631, mean_q: 4.273037\n",
            " 70175/100000: episode: 6787, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000016, mae: 3.104426, mean_q: 4.574999\n",
            " 70183/100000: episode: 6788, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.970580, mean_q: 4.421846\n",
            " 70193/100000: episode: 6789, duration: 0.089s, episode steps:  10, steps per second: 112, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000016, mae: 2.780581, mean_q: 4.200037\n",
            " 70203/100000: episode: 6790, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000008, mae: 2.902400, mean_q: 4.344090\n",
            " 70211/100000: episode: 6791, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.914109, mean_q: 4.359095\n",
            " 70219/100000: episode: 6792, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.954940, mean_q: 4.406254\n",
            " 70227/100000: episode: 6793, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.994485, mean_q: 4.452919\n",
            " 70236/100000: episode: 6794, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 2.876518, mean_q: 4.310318\n",
            " 70244/100000: episode: 6795, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.106370, mean_q: 4.578528\n",
            " 70253/100000: episode: 6796, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.969577, mean_q: 4.426678\n",
            " 70262/100000: episode: 6797, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.929833, mean_q: 4.374240\n",
            " 70271/100000: episode: 6798, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 3.243941, mean_q: 4.743371\n",
            " 70279/100000: episode: 6799, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.083803, mean_q: 4.554712\n",
            " 70287/100000: episode: 6800, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.783080, mean_q: 4.201639\n",
            " 70297/100000: episode: 6801, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000003, mae: 2.966090, mean_q: 4.414140\n",
            " 70305/100000: episode: 6802, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.129157, mean_q: 4.608101\n",
            " 70315/100000: episode: 6803, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 3.021885, mean_q: 4.484093\n",
            " 70323/100000: episode: 6804, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.962408, mean_q: 4.412239\n",
            " 70333/100000: episode: 6805, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.000282, mean_q: 4.456122\n",
            " 70341/100000: episode: 6806, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.873572, mean_q: 4.311671\n",
            " 70351/100000: episode: 6807, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000006, mae: 2.877272, mean_q: 4.319772\n",
            " 70360/100000: episode: 6808, duration: 0.099s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000006, mae: 2.949186, mean_q: 4.400299\n",
            " 70369/100000: episode: 6809, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 3.109648, mean_q: 4.586181\n",
            " 70379/100000: episode: 6810, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000008, mae: 2.922744, mean_q: 4.367516\n",
            " 70387/100000: episode: 6811, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.795055, mean_q: 4.223639\n",
            " 70396/100000: episode: 6812, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 2.962450, mean_q: 4.413559\n",
            " 70404/100000: episode: 6813, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.952430, mean_q: 4.404798\n",
            " 70413/100000: episode: 6814, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 3.055057, mean_q: 4.520207\n",
            " 70422/100000: episode: 6815, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 2.801636, mean_q: 4.230042\n",
            " 70430/100000: episode: 6816, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.861513, mean_q: 4.300597\n",
            " 70440/100000: episode: 6817, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000006, mae: 3.043594, mean_q: 4.508754\n",
            " 70448/100000: episode: 6818, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.789804, mean_q: 4.215029\n",
            " 70457/100000: episode: 6819, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.953171, mean_q: 4.403739\n",
            " 70467/100000: episode: 6820, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000005, mae: 2.784942, mean_q: 4.209338\n",
            " 70476/100000: episode: 6821, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 2.884413, mean_q: 4.325819\n",
            " 70484/100000: episode: 6822, duration: 0.077s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.900017, mean_q: 4.347690\n",
            " 70492/100000: episode: 6823, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.109015, mean_q: 4.582978\n",
            " 70500/100000: episode: 6824, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.891004, mean_q: 4.328245\n",
            " 70509/100000: episode: 6825, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.987236, mean_q: 4.438165\n",
            " 70517/100000: episode: 6826, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.822241, mean_q: 4.247267\n",
            " 70525/100000: episode: 6827, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.792751, mean_q: 4.217627\n",
            " 70533/100000: episode: 6828, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.100878, mean_q: 4.573071\n",
            " 70542/100000: episode: 6829, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.988307, mean_q: 4.439242\n",
            " 70552/100000: episode: 6830, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000001, mae: 2.794671, mean_q: 4.227167\n",
            " 70561/100000: episode: 6831, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.010642, mean_q: 4.474504\n",
            " 70569/100000: episode: 6832, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.944529, mean_q: 4.392520\n",
            " 70578/100000: episode: 6833, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.971058, mean_q: 4.421887\n",
            " 70586/100000: episode: 6834, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.171661, mean_q: 4.655215\n",
            " 70595/100000: episode: 6835, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 3.016111, mean_q: 4.476671\n",
            " 70605/100000: episode: 6836, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.063585, mean_q: 4.534987\n",
            " 70614/100000: episode: 6837, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 3.135282, mean_q: 4.612866\n",
            " 70623/100000: episode: 6838, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000004, mae: 3.022955, mean_q: 4.487294\n",
            " 70631/100000: episode: 6839, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.878296, mean_q: 4.313066\n",
            " 70641/100000: episode: 6840, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000013, mae: 3.037332, mean_q: 4.504277\n",
            " 70651/100000: episode: 6841, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000009, mae: 2.985690, mean_q: 4.446239\n",
            " 70659/100000: episode: 6842, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.795006, mean_q: 4.220057\n",
            " 70668/100000: episode: 6843, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 2.994460, mean_q: 4.451362\n",
            " 70676/100000: episode: 6844, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.022519, mean_q: 4.488087\n",
            " 70684/100000: episode: 6845, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.716841, mean_q: 4.123723\n",
            " 70692/100000: episode: 6846, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.126074, mean_q: 4.603865\n",
            " 70700/100000: episode: 6847, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.021511, mean_q: 4.485206\n",
            " 70708/100000: episode: 6848, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.009050, mean_q: 4.467996\n",
            " 70716/100000: episode: 6849, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.742996, mean_q: 4.161938\n",
            " 70724/100000: episode: 6850, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.063530, mean_q: 4.530800\n",
            " 70733/100000: episode: 6851, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 3.084489, mean_q: 4.558691\n",
            " 70741/100000: episode: 6852, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.856694, mean_q: 4.287158\n",
            " 70749/100000: episode: 6853, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.982112, mean_q: 4.435800\n",
            " 70757/100000: episode: 6854, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.836345, mean_q: 4.268031\n",
            " 70765/100000: episode: 6855, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.927351, mean_q: 4.379667\n",
            " 70775/100000: episode: 6856, duration: 0.095s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000003, mae: 2.878690, mean_q: 4.314386\n",
            " 70783/100000: episode: 6857, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.749419, mean_q: 4.163856\n",
            " 70791/100000: episode: 6858, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.025343, mean_q: 4.490662\n",
            " 70799/100000: episode: 6859, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.009275, mean_q: 4.467607\n",
            " 70808/100000: episode: 6860, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000012, mae: 3.005426, mean_q: 4.464515\n",
            " 70816/100000: episode: 6861, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.832403, mean_q: 4.260766\n",
            " 70824/100000: episode: 6862, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.994313, mean_q: 4.445531\n",
            " 70833/100000: episode: 6863, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000025, mae: 2.940824, mean_q: 4.390468\n",
            " 70842/100000: episode: 6864, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000053, mae: 2.884198, mean_q: 4.319686\n",
            " 70851/100000: episode: 6865, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000038, mae: 3.260735, mean_q: 4.756193\n",
            " 70859/100000: episode: 6866, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.028891, mean_q: 4.482243\n",
            " 70868/100000: episode: 6867, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000017, mae: 2.865192, mean_q: 4.302005\n",
            " 70876/100000: episode: 6868, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.240125, mean_q: 4.729482\n",
            " 70885/100000: episode: 6869, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000017, mae: 2.888681, mean_q: 4.330405\n",
            " 70893/100000: episode: 6870, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.968441, mean_q: 4.420928\n",
            " 70902/100000: episode: 6871, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000022, mae: 2.992903, mean_q: 4.443761\n",
            " 70911/100000: episode: 6872, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000063, mae: 2.944324, mean_q: 4.394187\n",
            " 70920/100000: episode: 6873, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000115, mae: 3.040454, mean_q: 4.505095\n",
            " 70928/100000: episode: 6874, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000139, mae: 2.944078, mean_q: 4.395322\n",
            " 70937/100000: episode: 6875, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000149, mae: 3.039296, mean_q: 4.498864\n",
            " 70948/100000: episode: 6876, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000153, mae: 3.072762, mean_q: 4.534302\n",
            " 70956/100000: episode: 6877, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000437, mae: 2.897797, mean_q: 4.338252\n",
            " 70964/100000: episode: 6878, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000339, mae: 2.944655, mean_q: 4.364443\n",
            " 70972/100000: episode: 6879, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000342, mae: 2.826719, mean_q: 4.240522\n",
            " 70981/100000: episode: 6880, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000159, mae: 2.961819, mean_q: 4.405312\n",
            " 70991/100000: episode: 6881, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000161, mae: 2.919683, mean_q: 4.354931\n",
            " 71001/100000: episode: 6882, duration: 0.163s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000125, mae: 2.929845, mean_q: 4.360059\n",
            " 71009/100000: episode: 6883, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000079, mae: 2.787022, mean_q: 4.215497\n",
            " 71018/100000: episode: 6884, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000067, mae: 3.128916, mean_q: 4.611157\n",
            " 71027/100000: episode: 6885, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000030, mae: 3.036314, mean_q: 4.498285\n",
            " 71037/100000: episode: 6886, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.802865, mean_q: 4.224351\n",
            " 71047/100000: episode: 6887, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000016, mae: 3.010767, mean_q: 4.465718\n",
            " 71055/100000: episode: 6888, duration: 0.192s, episode steps:   8, steps per second:  42, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.148370, mean_q: 4.629155\n",
            " 71064/100000: episode: 6889, duration: 0.202s, episode steps:   9, steps per second:  45, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000029, mae: 2.910028, mean_q: 4.352034\n",
            " 71073/100000: episode: 6890, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000076, mae: 3.005027, mean_q: 4.466787\n",
            " 71081/100000: episode: 6891, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 3.047858, mean_q: 4.518198\n",
            " 71089/100000: episode: 6892, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 3.130776, mean_q: 4.605742\n",
            " 71098/100000: episode: 6893, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000033, mae: 2.901302, mean_q: 4.338362\n",
            " 71106/100000: episode: 6894, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 2.975420, mean_q: 4.432112\n",
            " 71115/100000: episode: 6895, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000123, mae: 3.043170, mean_q: 4.497780\n",
            " 71126/100000: episode: 6896, duration: 0.132s, episode steps:  11, steps per second:  84, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000175, mae: 3.070118, mean_q: 4.525413\n",
            " 71135/100000: episode: 6897, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000159, mae: 2.897515, mean_q: 4.333798\n",
            " 71143/100000: episode: 6898, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 3.024317, mean_q: 4.474257\n",
            " 71151/100000: episode: 6899, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 2.891008, mean_q: 4.313546\n",
            " 71160/100000: episode: 6900, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000168, mae: 2.847570, mean_q: 4.270299\n",
            " 71168/100000: episode: 6901, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000143, mae: 2.851351, mean_q: 4.287373\n",
            " 71178/100000: episode: 6902, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000172, mae: 2.961992, mean_q: 4.407639\n",
            " 71187/100000: episode: 6903, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000318, mae: 2.959360, mean_q: 4.399362\n",
            " 71197/100000: episode: 6904, duration: 0.146s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000326, mae: 3.057622, mean_q: 4.505078\n",
            " 71206/100000: episode: 6905, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000209, mae: 2.913717, mean_q: 4.358531\n",
            " 71214/100000: episode: 6906, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000220, mae: 3.030929, mean_q: 4.495122\n",
            " 71223/100000: episode: 6907, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000249, mae: 2.839901, mean_q: 4.249712\n",
            " 71231/100000: episode: 6908, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000364, mae: 3.121494, mean_q: 4.590962\n",
            " 71240/100000: episode: 6909, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000372, mae: 2.805701, mean_q: 4.223231\n",
            " 71248/100000: episode: 6910, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000446, mae: 3.069380, mean_q: 4.516174\n",
            " 71256/100000: episode: 6911, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000405, mae: 2.924443, mean_q: 4.353514\n",
            " 71265/100000: episode: 6912, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000352, mae: 3.071461, mean_q: 4.529168\n",
            " 71273/100000: episode: 6913, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000097, mae: 3.029167, mean_q: 4.481860\n",
            " 71281/100000: episode: 6914, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000174, mae: 2.910881, mean_q: 4.331329\n",
            " 71289/100000: episode: 6915, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000358, mae: 2.824157, mean_q: 4.240993\n",
            " 71297/100000: episode: 6916, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000232, mae: 3.118905, mean_q: 4.586764\n",
            " 71306/100000: episode: 6917, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000195, mae: 2.988784, mean_q: 4.443209\n",
            " 71314/100000: episode: 6918, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000172, mae: 3.080725, mean_q: 4.544316\n",
            " 71323/100000: episode: 6919, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000236, mae: 2.834682, mean_q: 4.256732\n",
            " 71331/100000: episode: 6920, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000089, mae: 3.117216, mean_q: 4.596376\n",
            " 71340/100000: episode: 6921, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000062, mae: 2.919009, mean_q: 4.364404\n",
            " 71348/100000: episode: 6922, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 3.031142, mean_q: 4.489101\n",
            " 71357/100000: episode: 6923, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000090, mae: 2.713806, mean_q: 4.119505\n",
            " 71365/100000: episode: 6924, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 2.892970, mean_q: 4.329123\n",
            " 71373/100000: episode: 6925, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.992925, mean_q: 4.447706\n",
            " 71382/100000: episode: 6926, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000026, mae: 3.033305, mean_q: 4.493654\n",
            " 71392/100000: episode: 6927, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000020, mae: 2.966813, mean_q: 4.415038\n",
            " 71400/100000: episode: 6928, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.887688, mean_q: 4.325950\n",
            " 71409/100000: episode: 6929, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000027, mae: 2.987417, mean_q: 4.437425\n",
            " 71418/100000: episode: 6930, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000025, mae: 3.112042, mean_q: 4.585088\n",
            " 71427/100000: episode: 6931, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000013, mae: 3.079618, mean_q: 4.549069\n",
            " 71435/100000: episode: 6932, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.771470, mean_q: 4.189362\n",
            " 71443/100000: episode: 6933, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.793468, mean_q: 4.212138\n",
            " 71452/100000: episode: 6934, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000012, mae: 3.038182, mean_q: 4.497000\n",
            " 71462/100000: episode: 6935, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.061725, mean_q: 4.518983\n",
            " 71470/100000: episode: 6936, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.887865, mean_q: 4.324697\n",
            " 71480/100000: episode: 6937, duration: 0.112s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000032, mae: 2.669339, mean_q: 4.074033\n",
            " 71489/100000: episode: 6938, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000011, mae: 2.902378, mean_q: 4.347506\n",
            " 71498/100000: episode: 6939, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 2.938126, mean_q: 4.385967\n",
            " 71506/100000: episode: 6940, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.853208, mean_q: 4.291518\n",
            " 71514/100000: episode: 6941, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.660852, mean_q: 4.058333\n",
            " 71523/100000: episode: 6942, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 2.870680, mean_q: 4.303133\n",
            " 71531/100000: episode: 6943, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.121604, mean_q: 4.600808\n",
            " 71540/100000: episode: 6944, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000005, mae: 2.994442, mean_q: 4.452838\n",
            " 71548/100000: episode: 6945, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.990728, mean_q: 4.446696\n",
            " 71556/100000: episode: 6946, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.056528, mean_q: 4.526024\n",
            " 71564/100000: episode: 6947, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.814383, mean_q: 4.246811\n",
            " 71572/100000: episode: 6948, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.977504, mean_q: 4.434505\n",
            " 71580/100000: episode: 6949, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.919748, mean_q: 4.362182\n",
            " 71589/100000: episode: 6950, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.979964, mean_q: 4.434072\n",
            " 71598/100000: episode: 6951, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.763456, mean_q: 4.182185\n",
            " 71606/100000: episode: 6952, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.999815, mean_q: 4.464962\n",
            " 71616/100000: episode: 6953, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000006, mae: 3.066512, mean_q: 4.535872\n",
            " 71626/100000: episode: 6954, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000007, mae: 2.845698, mean_q: 4.279466\n",
            " 71636/100000: episode: 6955, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.058920, mean_q: 4.524085\n",
            " 71644/100000: episode: 6956, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.170501, mean_q: 4.659996\n",
            " 71653/100000: episode: 6957, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000004, mae: 3.003687, mean_q: 4.455908\n",
            " 71663/100000: episode: 6958, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000003, mae: 3.158398, mean_q: 4.640213\n",
            " 71671/100000: episode: 6959, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.956592, mean_q: 4.408237\n",
            " 71681/100000: episode: 6960, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000002, mae: 2.783705, mean_q: 4.205329\n",
            " 71689/100000: episode: 6961, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.702691, mean_q: 4.111800\n",
            " 71697/100000: episode: 6962, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.047956, mean_q: 4.516538\n",
            " 71705/100000: episode: 6963, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.968761, mean_q: 4.419241\n",
            " 71715/100000: episode: 6964, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000001, mae: 3.018373, mean_q: 4.474798\n",
            " 71723/100000: episode: 6965, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.148501, mean_q: 4.635520\n",
            " 71731/100000: episode: 6966, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.998593, mean_q: 4.455445\n",
            " 71739/100000: episode: 6967, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.894713, mean_q: 4.340827\n",
            " 71749/100000: episode: 6968, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000001, mae: 2.953081, mean_q: 4.404134\n",
            " 71757/100000: episode: 6969, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.855954, mean_q: 4.289437\n",
            " 71767/100000: episode: 6970, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000001, mae: 2.806656, mean_q: 4.231590\n",
            " 71775/100000: episode: 6971, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.120446, mean_q: 4.600797\n",
            " 71785/100000: episode: 6972, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000001, mae: 3.074845, mean_q: 4.548314\n",
            " 71793/100000: episode: 6973, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.935631, mean_q: 4.386838\n",
            " 71801/100000: episode: 6974, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.951957, mean_q: 4.403099\n",
            " 71809/100000: episode: 6975, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.075804, mean_q: 4.548526\n",
            " 71819/100000: episode: 6976, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 2.988494, mean_q: 4.441499\n",
            " 71828/100000: episode: 6977, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.890096, mean_q: 4.326329\n",
            " 71837/100000: episode: 6978, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 2.979237, mean_q: 4.428559\n",
            " 71846/100000: episode: 6979, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.853333, mean_q: 4.288983\n",
            " 71854/100000: episode: 6980, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.880886, mean_q: 4.318082\n",
            " 71862/100000: episode: 6981, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.905699, mean_q: 4.352606\n",
            " 71871/100000: episode: 6982, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.001812, mean_q: 4.463812\n",
            " 71881/100000: episode: 6983, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000002, mae: 2.946820, mean_q: 4.398026\n",
            " 71891/100000: episode: 6984, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000002, mae: 2.965810, mean_q: 4.419543\n",
            " 71903/100000: episode: 6985, duration: 0.124s, episode steps:  12, steps per second:  97, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.583 [0.000, 7.000],  loss: 0.000002, mae: 2.961791, mean_q: 4.408247\n",
            " 71911/100000: episode: 6986, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.050277, mean_q: 4.512707\n",
            " 71919/100000: episode: 6987, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.994637, mean_q: 4.455004\n",
            " 71929/100000: episode: 6988, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 2.999952, mean_q: 4.456895\n",
            " 71939/100000: episode: 6989, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 3.010816, mean_q: 4.468670\n",
            " 71947/100000: episode: 6990, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.999680, mean_q: 4.458000\n",
            " 71955/100000: episode: 6991, duration: 0.094s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.133844, mean_q: 4.615139\n",
            " 71963/100000: episode: 6992, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.222201, mean_q: 4.715771\n",
            " 71971/100000: episode: 6993, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.915051, mean_q: 4.359806\n",
            " 71979/100000: episode: 6994, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.777730, mean_q: 4.198339\n",
            " 71989/100000: episode: 6995, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000004, mae: 2.921109, mean_q: 4.365411\n",
            " 71997/100000: episode: 6996, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.979518, mean_q: 4.432335\n",
            " 72006/100000: episode: 6997, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000004, mae: 2.904511, mean_q: 4.346045\n",
            " 72015/100000: episode: 6998, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 3.172267, mean_q: 4.658185\n",
            " 72023/100000: episode: 6999, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.168143, mean_q: 4.655733\n",
            " 72031/100000: episode: 7000, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.975415, mean_q: 4.428046\n",
            " 72039/100000: episode: 7001, duration: 0.113s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.907691, mean_q: 4.349400\n",
            " 72047/100000: episode: 7002, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.865017, mean_q: 4.300754\n",
            " 72056/100000: episode: 7003, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000009, mae: 2.973134, mean_q: 4.421579\n",
            " 72065/100000: episode: 7004, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 2.850189, mean_q: 4.284456\n",
            " 72073/100000: episode: 7005, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.750246, mean_q: 4.160353\n",
            " 72081/100000: episode: 7006, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.738249, mean_q: 4.151333\n",
            " 72090/100000: episode: 7007, duration: 0.144s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000008, mae: 2.927682, mean_q: 4.367324\n",
            " 72098/100000: episode: 7008, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.955956, mean_q: 4.405569\n",
            " 72107/100000: episode: 7009, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000010, mae: 2.868148, mean_q: 4.299351\n",
            " 72118/100000: episode: 7010, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000018, mae: 3.014553, mean_q: 4.474316\n",
            " 72126/100000: episode: 7011, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.060338, mean_q: 4.535055\n",
            " 72134/100000: episode: 7012, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.963251, mean_q: 4.411229\n",
            " 72143/100000: episode: 7013, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000025, mae: 3.306055, mean_q: 4.808937\n",
            " 72152/100000: episode: 7014, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000019, mae: 2.920402, mean_q: 4.365312\n",
            " 72160/100000: episode: 7015, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.995561, mean_q: 4.454111\n",
            " 72168/100000: episode: 7016, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 3.005473, mean_q: 4.468071\n",
            " 72176/100000: episode: 7017, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.884093, mean_q: 4.312366\n",
            " 72184/100000: episode: 7018, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.970011, mean_q: 4.424768\n",
            " 72192/100000: episode: 7019, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.979439, mean_q: 4.440844\n",
            " 72200/100000: episode: 7020, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.199163, mean_q: 4.688486\n",
            " 72208/100000: episode: 7021, duration: 0.126s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 2.974586, mean_q: 4.419336\n",
            " 72216/100000: episode: 7022, duration: 0.153s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.995653, mean_q: 4.457517\n",
            " 72225/100000: episode: 7023, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000042, mae: 3.056565, mean_q: 4.532693\n",
            " 72234/100000: episode: 7024, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000043, mae: 3.032696, mean_q: 4.493540\n",
            " 72243/100000: episode: 7025, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000137, mae: 2.942239, mean_q: 4.383090\n",
            " 72252/100000: episode: 7026, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000149, mae: 3.039255, mean_q: 4.497770\n",
            " 72260/100000: episode: 7027, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000220, mae: 2.795235, mean_q: 4.208243\n",
            " 72268/100000: episode: 7028, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000281, mae: 2.914705, mean_q: 4.357481\n",
            " 72276/100000: episode: 7029, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000267, mae: 2.909837, mean_q: 4.340282\n",
            " 72286/100000: episode: 7030, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000650, mae: 3.041938, mean_q: 4.503232\n",
            " 72296/100000: episode: 7031, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.001039, mae: 3.124213, mean_q: 4.585390\n",
            " 72306/100000: episode: 7032, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000614, mae: 2.818027, mean_q: 4.227879\n",
            " 72314/100000: episode: 7033, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000426, mae: 3.041989, mean_q: 4.479809\n",
            " 72324/100000: episode: 7034, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000350, mae: 2.915960, mean_q: 4.338114\n",
            " 72332/100000: episode: 7035, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000285, mae: 2.961940, mean_q: 4.406251\n",
            " 72340/100000: episode: 7036, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000311, mae: 3.101549, mean_q: 4.574172\n",
            " 72348/100000: episode: 7037, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000165, mae: 2.956766, mean_q: 4.409245\n",
            " 72356/100000: episode: 7038, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000095, mae: 3.090416, mean_q: 4.556784\n",
            " 72364/100000: episode: 7039, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000157, mae: 3.010693, mean_q: 4.454586\n",
            " 72373/100000: episode: 7040, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000154, mae: 3.159200, mean_q: 4.635798\n",
            " 72382/100000: episode: 7041, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000175, mae: 3.078129, mean_q: 4.552927\n",
            " 72391/100000: episode: 7042, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000070, mae: 2.911098, mean_q: 4.351326\n",
            " 72399/100000: episode: 7043, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 3.038572, mean_q: 4.491345\n",
            " 72408/100000: episode: 7044, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000044, mae: 2.811779, mean_q: 4.231565\n",
            " 72417/100000: episode: 7045, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000021, mae: 2.845650, mean_q: 4.278582\n",
            " 72426/100000: episode: 7046, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000020, mae: 3.054193, mean_q: 4.521399\n",
            " 72434/100000: episode: 7047, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.900357, mean_q: 4.341477\n",
            " 72442/100000: episode: 7048, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.962820, mean_q: 4.404852\n",
            " 72451/100000: episode: 7049, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000012, mae: 2.942296, mean_q: 4.390594\n",
            " 72459/100000: episode: 7050, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.962201, mean_q: 4.413701\n",
            " 72468/100000: episode: 7051, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 3.056893, mean_q: 4.523277\n",
            " 72476/100000: episode: 7052, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.038949, mean_q: 4.502972\n",
            " 72485/100000: episode: 7053, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.938392, mean_q: 4.391617\n",
            " 72495/100000: episode: 7054, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000002, mae: 2.987088, mean_q: 4.441834\n",
            " 72504/100000: episode: 7055, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 3.039208, mean_q: 4.500859\n",
            " 72512/100000: episode: 7056, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.190344, mean_q: 4.678608\n",
            " 72522/100000: episode: 7057, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000003, mae: 3.079095, mean_q: 4.550270\n",
            " 72532/100000: episode: 7058, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000002, mae: 3.022661, mean_q: 4.485424\n",
            " 72541/100000: episode: 7059, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.934485, mean_q: 4.376887\n",
            " 72549/100000: episode: 7060, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.944178, mean_q: 4.396028\n",
            " 72558/100000: episode: 7061, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.027364, mean_q: 4.494094\n",
            " 72567/100000: episode: 7062, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.843092, mean_q: 4.275808\n",
            " 72575/100000: episode: 7063, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.957823, mean_q: 4.405374\n",
            " 72584/100000: episode: 7064, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000004, mae: 2.934002, mean_q: 4.381648\n",
            " 72593/100000: episode: 7065, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 3.046520, mean_q: 4.505976\n",
            " 72601/100000: episode: 7066, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.110599, mean_q: 4.589304\n",
            " 72611/100000: episode: 7067, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000001, mae: 3.043298, mean_q: 4.507303\n",
            " 72620/100000: episode: 7068, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.723146, mean_q: 4.139109\n",
            " 72628/100000: episode: 7069, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.967095, mean_q: 4.419366\n",
            " 72636/100000: episode: 7070, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.987701, mean_q: 4.445605\n",
            " 72644/100000: episode: 7071, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.166484, mean_q: 4.648684\n",
            " 72652/100000: episode: 7072, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.990901, mean_q: 4.446631\n",
            " 72660/100000: episode: 7073, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.742705, mean_q: 4.159224\n",
            " 72668/100000: episode: 7074, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.032682, mean_q: 4.495247\n",
            " 72677/100000: episode: 7075, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000000, mae: 2.789666, mean_q: 4.212374\n",
            " 72685/100000: episode: 7076, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.193727, mean_q: 4.681659\n",
            " 72694/100000: episode: 7077, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000000, mae: 3.009861, mean_q: 4.468565\n",
            " 72704/100000: episode: 7078, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.903714, mean_q: 4.345998\n",
            " 72713/100000: episode: 7079, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000000, mae: 2.925771, mean_q: 4.373558\n",
            " 72721/100000: episode: 7080, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.852105, mean_q: 4.286265\n",
            " 72731/100000: episode: 7081, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000000, mae: 2.858860, mean_q: 4.298619\n",
            " 72740/100000: episode: 7082, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 3.070157, mean_q: 4.539354\n",
            " 72748/100000: episode: 7083, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.781113, mean_q: 4.200366\n",
            " 72756/100000: episode: 7084, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.951840, mean_q: 4.402486\n",
            " 72764/100000: episode: 7085, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.959867, mean_q: 4.413361\n",
            " 72773/100000: episode: 7086, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.889378, mean_q: 4.327300\n",
            " 72781/100000: episode: 7087, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.988995, mean_q: 4.447078\n",
            " 72790/100000: episode: 7088, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.880605, mean_q: 4.324019\n",
            " 72798/100000: episode: 7089, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.953395, mean_q: 4.402857\n",
            " 72806/100000: episode: 7090, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.951365, mean_q: 4.402216\n",
            " 72814/100000: episode: 7091, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.675518, mean_q: 4.089676\n",
            " 72823/100000: episode: 7092, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000000, mae: 2.808179, mean_q: 4.239485\n",
            " 72831/100000: episode: 7093, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.992043, mean_q: 4.446135\n",
            " 72839/100000: episode: 7094, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.904353, mean_q: 4.348817\n",
            " 72847/100000: episode: 7095, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.127529, mean_q: 4.607315\n",
            " 72855/100000: episode: 7096, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.030947, mean_q: 4.492982\n",
            " 72863/100000: episode: 7097, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.966938, mean_q: 4.417901\n",
            " 72872/100000: episode: 7098, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.052428, mean_q: 4.513378\n",
            " 72881/100000: episode: 7099, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.796010, mean_q: 4.218097\n",
            " 72891/100000: episode: 7100, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000001, mae: 2.779037, mean_q: 4.199211\n",
            " 72901/100000: episode: 7101, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 2.932255, mean_q: 4.379947\n",
            " 72909/100000: episode: 7102, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.004115, mean_q: 4.460039\n",
            " 72917/100000: episode: 7103, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.922856, mean_q: 4.373854\n",
            " 72925/100000: episode: 7104, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.019573, mean_q: 4.482560\n",
            " 72934/100000: episode: 7105, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 3.142037, mean_q: 4.620584\n",
            " 72942/100000: episode: 7106, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.967470, mean_q: 4.421288\n",
            " 72950/100000: episode: 7107, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.925011, mean_q: 4.369076\n",
            " 72960/100000: episode: 7108, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 2.886532, mean_q: 4.324779\n",
            " 72968/100000: episode: 7109, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.996092, mean_q: 4.455863\n",
            " 72976/100000: episode: 7110, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.884500, mean_q: 4.325153\n",
            " 72985/100000: episode: 7111, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.098681, mean_q: 4.575868\n",
            " 72994/100000: episode: 7112, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 2.998034, mean_q: 4.453444\n",
            " 73002/100000: episode: 7113, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.952032, mean_q: 4.401184\n",
            " 73010/100000: episode: 7114, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.995949, mean_q: 4.455859\n",
            " 73019/100000: episode: 7115, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.900363, mean_q: 4.337428\n",
            " 73028/100000: episode: 7116, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 3.095394, mean_q: 4.563990\n",
            " 73037/100000: episode: 7117, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000004, mae: 2.862910, mean_q: 4.301481\n",
            " 73046/100000: episode: 7118, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.782341, mean_q: 4.209746\n",
            " 73054/100000: episode: 7119, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.920799, mean_q: 4.366337\n",
            " 73062/100000: episode: 7120, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.035937, mean_q: 4.500591\n",
            " 73071/100000: episode: 7121, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 3.014344, mean_q: 4.477309\n",
            " 73080/100000: episode: 7122, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.943719, mean_q: 4.395944\n",
            " 73088/100000: episode: 7123, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.990305, mean_q: 4.441232\n",
            " 73097/100000: episode: 7124, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.895584, mean_q: 4.337743\n",
            " 73108/100000: episode: 7125, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.818 [0.000, 7.000],  loss: 0.000002, mae: 2.809035, mean_q: 4.237848\n",
            " 73116/100000: episode: 7126, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.968772, mean_q: 4.424575\n",
            " 73125/100000: episode: 7127, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000004, mae: 2.925011, mean_q: 4.369058\n",
            " 73135/100000: episode: 7128, duration: 0.089s, episode steps:  10, steps per second: 112, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000004, mae: 3.034680, mean_q: 4.499734\n",
            " 73143/100000: episode: 7129, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.885554, mean_q: 4.330951\n",
            " 73151/100000: episode: 7130, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.945262, mean_q: 4.394075\n",
            " 73161/100000: episode: 7131, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000015, mae: 2.907111, mean_q: 4.343562\n",
            " 73169/100000: episode: 7132, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.981478, mean_q: 4.433073\n",
            " 73178/100000: episode: 7133, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000023, mae: 2.942420, mean_q: 4.390462\n",
            " 73188/100000: episode: 7134, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000063, mae: 2.879897, mean_q: 4.308491\n",
            " 73196/100000: episode: 7135, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 2.855718, mean_q: 4.282990\n",
            " 73204/100000: episode: 7136, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000124, mae: 2.905531, mean_q: 4.353568\n",
            " 73212/100000: episode: 7137, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.954971, mean_q: 4.402177\n",
            " 73220/100000: episode: 7138, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000119, mae: 3.030217, mean_q: 4.487823\n",
            " 73228/100000: episode: 7139, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 3.103762, mean_q: 4.573977\n",
            " 73237/100000: episode: 7140, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000128, mae: 3.018732, mean_q: 4.471652\n",
            " 73245/100000: episode: 7141, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 2.885860, mean_q: 4.316799\n",
            " 73253/100000: episode: 7142, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 3.099480, mean_q: 4.569455\n",
            " 73261/100000: episode: 7143, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000092, mae: 3.004018, mean_q: 4.459769\n",
            " 73270/100000: episode: 7144, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000074, mae: 3.155996, mean_q: 4.629937\n",
            " 73278/100000: episode: 7145, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000079, mae: 2.986733, mean_q: 4.441354\n",
            " 73286/100000: episode: 7146, duration: 0.153s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000080, mae: 2.752423, mean_q: 4.170101\n",
            " 73294/100000: episode: 7147, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 3.085929, mean_q: 4.551358\n",
            " 73302/100000: episode: 7148, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000145, mae: 2.875108, mean_q: 4.303731\n",
            " 73313/100000: episode: 7149, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000201, mae: 3.031951, mean_q: 4.481252\n",
            " 73324/100000: episode: 7150, duration: 0.205s, episode steps:  11, steps per second:  54, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000117, mae: 2.830870, mean_q: 4.258028\n",
            " 73335/100000: episode: 7151, duration: 0.234s, episode steps:  11, steps per second:  47, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000071, mae: 2.793373, mean_q: 4.218767\n",
            " 73344/100000: episode: 7152, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000037, mae: 2.832485, mean_q: 4.264864\n",
            " 73352/100000: episode: 7153, duration: 0.175s, episode steps:   8, steps per second:  46, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.984787, mean_q: 4.438070\n",
            " 73361/100000: episode: 7154, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000039, mae: 2.923631, mean_q: 4.361919\n",
            " 73373/100000: episode: 7155, duration: 0.188s, episode steps:  12, steps per second:  64, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.417 [0.000, 7.000],  loss: 0.000067, mae: 2.960972, mean_q: 4.407316\n",
            " 73381/100000: episode: 7156, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.848920, mean_q: 4.278050\n",
            " 73389/100000: episode: 7157, duration: 0.158s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 3.208650, mean_q: 4.696999\n",
            " 73398/100000: episode: 7158, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000064, mae: 2.967021, mean_q: 4.420350\n",
            " 73408/100000: episode: 7159, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000054, mae: 2.909419, mean_q: 4.348845\n",
            " 73416/100000: episode: 7160, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.800384, mean_q: 4.224747\n",
            " 73426/100000: episode: 7161, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000038, mae: 3.068525, mean_q: 4.538436\n",
            " 73434/100000: episode: 7162, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000185, mae: 2.774911, mean_q: 4.189000\n",
            " 73443/100000: episode: 7163, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000268, mae: 2.820150, mean_q: 4.243273\n",
            " 73451/100000: episode: 7164, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000113, mae: 3.039528, mean_q: 4.497636\n",
            " 73460/100000: episode: 7165, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000341, mae: 3.070941, mean_q: 4.529507\n",
            " 73469/100000: episode: 7166, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000259, mae: 2.795601, mean_q: 4.220860\n",
            " 73479/100000: episode: 7167, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000127, mae: 2.915534, mean_q: 4.351406\n",
            " 73488/100000: episode: 7168, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000134, mae: 2.891684, mean_q: 4.328722\n",
            " 73500/100000: episode: 7169, duration: 0.128s, episode steps:  12, steps per second:  93, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 4.333 [0.000, 7.000],  loss: 0.000178, mae: 2.898183, mean_q: 4.342268\n",
            " 73508/100000: episode: 7170, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000218, mae: 2.884047, mean_q: 4.310240\n",
            " 73517/100000: episode: 7171, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000200, mae: 3.020267, mean_q: 4.476116\n",
            " 73525/100000: episode: 7172, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000183, mae: 3.200587, mean_q: 4.677960\n",
            " 73534/100000: episode: 7173, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000378, mae: 3.085364, mean_q: 4.537441\n",
            " 73542/100000: episode: 7174, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000262, mae: 2.892529, mean_q: 4.330971\n",
            " 73551/100000: episode: 7175, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000120, mae: 2.923054, mean_q: 4.352265\n",
            " 73559/100000: episode: 7176, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 3.122953, mean_q: 4.572221\n",
            " 73569/100000: episode: 7177, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000191, mae: 2.947814, mean_q: 4.390443\n",
            " 73577/100000: episode: 7178, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000134, mae: 3.001190, mean_q: 4.460347\n",
            " 73586/100000: episode: 7179, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000159, mae: 2.961370, mean_q: 4.408653\n",
            " 73596/100000: episode: 7180, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000125, mae: 2.576611, mean_q: 3.960770\n",
            " 73604/100000: episode: 7181, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000099, mae: 3.020875, mean_q: 4.478142\n",
            " 73612/100000: episode: 7182, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000147, mae: 3.200530, mean_q: 4.680394\n",
            " 73620/100000: episode: 7183, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000101, mae: 3.041837, mean_q: 4.498756\n",
            " 73628/100000: episode: 7184, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 3.076749, mean_q: 4.543441\n",
            " 73636/100000: episode: 7185, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.830850, mean_q: 4.254694\n",
            " 73644/100000: episode: 7186, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000085, mae: 2.952523, mean_q: 4.391159\n",
            " 73653/100000: episode: 7187, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000126, mae: 3.097091, mean_q: 4.560133\n",
            " 73661/100000: episode: 7188, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000185, mae: 2.969735, mean_q: 4.400928\n",
            " 73670/100000: episode: 7189, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000105, mae: 2.790865, mean_q: 4.197802\n",
            " 73678/100000: episode: 7190, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 3.116478, mean_q: 4.583849\n",
            " 73687/100000: episode: 7191, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000039, mae: 3.014838, mean_q: 4.464193\n",
            " 73695/100000: episode: 7192, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000152, mae: 2.982723, mean_q: 4.436580\n",
            " 73703/100000: episode: 7193, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.837976, mean_q: 4.267198\n",
            " 73711/100000: episode: 7194, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 2.893340, mean_q: 4.329175\n",
            " 73721/100000: episode: 7195, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000116, mae: 2.830970, mean_q: 4.257942\n",
            " 73730/100000: episode: 7196, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000056, mae: 2.996901, mean_q: 4.453306\n",
            " 73740/100000: episode: 7197, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.959147, mean_q: 4.415345\n",
            " 73750/100000: episode: 7198, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000040, mae: 2.868971, mean_q: 4.302674\n",
            " 73758/100000: episode: 7199, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 2.860371, mean_q: 4.281938\n",
            " 73769/100000: episode: 7200, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.091 [0.000, 7.000],  loss: 0.000065, mae: 2.839427, mean_q: 4.267727\n",
            " 73777/100000: episode: 7201, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.911656, mean_q: 4.351673\n",
            " 73785/100000: episode: 7202, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 3.023864, mean_q: 4.477945\n",
            " 73793/100000: episode: 7203, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.091924, mean_q: 4.567445\n",
            " 73801/100000: episode: 7204, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.895958, mean_q: 4.334598\n",
            " 73810/100000: episode: 7205, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000010, mae: 2.968033, mean_q: 4.413667\n",
            " 73818/100000: episode: 7206, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.900428, mean_q: 4.332577\n",
            " 73828/100000: episode: 7207, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000021, mae: 3.037682, mean_q: 4.501624\n",
            " 73837/100000: episode: 7208, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000035, mae: 2.941689, mean_q: 4.389539\n",
            " 73846/100000: episode: 7209, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000033, mae: 2.820842, mean_q: 4.250116\n",
            " 73855/100000: episode: 7210, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000049, mae: 2.999137, mean_q: 4.457677\n",
            " 73864/100000: episode: 7211, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000040, mae: 3.032537, mean_q: 4.495449\n",
            " 73874/100000: episode: 7212, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000023, mae: 2.993095, mean_q: 4.452918\n",
            " 73883/100000: episode: 7213, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000015, mae: 2.851781, mean_q: 4.286959\n",
            " 73891/100000: episode: 7214, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.915703, mean_q: 4.361259\n",
            " 73899/100000: episode: 7215, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.818353, mean_q: 4.241733\n",
            " 73908/100000: episode: 7216, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 3.032504, mean_q: 4.497238\n",
            " 73916/100000: episode: 7217, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.071828, mean_q: 4.534898\n",
            " 73924/100000: episode: 7218, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.925167, mean_q: 4.365398\n",
            " 73933/100000: episode: 7219, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.886054, mean_q: 4.326876\n",
            " 73941/100000: episode: 7220, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.189163, mean_q: 4.676791\n",
            " 73951/100000: episode: 7221, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000003, mae: 3.110467, mean_q: 4.592502\n",
            " 73960/100000: episode: 7222, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000009, mae: 2.668444, mean_q: 4.073301\n",
            " 73968/100000: episode: 7223, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.967686, mean_q: 4.416451\n",
            " 73976/100000: episode: 7224, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.131357, mean_q: 4.608457\n",
            " 73986/100000: episode: 7225, duration: 0.106s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000015, mae: 3.081278, mean_q: 4.550189\n",
            " 73995/100000: episode: 7226, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000008, mae: 2.925579, mean_q: 4.369386\n",
            " 74003/100000: episode: 7227, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.918214, mean_q: 4.363325\n",
            " 74012/100000: episode: 7228, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000018, mae: 2.912385, mean_q: 4.355944\n",
            " 74022/100000: episode: 7229, duration: 0.097s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000027, mae: 2.929592, mean_q: 4.369753\n",
            " 74030/100000: episode: 7230, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.924983, mean_q: 4.368850\n",
            " 74038/100000: episode: 7231, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.055433, mean_q: 4.521225\n",
            " 74046/100000: episode: 7232, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.003495, mean_q: 4.460734\n",
            " 74054/100000: episode: 7233, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.921658, mean_q: 4.368830\n",
            " 74062/100000: episode: 7234, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.101693, mean_q: 4.577084\n",
            " 74070/100000: episode: 7235, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.968194, mean_q: 4.418487\n",
            " 74079/100000: episode: 7236, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 2.830651, mean_q: 4.262998\n",
            " 74090/100000: episode: 7237, duration: 0.117s, episode steps:  11, steps per second:  94, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.091 [0.000, 7.000],  loss: 0.000007, mae: 2.871147, mean_q: 4.307457\n",
            " 74098/100000: episode: 7238, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.038052, mean_q: 4.503485\n",
            " 74106/100000: episode: 7239, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.946042, mean_q: 4.391425\n",
            " 74114/100000: episode: 7240, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.073601, mean_q: 4.539045\n",
            " 74123/100000: episode: 7241, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.968740, mean_q: 4.418908\n",
            " 74131/100000: episode: 7242, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.982805, mean_q: 4.437063\n",
            " 74139/100000: episode: 7243, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.970971, mean_q: 4.426797\n",
            " 74147/100000: episode: 7244, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.993672, mean_q: 4.453696\n",
            " 74156/100000: episode: 7245, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.955790, mean_q: 4.404022\n",
            " 74164/100000: episode: 7246, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.945276, mean_q: 4.396924\n",
            " 74174/100000: episode: 7247, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000004, mae: 3.078287, mean_q: 4.552061\n",
            " 74184/100000: episode: 7248, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000005, mae: 2.853631, mean_q: 4.287481\n",
            " 74195/100000: episode: 7249, duration: 0.122s, episode steps:  11, steps per second:  90, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000007, mae: 3.054752, mean_q: 4.519705\n",
            " 74204/100000: episode: 7250, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 3.094773, mean_q: 4.570796\n",
            " 74214/100000: episode: 7251, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000007, mae: 2.934479, mean_q: 4.379776\n",
            " 74223/100000: episode: 7252, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.668690, mean_q: 4.077664\n",
            " 74231/100000: episode: 7253, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.928867, mean_q: 4.371932\n",
            " 74239/100000: episode: 7254, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.823049, mean_q: 4.253321\n",
            " 74248/100000: episode: 7255, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000015, mae: 2.971247, mean_q: 4.422549\n",
            " 74257/100000: episode: 7256, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000014, mae: 2.975675, mean_q: 4.430161\n",
            " 74266/100000: episode: 7257, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000016, mae: 3.092322, mean_q: 4.566178\n",
            " 74274/100000: episode: 7258, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.250313, mean_q: 4.750410\n",
            " 74284/100000: episode: 7259, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000018, mae: 2.777541, mean_q: 4.199189\n",
            " 74295/100000: episode: 7260, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000011, mae: 2.951871, mean_q: 4.398881\n",
            " 74303/100000: episode: 7261, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.055430, mean_q: 4.519564\n",
            " 74312/100000: episode: 7262, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 2.948586, mean_q: 4.396051\n",
            " 74320/100000: episode: 7263, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.942474, mean_q: 4.389541\n",
            " 74328/100000: episode: 7264, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.161137, mean_q: 4.637489\n",
            " 74337/100000: episode: 7265, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000044, mae: 2.901842, mean_q: 4.343756\n",
            " 74345/100000: episode: 7266, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000097, mae: 2.941665, mean_q: 4.384274\n",
            " 74353/100000: episode: 7267, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000104, mae: 3.111073, mean_q: 4.581609\n",
            " 74363/100000: episode: 7268, duration: 0.142s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000056, mae: 3.062369, mean_q: 4.526092\n",
            " 74371/100000: episode: 7269, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.954459, mean_q: 4.402808\n",
            " 74381/100000: episode: 7270, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.863210, mean_q: 4.303728\n",
            " 74389/100000: episode: 7271, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.988169, mean_q: 4.442181\n",
            " 74398/100000: episode: 7272, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000020, mae: 2.837553, mean_q: 4.266577\n",
            " 74406/100000: episode: 7273, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.067785, mean_q: 4.533378\n",
            " 74414/100000: episode: 7274, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.985485, mean_q: 4.437347\n",
            " 74423/100000: episode: 7275, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000028, mae: 2.982635, mean_q: 4.436532\n",
            " 74431/100000: episode: 7276, duration: 0.157s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.836143, mean_q: 4.263677\n",
            " 74439/100000: episode: 7277, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.904700, mean_q: 4.349519\n",
            " 74449/100000: episode: 7278, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000025, mae: 2.930115, mean_q: 4.373548\n",
            " 74460/100000: episode: 7279, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000023, mae: 2.857976, mean_q: 4.293295\n",
            " 74468/100000: episode: 7280, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.182897, mean_q: 4.664835\n",
            " 74477/100000: episode: 7281, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000024, mae: 2.694437, mean_q: 4.097083\n",
            " 74485/100000: episode: 7282, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.814984, mean_q: 4.244416\n",
            " 74493/100000: episode: 7283, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.151236, mean_q: 4.631176\n",
            " 74501/100000: episode: 7284, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.906953, mean_q: 4.349349\n",
            " 74509/100000: episode: 7285, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.922245, mean_q: 4.360942\n",
            " 74518/100000: episode: 7286, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000048, mae: 2.905153, mean_q: 4.348206\n",
            " 74527/100000: episode: 7287, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000048, mae: 3.149595, mean_q: 4.625382\n",
            " 74537/100000: episode: 7288, duration: 0.222s, episode steps:  10, steps per second:  45, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000060, mae: 3.056281, mean_q: 4.518968\n",
            " 74545/100000: episode: 7289, duration: 0.200s, episode steps:   8, steps per second:  40, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.989087, mean_q: 4.436739\n",
            " 74553/100000: episode: 7290, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.969408, mean_q: 4.423479\n",
            " 74561/100000: episode: 7291, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 2.964262, mean_q: 4.417194\n",
            " 74570/100000: episode: 7292, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000041, mae: 2.899749, mean_q: 4.336748\n",
            " 74578/100000: episode: 7293, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.970398, mean_q: 4.420092\n",
            " 74587/100000: episode: 7294, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000012, mae: 3.034403, mean_q: 4.493990\n",
            " 74596/100000: episode: 7295, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000012, mae: 2.738940, mean_q: 4.154141\n",
            " 74604/100000: episode: 7296, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.993277, mean_q: 4.447201\n",
            " 74612/100000: episode: 7297, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.006407, mean_q: 4.463606\n",
            " 74620/100000: episode: 7298, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.964787, mean_q: 4.416339\n",
            " 74629/100000: episode: 7299, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000019, mae: 3.036363, mean_q: 4.494806\n",
            " 74637/100000: episode: 7300, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 2.920825, mean_q: 4.353753\n",
            " 74645/100000: episode: 7301, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000171, mae: 3.086884, mean_q: 4.557140\n",
            " 74654/100000: episode: 7302, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000074, mae: 3.000535, mean_q: 4.454172\n",
            " 74664/100000: episode: 7303, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000078, mae: 3.098058, mean_q: 4.566843\n",
            " 74673/100000: episode: 7304, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000098, mae: 3.187064, mean_q: 4.663831\n",
            " 74683/100000: episode: 7305, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000102, mae: 3.012974, mean_q: 4.463384\n",
            " 74693/100000: episode: 7306, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000205, mae: 2.470896, mean_q: 3.828858\n",
            " 74701/100000: episode: 7307, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000374, mae: 2.969538, mean_q: 4.418371\n",
            " 74710/100000: episode: 7308, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000367, mae: 3.026853, mean_q: 4.470142\n",
            " 74721/100000: episode: 7309, duration: 0.111s, episode steps:  11, steps per second:  99, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000235, mae: 3.001801, mean_q: 4.441038\n",
            " 74729/100000: episode: 7310, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000264, mae: 2.848884, mean_q: 4.271767\n",
            " 74739/100000: episode: 7311, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000149, mae: 3.051626, mean_q: 4.505575\n",
            " 74748/100000: episode: 7312, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000336, mae: 2.932026, mean_q: 4.366877\n",
            " 74756/100000: episode: 7313, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000435, mae: 3.094554, mean_q: 4.560688\n",
            " 74764/100000: episode: 7314, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000308, mae: 3.081783, mean_q: 4.540980\n",
            " 74772/100000: episode: 7315, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000403, mae: 2.765135, mean_q: 4.171249\n",
            " 74781/100000: episode: 7316, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000454, mae: 2.846920, mean_q: 4.255384\n",
            " 74789/100000: episode: 7317, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000422, mae: 2.988786, mean_q: 4.429286\n",
            " 74797/100000: episode: 7318, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000657, mae: 3.021797, mean_q: 4.495760\n",
            " 74805/100000: episode: 7319, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000723, mae: 2.927587, mean_q: 4.356482\n",
            " 74814/100000: episode: 7320, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000378, mae: 3.015668, mean_q: 4.469999\n",
            " 74823/100000: episode: 7321, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000216, mae: 3.172133, mean_q: 4.642146\n",
            " 74831/100000: episode: 7322, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000522, mae: 2.789120, mean_q: 4.206605\n",
            " 74840/100000: episode: 7323, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000657, mae: 2.678702, mean_q: 4.080550\n",
            " 74848/100000: episode: 7324, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000710, mae: 2.856295, mean_q: 4.288198\n",
            " 74856/100000: episode: 7325, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000907, mae: 3.007869, mean_q: 4.442034\n",
            " 74866/100000: episode: 7326, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.002185, mae: 2.724936, mean_q: 4.123796\n",
            " 74875/100000: episode: 7327, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000763, mae: 3.011785, mean_q: 4.437403\n",
            " 74884/100000: episode: 7328, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000702, mae: 2.923697, mean_q: 4.324426\n",
            " 74893/100000: episode: 7329, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000365, mae: 2.969255, mean_q: 4.408414\n",
            " 74901/100000: episode: 7330, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000335, mae: 2.949487, mean_q: 4.396650\n",
            " 74910/100000: episode: 7331, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000391, mae: 2.807735, mean_q: 4.229628\n",
            " 74921/100000: episode: 7332, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000119, mae: 2.919364, mean_q: 4.367661\n",
            " 74931/100000: episode: 7333, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000185, mae: 2.875274, mean_q: 4.322791\n",
            " 74939/100000: episode: 7334, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000156, mae: 2.898906, mean_q: 4.330844\n",
            " 74948/100000: episode: 7335, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000072, mae: 2.954811, mean_q: 4.399045\n",
            " 74956/100000: episode: 7336, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000109, mae: 2.985918, mean_q: 4.438303\n",
            " 74965/100000: episode: 7337, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000041, mae: 2.861841, mean_q: 4.300135\n",
            " 74973/100000: episode: 7338, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.942288, mean_q: 4.389332\n",
            " 74982/100000: episode: 7339, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000026, mae: 3.113865, mean_q: 4.586205\n",
            " 74990/100000: episode: 7340, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.998502, mean_q: 4.456348\n",
            " 74998/100000: episode: 7341, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.035854, mean_q: 4.492749\n",
            " 75007/100000: episode: 7342, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 2.914947, mean_q: 4.348242\n",
            " 75016/100000: episode: 7343, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000012, mae: 2.853032, mean_q: 4.287146\n",
            " 75024/100000: episode: 7344, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.863251, mean_q: 4.302799\n",
            " 75033/100000: episode: 7345, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 2.889999, mean_q: 4.331700\n",
            " 75042/100000: episode: 7346, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.986453, mean_q: 4.439968\n",
            " 75051/100000: episode: 7347, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000016, mae: 3.072339, mean_q: 4.541651\n",
            " 75059/100000: episode: 7348, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.843823, mean_q: 4.275036\n",
            " 75068/100000: episode: 7349, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.872622, mean_q: 4.311608\n",
            " 75078/100000: episode: 7350, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000002, mae: 2.902309, mean_q: 4.345845\n",
            " 75086/100000: episode: 7351, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.852492, mean_q: 4.287594\n",
            " 75094/100000: episode: 7352, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.997585, mean_q: 4.451809\n",
            " 75102/100000: episode: 7353, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.964003, mean_q: 4.413304\n",
            " 75110/100000: episode: 7354, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.976441, mean_q: 4.425850\n",
            " 75121/100000: episode: 7355, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000002, mae: 3.126911, mean_q: 4.601277\n",
            " 75130/100000: episode: 7356, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.068500, mean_q: 4.533435\n",
            " 75138/100000: episode: 7357, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.925072, mean_q: 4.370423\n",
            " 75146/100000: episode: 7358, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.006034, mean_q: 4.467272\n",
            " 75155/100000: episode: 7359, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.838313, mean_q: 4.273371\n",
            " 75163/100000: episode: 7360, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.143862, mean_q: 4.626554\n",
            " 75172/100000: episode: 7361, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.916618, mean_q: 4.362086\n",
            " 75180/100000: episode: 7362, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.985969, mean_q: 4.436938\n",
            " 75188/100000: episode: 7363, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.969640, mean_q: 4.419066\n",
            " 75196/100000: episode: 7364, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.046607, mean_q: 4.512558\n",
            " 75204/100000: episode: 7365, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.042816, mean_q: 4.505387\n",
            " 75214/100000: episode: 7366, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000002, mae: 2.825716, mean_q: 4.259873\n",
            " 75223/100000: episode: 7367, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 3.070067, mean_q: 4.540117\n",
            " 75231/100000: episode: 7368, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.730950, mean_q: 4.153179\n",
            " 75241/100000: episode: 7369, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000001, mae: 3.030955, mean_q: 4.493246\n",
            " 75251/100000: episode: 7370, duration: 0.144s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000000, mae: 2.919711, mean_q: 4.363414\n",
            " 75259/100000: episode: 7371, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.897585, mean_q: 4.339155\n",
            " 75269/100000: episode: 7372, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 2.948118, mean_q: 4.399072\n",
            " 75278/100000: episode: 7373, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.961991, mean_q: 4.412817\n",
            " 75287/100000: episode: 7374, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.871439, mean_q: 4.305372\n",
            " 75296/100000: episode: 7375, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.668199, mean_q: 4.074265\n",
            " 75304/100000: episode: 7376, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.717937, mean_q: 4.129669\n",
            " 75312/100000: episode: 7377, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.956222, mean_q: 4.408878\n",
            " 75320/100000: episode: 7378, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.069813, mean_q: 4.541823\n",
            " 75328/100000: episode: 7379, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.974308, mean_q: 4.430432\n",
            " 75337/100000: episode: 7380, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.836253, mean_q: 4.266029\n",
            " 75347/100000: episode: 7381, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 2.989646, mean_q: 4.445809\n",
            " 75357/100000: episode: 7382, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 2.823013, mean_q: 4.256179\n",
            " 75366/100000: episode: 7383, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 3.003833, mean_q: 4.460557\n",
            " 75376/100000: episode: 7384, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000001, mae: 2.843925, mean_q: 4.278319\n",
            " 75385/100000: episode: 7385, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.072307, mean_q: 4.543505\n",
            " 75393/100000: episode: 7386, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.049052, mean_q: 4.518894\n",
            " 75401/100000: episode: 7387, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.792393, mean_q: 4.220573\n",
            " 75409/100000: episode: 7388, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.174793, mean_q: 4.659372\n",
            " 75417/100000: episode: 7389, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.990281, mean_q: 4.450291\n",
            " 75425/100000: episode: 7390, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.078107, mean_q: 4.546551\n",
            " 75433/100000: episode: 7391, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.014663, mean_q: 4.471661\n",
            " 75442/100000: episode: 7392, duration: 0.090s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.064393, mean_q: 4.531721\n",
            " 75450/100000: episode: 7393, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.951506, mean_q: 4.400192\n",
            " 75458/100000: episode: 7394, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.677062, mean_q: 4.080694\n",
            " 75467/100000: episode: 7395, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 2.975140, mean_q: 4.425911\n",
            " 75477/100000: episode: 7396, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 2.915760, mean_q: 4.358978\n",
            " 75487/100000: episode: 7397, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000001, mae: 3.069623, mean_q: 4.537752\n",
            " 75495/100000: episode: 7398, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.831137, mean_q: 4.263188\n",
            " 75503/100000: episode: 7399, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.929274, mean_q: 4.375063\n",
            " 75511/100000: episode: 7400, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.051254, mean_q: 4.516161\n",
            " 75520/100000: episode: 7401, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.927141, mean_q: 4.372581\n",
            " 75528/100000: episode: 7402, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.173055, mean_q: 4.655054\n",
            " 75536/100000: episode: 7403, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.863427, mean_q: 4.303097\n",
            " 75544/100000: episode: 7404, duration: 0.155s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.881070, mean_q: 4.322304\n",
            " 75552/100000: episode: 7405, duration: 0.196s, episode steps:   8, steps per second:  41, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.903714, mean_q: 4.344259\n",
            " 75561/100000: episode: 7406, duration: 0.218s, episode steps:   9, steps per second:  41, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 3.125737, mean_q: 4.602050\n",
            " 75570/100000: episode: 7407, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.647548, mean_q: 4.048858\n",
            " 75579/100000: episode: 7408, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.080091, mean_q: 4.553522\n",
            " 75588/100000: episode: 7409, duration: 0.152s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.963242, mean_q: 4.412298\n",
            " 75597/100000: episode: 7410, duration: 0.202s, episode steps:   9, steps per second:  45, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 2.937620, mean_q: 4.388796\n",
            " 75607/100000: episode: 7411, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000004, mae: 2.914680, mean_q: 4.353413\n",
            " 75615/100000: episode: 7412, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.064687, mean_q: 4.532545\n",
            " 75623/100000: episode: 7413, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.325715, mean_q: 4.834771\n",
            " 75632/100000: episode: 7414, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.039395, mean_q: 4.502845\n",
            " 75640/100000: episode: 7415, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.922926, mean_q: 4.365805\n",
            " 75649/100000: episode: 7416, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.900728, mean_q: 4.343608\n",
            " 75658/100000: episode: 7417, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 3.101307, mean_q: 4.573793\n",
            " 75667/100000: episode: 7418, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 3.113847, mean_q: 4.589472\n",
            " 75675/100000: episode: 7419, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.028389, mean_q: 4.482393\n",
            " 75684/100000: episode: 7420, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 3.008767, mean_q: 4.467939\n",
            " 75694/100000: episode: 7421, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000002, mae: 2.845534, mean_q: 4.280240\n",
            " 75703/100000: episode: 7422, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.961043, mean_q: 4.410521\n",
            " 75711/100000: episode: 7423, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.824336, mean_q: 4.256164\n",
            " 75722/100000: episode: 7424, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000010, mae: 2.933215, mean_q: 4.378387\n",
            " 75731/100000: episode: 7425, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000015, mae: 3.069124, mean_q: 4.538969\n",
            " 75739/100000: episode: 7426, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.962879, mean_q: 4.422812\n",
            " 75749/100000: episode: 7427, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000007, mae: 3.024395, mean_q: 4.486921\n",
            " 75757/100000: episode: 7428, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.133323, mean_q: 4.611427\n",
            " 75766/100000: episode: 7429, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.977379, mean_q: 4.429872\n",
            " 75776/100000: episode: 7430, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000007, mae: 3.207570, mean_q: 4.697033\n",
            " 75785/100000: episode: 7431, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.834115, mean_q: 4.268506\n",
            " 75793/100000: episode: 7432, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.959901, mean_q: 4.409221\n",
            " 75803/100000: episode: 7433, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000005, mae: 3.015798, mean_q: 4.475103\n",
            " 75811/100000: episode: 7434, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.019521, mean_q: 4.477942\n",
            " 75819/100000: episode: 7435, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.058434, mean_q: 4.528597\n",
            " 75828/100000: episode: 7436, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000009, mae: 2.982762, mean_q: 4.437923\n",
            " 75837/100000: episode: 7437, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.958772, mean_q: 4.410305\n",
            " 75846/100000: episode: 7438, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.929919, mean_q: 4.372747\n",
            " 75854/100000: episode: 7439, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.988248, mean_q: 4.445715\n",
            " 75863/100000: episode: 7440, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 3.074175, mean_q: 4.540886\n",
            " 75873/100000: episode: 7441, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000002, mae: 3.178901, mean_q: 4.668688\n",
            " 75883/100000: episode: 7442, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000002, mae: 3.009035, mean_q: 4.471363\n",
            " 75893/100000: episode: 7443, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.858231, mean_q: 4.295521\n",
            " 75901/100000: episode: 7444, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.122375, mean_q: 4.598787\n",
            " 75909/100000: episode: 7445, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.907516, mean_q: 4.349355\n",
            " 75917/100000: episode: 7446, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.052590, mean_q: 4.512671\n",
            " 75926/100000: episode: 7447, duration: 0.085s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.898781, mean_q: 4.334193\n",
            " 75935/100000: episode: 7448, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000016, mae: 3.028910, mean_q: 4.490974\n",
            " 75943/100000: episode: 7449, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.043057, mean_q: 4.519317\n",
            " 75951/100000: episode: 7450, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000100, mae: 2.724892, mean_q: 4.141635\n",
            " 75959/100000: episode: 7451, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 3.042116, mean_q: 4.495059\n",
            " 75967/100000: episode: 7452, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 3.100505, mean_q: 4.577350\n",
            " 75977/100000: episode: 7453, duration: 0.113s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000058, mae: 2.904883, mean_q: 4.339638\n",
            " 75986/100000: episode: 7454, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000059, mae: 2.780102, mean_q: 4.195227\n",
            " 75995/100000: episode: 7455, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000042, mae: 3.058059, mean_q: 4.528644\n",
            " 76003/100000: episode: 7456, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.981311, mean_q: 4.434688\n",
            " 76011/100000: episode: 7457, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.847905, mean_q: 4.281813\n",
            " 76022/100000: episode: 7458, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000048, mae: 2.791181, mean_q: 4.205095\n",
            " 76030/100000: episode: 7459, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.732491, mean_q: 4.142170\n",
            " 76038/100000: episode: 7460, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.905333, mean_q: 4.353280\n",
            " 76048/100000: episode: 7461, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000059, mae: 2.770273, mean_q: 4.192117\n",
            " 76057/100000: episode: 7462, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000057, mae: 2.816102, mean_q: 4.239957\n",
            " 76065/100000: episode: 7463, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 3.049942, mean_q: 4.508991\n",
            " 76073/100000: episode: 7464, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 3.197389, mean_q: 4.681935\n",
            " 76082/100000: episode: 7465, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000031, mae: 2.845637, mean_q: 4.281008\n",
            " 76093/100000: episode: 7466, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000018, mae: 3.017313, mean_q: 4.479474\n",
            " 76103/100000: episode: 7467, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000011, mae: 3.031457, mean_q: 4.501592\n",
            " 76113/100000: episode: 7468, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000025, mae: 2.969933, mean_q: 4.421739\n",
            " 76121/100000: episode: 7469, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.962228, mean_q: 4.410256\n",
            " 76129/100000: episode: 7470, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.887260, mean_q: 4.324935\n",
            " 76137/100000: episode: 7471, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.860962, mean_q: 4.297546\n",
            " 76145/100000: episode: 7472, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.889322, mean_q: 4.330839\n",
            " 76153/100000: episode: 7473, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.892186, mean_q: 4.327809\n",
            " 76162/100000: episode: 7474, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000016, mae: 3.145962, mean_q: 4.622186\n",
            " 76173/100000: episode: 7475, duration: 0.112s, episode steps:  11, steps per second:  99, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000019, mae: 2.851932, mean_q: 4.287124\n",
            " 76182/100000: episode: 7476, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000026, mae: 3.051775, mean_q: 4.515563\n",
            " 76191/100000: episode: 7477, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000028, mae: 2.786633, mean_q: 4.210499\n",
            " 76199/100000: episode: 7478, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.810441, mean_q: 4.239012\n",
            " 76207/100000: episode: 7479, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.757268, mean_q: 4.174610\n",
            " 76215/100000: episode: 7480, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.719705, mean_q: 4.135700\n",
            " 76223/100000: episode: 7481, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.703983, mean_q: 4.117887\n",
            " 76231/100000: episode: 7482, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.901872, mean_q: 4.342904\n",
            " 76240/100000: episode: 7483, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 2.965913, mean_q: 4.416851\n",
            " 76249/100000: episode: 7484, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000013, mae: 2.834833, mean_q: 4.267004\n",
            " 76258/100000: episode: 7485, duration: 0.151s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000014, mae: 2.981318, mean_q: 4.437252\n",
            " 76267/100000: episode: 7486, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000026, mae: 2.735726, mean_q: 4.146947\n",
            " 76276/100000: episode: 7487, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000042, mae: 2.999123, mean_q: 4.454069\n",
            " 76286/100000: episode: 7488, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000038, mae: 2.968373, mean_q: 4.421649\n",
            " 76295/100000: episode: 7489, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000053, mae: 2.683260, mean_q: 4.091944\n",
            " 76303/100000: episode: 7490, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 2.915395, mean_q: 4.353862\n",
            " 76311/100000: episode: 7491, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.917343, mean_q: 4.363459\n",
            " 76319/100000: episode: 7492, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 3.023823, mean_q: 4.482396\n",
            " 76328/100000: episode: 7493, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000078, mae: 3.021031, mean_q: 4.474437\n",
            " 76338/100000: episode: 7494, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000115, mae: 2.970211, mean_q: 4.410424\n",
            " 76347/100000: episode: 7495, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000091, mae: 3.031772, mean_q: 4.485907\n",
            " 76356/100000: episode: 7496, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000071, mae: 2.941514, mean_q: 4.387610\n",
            " 76364/100000: episode: 7497, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000202, mae: 2.789788, mean_q: 4.195613\n",
            " 76374/100000: episode: 7498, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000106, mae: 2.806245, mean_q: 4.217875\n",
            " 76382/100000: episode: 7499, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000120, mae: 2.904048, mean_q: 4.322415\n",
            " 76391/100000: episode: 7500, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000205, mae: 2.984482, mean_q: 4.436441\n",
            " 76399/100000: episode: 7501, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000283, mae: 2.983665, mean_q: 4.440520\n",
            " 76408/100000: episode: 7502, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000648, mae: 2.742820, mean_q: 4.155848\n",
            " 76416/100000: episode: 7503, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000385, mae: 2.923732, mean_q: 4.351709\n",
            " 76426/100000: episode: 7504, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000193, mae: 3.087498, mean_q: 4.562971\n",
            " 76434/100000: episode: 7505, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000260, mae: 2.980628, mean_q: 4.444757\n",
            " 76442/100000: episode: 7506, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000127, mae: 2.979091, mean_q: 4.424510\n",
            " 76450/100000: episode: 7507, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000126, mae: 2.844906, mean_q: 4.253085\n",
            " 76459/100000: episode: 7508, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000341, mae: 2.838857, mean_q: 4.267147\n",
            " 76469/100000: episode: 7509, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000463, mae: 2.911165, mean_q: 4.337945\n",
            " 76479/100000: episode: 7510, duration: 0.104s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000263, mae: 3.137518, mean_q: 4.596685\n",
            " 76488/100000: episode: 7511, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000199, mae: 3.044329, mean_q: 4.497447\n",
            " 76497/100000: episode: 7512, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000147, mae: 3.011405, mean_q: 4.453770\n",
            " 76505/100000: episode: 7513, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000210, mae: 2.939943, mean_q: 4.372454\n",
            " 76514/100000: episode: 7514, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000084, mae: 3.001008, mean_q: 4.457562\n",
            " 76523/100000: episode: 7515, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000075, mae: 2.947277, mean_q: 4.396353\n",
            " 76532/100000: episode: 7516, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000127, mae: 2.899755, mean_q: 4.343519\n",
            " 76540/100000: episode: 7517, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000117, mae: 3.053366, mean_q: 4.531449\n",
            " 76549/100000: episode: 7518, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000116, mae: 3.032093, mean_q: 4.497490\n",
            " 76557/100000: episode: 7519, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000088, mae: 2.941505, mean_q: 4.391576\n",
            " 76565/100000: episode: 7520, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000098, mae: 2.688961, mean_q: 4.089232\n",
            " 76573/100000: episode: 7521, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000081, mae: 2.892240, mean_q: 4.330719\n",
            " 76582/100000: episode: 7522, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000051, mae: 2.826497, mean_q: 4.260245\n",
            " 76591/100000: episode: 7523, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000019, mae: 2.913358, mean_q: 4.355436\n",
            " 76600/100000: episode: 7524, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000032, mae: 3.144475, mean_q: 4.618845\n",
            " 76608/100000: episode: 7525, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.808786, mean_q: 4.232795\n",
            " 76618/100000: episode: 7526, duration: 0.160s, episode steps:  10, steps per second:  63, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000027, mae: 2.914633, mean_q: 4.354039\n",
            " 76626/100000: episode: 7527, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 3.013896, mean_q: 4.467538\n",
            " 76634/100000: episode: 7528, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 3.213794, mean_q: 4.696824\n",
            " 76643/100000: episode: 7529, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000030, mae: 2.944818, mean_q: 4.389777\n",
            " 76652/100000: episode: 7530, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000025, mae: 2.982018, mean_q: 4.435350\n",
            " 76662/100000: episode: 7531, duration: 0.177s, episode steps:  10, steps per second:  57, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000013, mae: 2.902601, mean_q: 4.342248\n",
            " 76670/100000: episode: 7532, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.952286, mean_q: 4.401758\n",
            " 76681/100000: episode: 7533, duration: 0.188s, episode steps:  11, steps per second:  59, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000013, mae: 3.038630, mean_q: 4.497440\n",
            " 76691/100000: episode: 7534, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000005, mae: 2.878022, mean_q: 4.321312\n",
            " 76700/100000: episode: 7535, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000008, mae: 2.980089, mean_q: 4.435306\n",
            " 76709/100000: episode: 7536, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000007, mae: 3.031569, mean_q: 4.493331\n",
            " 76717/100000: episode: 7537, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.944724, mean_q: 4.396521\n",
            " 76725/100000: episode: 7538, duration: 0.147s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.958758, mean_q: 4.411303\n",
            " 76734/100000: episode: 7539, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.129862, mean_q: 4.606848\n",
            " 76742/100000: episode: 7540, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.877003, mean_q: 4.313987\n",
            " 76750/100000: episode: 7541, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.082232, mean_q: 4.550969\n",
            " 76758/100000: episode: 7542, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.019469, mean_q: 4.483018\n",
            " 76766/100000: episode: 7543, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.011084, mean_q: 4.467960\n",
            " 76775/100000: episode: 7544, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.964216, mean_q: 4.417734\n",
            " 76785/100000: episode: 7545, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000001, mae: 3.126548, mean_q: 4.601233\n",
            " 76794/100000: episode: 7546, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 3.007154, mean_q: 4.466040\n",
            " 76802/100000: episode: 7547, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.052622, mean_q: 4.513594\n",
            " 76810/100000: episode: 7548, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.828727, mean_q: 4.255712\n",
            " 76818/100000: episode: 7549, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.935899, mean_q: 4.382897\n",
            " 76826/100000: episode: 7550, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.179798, mean_q: 4.667918\n",
            " 76835/100000: episode: 7551, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.817729, mean_q: 4.248079\n",
            " 76843/100000: episode: 7552, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.086370, mean_q: 4.558275\n",
            " 76852/100000: episode: 7553, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 2.967141, mean_q: 4.416784\n",
            " 76861/100000: episode: 7554, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 2.793629, mean_q: 4.217021\n",
            " 76869/100000: episode: 7555, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.948329, mean_q: 4.399961\n",
            " 76879/100000: episode: 7556, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.050155, mean_q: 4.515826\n",
            " 76889/100000: episode: 7557, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000005, mae: 2.970656, mean_q: 4.422506\n",
            " 76897/100000: episode: 7558, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.941827, mean_q: 4.393871\n",
            " 76907/100000: episode: 7559, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000005, mae: 2.847042, mean_q: 4.286260\n",
            " 76917/100000: episode: 7560, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000002, mae: 3.084376, mean_q: 4.554730\n",
            " 76925/100000: episode: 7561, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.898704, mean_q: 4.337316\n",
            " 76933/100000: episode: 7562, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.039753, mean_q: 4.505089\n",
            " 76945/100000: episode: 7563, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.167 [0.000, 7.000],  loss: 0.000004, mae: 3.050754, mean_q: 4.515702\n",
            " 76954/100000: episode: 7564, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 2.904193, mean_q: 4.343798\n",
            " 76962/100000: episode: 7565, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.067070, mean_q: 4.531365\n",
            " 76972/100000: episode: 7566, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000005, mae: 2.841197, mean_q: 4.273381\n",
            " 76980/100000: episode: 7567, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.989688, mean_q: 4.448928\n",
            " 76988/100000: episode: 7568, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.833628, mean_q: 4.268700\n",
            " 76998/100000: episode: 7569, duration: 0.127s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000006, mae: 3.004238, mean_q: 4.462525\n",
            " 77006/100000: episode: 7570, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.828961, mean_q: 4.257558\n",
            " 77015/100000: episode: 7571, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 2.965371, mean_q: 4.416885\n",
            " 77023/100000: episode: 7572, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.963630, mean_q: 4.414964\n",
            " 77031/100000: episode: 7573, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.105189, mean_q: 4.582971\n",
            " 77039/100000: episode: 7574, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.029397, mean_q: 4.490786\n",
            " 77048/100000: episode: 7575, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 2.992386, mean_q: 4.452004\n",
            " 77056/100000: episode: 7576, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.899320, mean_q: 4.341761\n",
            " 77064/100000: episode: 7577, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.996643, mean_q: 4.456093\n",
            " 77073/100000: episode: 7578, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000009, mae: 2.778745, mean_q: 4.200955\n",
            " 77081/100000: episode: 7579, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.976727, mean_q: 4.433275\n",
            " 77089/100000: episode: 7580, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.958215, mean_q: 4.409708\n",
            " 77097/100000: episode: 7581, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.032725, mean_q: 4.496637\n",
            " 77106/100000: episode: 7582, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 2.733554, mean_q: 4.150379\n",
            " 77114/100000: episode: 7583, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.826166, mean_q: 4.258754\n",
            " 77123/100000: episode: 7584, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.959072, mean_q: 4.411146\n",
            " 77131/100000: episode: 7585, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.080390, mean_q: 4.552491\n",
            " 77139/100000: episode: 7586, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.121735, mean_q: 4.597466\n",
            " 77148/100000: episode: 7587, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 3.014740, mean_q: 4.474847\n",
            " 77156/100000: episode: 7588, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.033284, mean_q: 4.495360\n",
            " 77167/100000: episode: 7589, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000003, mae: 2.896783, mean_q: 4.338530\n",
            " 77175/100000: episode: 7590, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.120891, mean_q: 4.600539\n",
            " 77183/100000: episode: 7591, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.158667, mean_q: 4.642061\n",
            " 77194/100000: episode: 7592, duration: 0.107s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000004, mae: 3.150182, mean_q: 4.635513\n",
            " 77204/100000: episode: 7593, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.181790, mean_q: 4.666932\n",
            " 77212/100000: episode: 7594, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.906410, mean_q: 4.352433\n",
            " 77220/100000: episode: 7595, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.905980, mean_q: 4.348325\n",
            " 77230/100000: episode: 7596, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000010, mae: 3.026541, mean_q: 4.489267\n",
            " 77239/100000: episode: 7597, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000010, mae: 2.854177, mean_q: 4.289728\n",
            " 77247/100000: episode: 7598, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.764775, mean_q: 4.185025\n",
            " 77255/100000: episode: 7599, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.085300, mean_q: 4.552283\n",
            " 77265/100000: episode: 7600, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000003, mae: 2.941556, mean_q: 4.389119\n",
            " 77273/100000: episode: 7601, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.945258, mean_q: 4.393290\n",
            " 77282/100000: episode: 7602, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000004, mae: 2.948931, mean_q: 4.399020\n",
            " 77290/100000: episode: 7603, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.016071, mean_q: 4.479706\n",
            " 77298/100000: episode: 7604, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.665443, mean_q: 4.069867\n",
            " 77307/100000: episode: 7605, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.819283, mean_q: 4.245965\n",
            " 77316/100000: episode: 7606, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.826440, mean_q: 4.255450\n",
            " 77324/100000: episode: 7607, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.792182, mean_q: 4.216858\n",
            " 77333/100000: episode: 7608, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.764283, mean_q: 4.185350\n",
            " 77342/100000: episode: 7609, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.987509, mean_q: 4.437540\n",
            " 77352/100000: episode: 7610, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000002, mae: 2.924995, mean_q: 4.368934\n",
            " 77361/100000: episode: 7611, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.957888, mean_q: 4.401953\n",
            " 77371/100000: episode: 7612, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000002, mae: 3.097550, mean_q: 4.573611\n",
            " 77380/100000: episode: 7613, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.953189, mean_q: 4.402193\n",
            " 77389/100000: episode: 7614, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 2.963853, mean_q: 4.418879\n",
            " 77398/100000: episode: 7615, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.905731, mean_q: 4.348719\n",
            " 77408/100000: episode: 7616, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000003, mae: 2.953680, mean_q: 4.406711\n",
            " 77417/100000: episode: 7617, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 3.017762, mean_q: 4.477438\n",
            " 77425/100000: episode: 7618, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.027795, mean_q: 4.485460\n",
            " 77433/100000: episode: 7619, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.133002, mean_q: 4.610585\n",
            " 77441/100000: episode: 7620, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.001017, mean_q: 4.461719\n",
            " 77449/100000: episode: 7621, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.117560, mean_q: 4.592924\n",
            " 77457/100000: episode: 7622, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.175842, mean_q: 4.659851\n",
            " 77465/100000: episode: 7623, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.788846, mean_q: 4.214594\n",
            " 77473/100000: episode: 7624, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.917905, mean_q: 4.358527\n",
            " 77484/100000: episode: 7625, duration: 0.126s, episode steps:  11, steps per second:  87, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.091 [0.000, 7.000],  loss: 0.000006, mae: 3.075426, mean_q: 4.541565\n",
            " 77492/100000: episode: 7626, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.153069, mean_q: 4.635523\n",
            " 77500/100000: episode: 7627, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.971467, mean_q: 4.420730\n",
            " 77509/100000: episode: 7628, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 2.877738, mean_q: 4.315360\n",
            " 77517/100000: episode: 7629, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.911767, mean_q: 4.358932\n",
            " 77526/100000: episode: 7630, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000012, mae: 3.031980, mean_q: 4.499239\n",
            " 77535/100000: episode: 7631, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 2.894805, mean_q: 4.342771\n",
            " 77543/100000: episode: 7632, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.978051, mean_q: 4.437454\n",
            " 77553/100000: episode: 7633, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000030, mae: 3.005872, mean_q: 4.470063\n",
            " 77561/100000: episode: 7634, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.928711, mean_q: 4.377230\n",
            " 77570/100000: episode: 7635, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 2.963413, mean_q: 4.409731\n",
            " 77579/100000: episode: 7636, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000024, mae: 2.814763, mean_q: 4.236917\n",
            " 77588/100000: episode: 7637, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000042, mae: 3.123311, mean_q: 4.600399\n",
            " 77596/100000: episode: 7638, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.722809, mean_q: 4.136064\n",
            " 77604/100000: episode: 7639, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 3.069257, mean_q: 4.537902\n",
            " 77613/100000: episode: 7640, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000049, mae: 2.983251, mean_q: 4.439264\n",
            " 77623/100000: episode: 7641, duration: 0.149s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000044, mae: 3.073232, mean_q: 4.542425\n",
            " 77631/100000: episode: 7642, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 2.791629, mean_q: 4.212292\n",
            " 77639/100000: episode: 7643, duration: 0.162s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000129, mae: 3.061473, mean_q: 4.514663\n",
            " 77647/100000: episode: 7644, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000153, mae: 3.018514, mean_q: 4.478859\n",
            " 77655/100000: episode: 7645, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000219, mae: 2.835169, mean_q: 4.268390\n",
            " 77663/100000: episode: 7646, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000316, mae: 3.042272, mean_q: 4.502166\n",
            " 77673/100000: episode: 7647, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000327, mae: 2.846400, mean_q: 4.277932\n",
            " 77682/100000: episode: 7648, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000245, mae: 2.978092, mean_q: 4.415312\n",
            " 77690/100000: episode: 7649, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000170, mae: 3.046414, mean_q: 4.505164\n",
            " 77698/100000: episode: 7650, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000335, mae: 2.916002, mean_q: 4.347621\n",
            " 77708/100000: episode: 7651, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000202, mae: 2.934620, mean_q: 4.368085\n",
            " 77716/100000: episode: 7652, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.930730, mean_q: 4.376963\n",
            " 77724/100000: episode: 7653, duration: 0.151s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000104, mae: 3.228045, mean_q: 4.714637\n",
            " 77733/100000: episode: 7654, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000118, mae: 2.864234, mean_q: 4.295205\n",
            " 77744/100000: episode: 7655, duration: 0.169s, episode steps:  11, steps per second:  65, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000062, mae: 3.017852, mean_q: 4.478209\n",
            " 77752/100000: episode: 7656, duration: 0.144s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 3.306901, mean_q: 4.807354\n",
            " 77760/100000: episode: 7657, duration: 0.178s, episode steps:   8, steps per second:  45, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.903911, mean_q: 4.325379\n",
            " 77769/100000: episode: 7658, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000130, mae: 2.691727, mean_q: 4.090059\n",
            " 77777/100000: episode: 7659, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000240, mae: 2.929798, mean_q: 4.369734\n",
            " 77787/100000: episode: 7660, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000236, mae: 2.755158, mean_q: 4.163039\n",
            " 77795/100000: episode: 7661, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000173, mae: 2.732868, mean_q: 4.149923\n",
            " 77804/100000: episode: 7662, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000156, mae: 2.870959, mean_q: 4.310664\n",
            " 77813/100000: episode: 7663, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000146, mae: 2.811742, mean_q: 4.229791\n",
            " 77821/100000: episode: 7664, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000113, mae: 2.779086, mean_q: 4.197520\n",
            " 77830/100000: episode: 7665, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000350, mae: 2.726338, mean_q: 4.143577\n",
            " 77838/100000: episode: 7666, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000474, mae: 3.051285, mean_q: 4.519471\n",
            " 77846/100000: episode: 7667, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000403, mae: 3.023361, mean_q: 4.475399\n",
            " 77854/100000: episode: 7668, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000524, mae: 3.028856, mean_q: 4.480041\n",
            " 77863/100000: episode: 7669, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000253, mae: 2.902393, mean_q: 4.333465\n",
            " 77874/100000: episode: 7670, duration: 0.121s, episode steps:  11, steps per second:  91, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.909 [0.000, 7.000],  loss: 0.000121, mae: 2.875540, mean_q: 4.302561\n",
            " 77883/100000: episode: 7671, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000056, mae: 3.116327, mean_q: 4.588614\n",
            " 77891/100000: episode: 7672, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.845162, mean_q: 4.280753\n",
            " 77899/100000: episode: 7673, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.889780, mean_q: 4.328083\n",
            " 77907/100000: episode: 7674, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.899927, mean_q: 4.330954\n",
            " 77915/100000: episode: 7675, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.014822, mean_q: 4.471591\n",
            " 77923/100000: episode: 7676, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.042874, mean_q: 4.508753\n",
            " 77931/100000: episode: 7677, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.697217, mean_q: 4.109334\n",
            " 77941/100000: episode: 7678, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000035, mae: 3.135768, mean_q: 4.613614\n",
            " 77949/100000: episode: 7679, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.038021, mean_q: 4.492759\n",
            " 77957/100000: episode: 7680, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 3.138438, mean_q: 4.614001\n",
            " 77965/100000: episode: 7681, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.021711, mean_q: 4.484250\n",
            " 77973/100000: episode: 7682, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.075465, mean_q: 4.544023\n",
            " 77983/100000: episode: 7683, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.935622, mean_q: 4.378950\n",
            " 77991/100000: episode: 7684, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.844384, mean_q: 4.273870\n",
            " 77999/100000: episode: 7685, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.088559, mean_q: 4.557639\n",
            " 78009/100000: episode: 7686, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000008, mae: 2.872965, mean_q: 4.310472\n",
            " 78018/100000: episode: 7687, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000010, mae: 2.886225, mean_q: 4.323593\n",
            " 78027/100000: episode: 7688, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000008, mae: 2.834361, mean_q: 4.261942\n",
            " 78036/100000: episode: 7689, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 2.997391, mean_q: 4.452169\n",
            " 78044/100000: episode: 7690, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.193502, mean_q: 4.682264\n",
            " 78053/100000: episode: 7691, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000010, mae: 2.878392, mean_q: 4.315690\n",
            " 78063/100000: episode: 7692, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000005, mae: 2.994183, mean_q: 4.448541\n",
            " 78073/100000: episode: 7693, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000008, mae: 2.900235, mean_q: 4.343279\n",
            " 78082/100000: episode: 7694, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.941768, mean_q: 4.389303\n",
            " 78092/100000: episode: 7695, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000027, mae: 2.838095, mean_q: 4.270328\n",
            " 78101/100000: episode: 7696, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000009, mae: 2.844673, mean_q: 4.275995\n",
            " 78110/100000: episode: 7697, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000009, mae: 2.978297, mean_q: 4.433682\n",
            " 78118/100000: episode: 7698, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.096797, mean_q: 4.563119\n",
            " 78128/100000: episode: 7699, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000008, mae: 2.952162, mean_q: 4.397616\n",
            " 78138/100000: episode: 7700, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000009, mae: 3.029023, mean_q: 4.487828\n",
            " 78146/100000: episode: 7701, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.901541, mean_q: 4.344357\n",
            " 78157/100000: episode: 7702, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000015, mae: 2.912472, mean_q: 4.356074\n",
            " 78166/100000: episode: 7703, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 2.976137, mean_q: 4.431904\n",
            " 78175/100000: episode: 7704, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000030, mae: 2.590423, mean_q: 3.976638\n",
            " 78183/100000: episode: 7705, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.925305, mean_q: 4.368952\n",
            " 78194/100000: episode: 7706, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000012, mae: 3.049987, mean_q: 4.512582\n",
            " 78202/100000: episode: 7707, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.962680, mean_q: 4.413958\n",
            " 78211/100000: episode: 7708, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000029, mae: 2.928798, mean_q: 4.369092\n",
            " 78220/100000: episode: 7709, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000047, mae: 2.913986, mean_q: 4.348902\n",
            " 78228/100000: episode: 7710, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.809303, mean_q: 4.234791\n",
            " 78236/100000: episode: 7711, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.828448, mean_q: 4.256412\n",
            " 78245/100000: episode: 7712, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 3.092567, mean_q: 4.561337\n",
            " 78253/100000: episode: 7713, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.791747, mean_q: 4.213649\n",
            " 78262/100000: episode: 7714, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000023, mae: 2.920880, mean_q: 4.366544\n",
            " 78271/100000: episode: 7715, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000044, mae: 3.153160, mean_q: 4.634756\n",
            " 78280/100000: episode: 7716, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000117, mae: 2.775711, mean_q: 4.189209\n",
            " 78289/100000: episode: 7717, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000173, mae: 2.933390, mean_q: 4.373912\n",
            " 78297/100000: episode: 7718, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000086, mae: 3.038644, mean_q: 4.499381\n",
            " 78305/100000: episode: 7719, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000140, mae: 3.306575, mean_q: 4.809651\n",
            " 78313/100000: episode: 7720, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000158, mae: 2.875243, mean_q: 4.310114\n",
            " 78321/100000: episode: 7721, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000184, mae: 3.054609, mean_q: 4.521244\n",
            " 78329/100000: episode: 7722, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000130, mae: 3.177788, mean_q: 4.658123\n",
            " 78337/100000: episode: 7723, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 2.936181, mean_q: 4.379474\n",
            " 78345/100000: episode: 7724, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.796465, mean_q: 4.216971\n",
            " 78353/100000: episode: 7725, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 2.880836, mean_q: 4.309696\n",
            " 78361/100000: episode: 7726, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.989603, mean_q: 4.435210\n",
            " 78370/100000: episode: 7727, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000021, mae: 2.818085, mean_q: 4.246552\n",
            " 78379/100000: episode: 7728, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.903489, mean_q: 4.343243\n",
            " 78387/100000: episode: 7729, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.080520, mean_q: 4.546972\n",
            " 78395/100000: episode: 7730, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.955692, mean_q: 4.413592\n",
            " 78405/100000: episode: 7731, duration: 0.112s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000007, mae: 3.066383, mean_q: 4.533626\n",
            " 78415/100000: episode: 7732, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000008, mae: 2.757805, mean_q: 4.175942\n",
            " 78423/100000: episode: 7733, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.065398, mean_q: 4.529772\n",
            " 78431/100000: episode: 7734, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.179219, mean_q: 4.661136\n",
            " 78439/100000: episode: 7735, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.890992, mean_q: 4.330613\n",
            " 78448/100000: episode: 7736, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.901679, mean_q: 4.338212\n",
            " 78458/100000: episode: 7737, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 2.911792, mean_q: 4.353267\n",
            " 78468/100000: episode: 7738, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 2.876123, mean_q: 4.314271\n",
            " 78477/100000: episode: 7739, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.776447, mean_q: 4.195356\n",
            " 78485/100000: episode: 7740, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 3.016792, mean_q: 4.471217\n",
            " 78493/100000: episode: 7741, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.970245, mean_q: 4.427623\n",
            " 78503/100000: episode: 7742, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000023, mae: 3.027188, mean_q: 4.491798\n",
            " 78512/100000: episode: 7743, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000019, mae: 3.015169, mean_q: 4.471640\n",
            " 78522/100000: episode: 7744, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000025, mae: 2.884052, mean_q: 4.320506\n",
            " 78530/100000: episode: 7745, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.137588, mean_q: 4.611966\n",
            " 78538/100000: episode: 7746, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.963097, mean_q: 4.413641\n",
            " 78546/100000: episode: 7747, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.898087, mean_q: 4.334061\n",
            " 78554/100000: episode: 7748, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.857303, mean_q: 4.296848\n",
            " 78562/100000: episode: 7749, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.799021, mean_q: 4.222832\n",
            " 78570/100000: episode: 7750, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.912379, mean_q: 4.356984\n",
            " 78578/100000: episode: 7751, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.008578, mean_q: 4.460621\n",
            " 78586/100000: episode: 7752, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.963028, mean_q: 4.407190\n",
            " 78595/100000: episode: 7753, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000006, mae: 2.850198, mean_q: 4.283598\n",
            " 78606/100000: episode: 7754, duration: 0.144s, episode steps:  11, steps per second:  77, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000005, mae: 2.922924, mean_q: 4.369980\n",
            " 78617/100000: episode: 7755, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000006, mae: 2.898885, mean_q: 4.338740\n",
            " 78626/100000: episode: 7756, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.609045, mean_q: 4.010947\n",
            " 78634/100000: episode: 7757, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.943990, mean_q: 4.385201\n",
            " 78642/100000: episode: 7758, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.128730, mean_q: 4.605776\n",
            " 78650/100000: episode: 7759, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.873539, mean_q: 4.307052\n",
            " 78658/100000: episode: 7760, duration: 0.155s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.957801, mean_q: 4.407160\n",
            " 78667/100000: episode: 7761, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.914515, mean_q: 4.359423\n",
            " 78675/100000: episode: 7762, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.059563, mean_q: 4.520458\n",
            " 78686/100000: episode: 7763, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000006, mae: 2.965193, mean_q: 4.419882\n",
            " 78695/100000: episode: 7764, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.995854, mean_q: 4.456551\n",
            " 78703/100000: episode: 7765, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.887397, mean_q: 4.324539\n",
            " 78714/100000: episode: 7766, duration: 0.169s, episode steps:  11, steps per second:  65, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000004, mae: 2.917480, mean_q: 4.360450\n",
            " 78722/100000: episode: 7767, duration: 0.147s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.792685, mean_q: 4.217948\n",
            " 78732/100000: episode: 7768, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.888908, mean_q: 4.329512\n",
            " 78743/100000: episode: 7769, duration: 0.155s, episode steps:  11, steps per second:  71, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000005, mae: 2.908976, mean_q: 4.353350\n",
            " 78752/100000: episode: 7770, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000010, mae: 2.976762, mean_q: 4.425133\n",
            " 78760/100000: episode: 7771, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.894581, mean_q: 4.336589\n",
            " 78768/100000: episode: 7772, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.978726, mean_q: 4.432584\n",
            " 78777/100000: episode: 7773, duration: 0.170s, episode steps:   9, steps per second:  53, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000010, mae: 3.074512, mean_q: 4.544447\n",
            " 78787/100000: episode: 7774, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.848523, mean_q: 4.282298\n",
            " 78795/100000: episode: 7775, duration: 0.161s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.004401, mean_q: 4.462461\n",
            " 78803/100000: episode: 7776, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.094112, mean_q: 4.559484\n",
            " 78811/100000: episode: 7777, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.046698, mean_q: 4.508988\n",
            " 78819/100000: episode: 7778, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.927586, mean_q: 4.370539\n",
            " 78827/100000: episode: 7779, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.944609, mean_q: 4.396720\n",
            " 78835/100000: episode: 7780, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.730841, mean_q: 4.141118\n",
            " 78843/100000: episode: 7781, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.931898, mean_q: 4.374768\n",
            " 78853/100000: episode: 7782, duration: 0.174s, episode steps:  10, steps per second:  57, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.711617, mean_q: 4.120508\n",
            " 78861/100000: episode: 7783, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.853356, mean_q: 4.290938\n",
            " 78869/100000: episode: 7784, duration: 0.137s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.895212, mean_q: 4.335920\n",
            " 78878/100000: episode: 7785, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000018, mae: 2.858605, mean_q: 4.288607\n",
            " 78886/100000: episode: 7786, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.817910, mean_q: 4.242109\n",
            " 78897/100000: episode: 7787, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000016, mae: 2.813374, mean_q: 4.234696\n",
            " 78905/100000: episode: 7788, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.984757, mean_q: 4.433544\n",
            " 78914/100000: episode: 7789, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 3.024728, mean_q: 4.485820\n",
            " 78923/100000: episode: 7790, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000009, mae: 2.950272, mean_q: 4.397148\n",
            " 78931/100000: episode: 7791, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.047789, mean_q: 4.507510\n",
            " 78939/100000: episode: 7792, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.931787, mean_q: 4.373165\n",
            " 78947/100000: episode: 7793, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.955023, mean_q: 4.401640\n",
            " 78955/100000: episode: 7794, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.805933, mean_q: 4.226689\n",
            " 78963/100000: episode: 7795, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.982677, mean_q: 4.432275\n",
            " 78975/100000: episode: 7796, duration: 0.137s, episode steps:  12, steps per second:  88, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 4.167 [0.000, 7.000],  loss: 0.000087, mae: 3.006405, mean_q: 4.461563\n",
            " 78985/100000: episode: 7797, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000063, mae: 2.841094, mean_q: 4.270479\n",
            " 78994/100000: episode: 7798, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000060, mae: 2.994459, mean_q: 4.450968\n",
            " 79003/100000: episode: 7799, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000073, mae: 2.851605, mean_q: 4.280249\n",
            " 79012/100000: episode: 7800, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000038, mae: 2.995025, mean_q: 4.445283\n",
            " 79023/100000: episode: 7801, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000119, mae: 2.976337, mean_q: 4.424321\n",
            " 79033/100000: episode: 7802, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000223, mae: 3.063842, mean_q: 4.528880\n",
            " 79042/100000: episode: 7803, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000217, mae: 3.038858, mean_q: 4.502841\n",
            " 79050/100000: episode: 7804, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000223, mae: 3.104284, mean_q: 4.574481\n",
            " 79058/100000: episode: 7805, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000347, mae: 3.155709, mean_q: 4.633650\n",
            " 79067/100000: episode: 7806, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000245, mae: 2.969178, mean_q: 4.406327\n",
            " 79075/100000: episode: 7807, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000185, mae: 2.937384, mean_q: 4.382133\n",
            " 79084/100000: episode: 7808, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000046, mae: 2.957004, mean_q: 4.409405\n",
            " 79093/100000: episode: 7809, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000053, mae: 3.064949, mean_q: 4.524433\n",
            " 79101/100000: episode: 7810, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.029803, mean_q: 4.483510\n",
            " 79110/100000: episode: 7811, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000017, mae: 2.956306, mean_q: 4.397721\n",
            " 79119/100000: episode: 7812, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000027, mae: 2.986609, mean_q: 4.438894\n",
            " 79128/100000: episode: 7813, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000027, mae: 2.969561, mean_q: 4.419935\n",
            " 79138/100000: episode: 7814, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000045, mae: 2.780153, mean_q: 4.199273\n",
            " 79146/100000: episode: 7815, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.903580, mean_q: 4.344022\n",
            " 79154/100000: episode: 7816, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 3.016362, mean_q: 4.474113\n",
            " 79162/100000: episode: 7817, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 2.908457, mean_q: 4.349906\n",
            " 79170/100000: episode: 7818, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000085, mae: 2.892078, mean_q: 4.329201\n",
            " 79178/100000: episode: 7819, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000061, mae: 3.176989, mean_q: 4.651532\n",
            " 79188/100000: episode: 7820, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000087, mae: 2.841758, mean_q: 4.267737\n",
            " 79196/100000: episode: 7821, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 3.041671, mean_q: 4.504550\n",
            " 79204/100000: episode: 7822, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000068, mae: 2.847828, mean_q: 4.282324\n",
            " 79212/100000: episode: 7823, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 2.897174, mean_q: 4.342027\n",
            " 79220/100000: episode: 7824, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.951059, mean_q: 4.407606\n",
            " 79228/100000: episode: 7825, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 2.924851, mean_q: 4.370022\n",
            " 79237/100000: episode: 7826, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000061, mae: 2.935165, mean_q: 4.384063\n",
            " 79245/100000: episode: 7827, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000063, mae: 2.996877, mean_q: 4.452127\n",
            " 79253/100000: episode: 7828, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000060, mae: 2.908697, mean_q: 4.359267\n",
            " 79263/100000: episode: 7829, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.916027, mean_q: 4.360765\n",
            " 79273/100000: episode: 7830, duration: 0.121s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000064, mae: 2.847635, mean_q: 4.275371\n",
            " 79283/100000: episode: 7831, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000046, mae: 2.850511, mean_q: 4.280053\n",
            " 79291/100000: episode: 7832, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.949264, mean_q: 4.394533\n",
            " 79299/100000: episode: 7833, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.676838, mean_q: 4.084408\n",
            " 79308/100000: episode: 7834, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000052, mae: 2.986505, mean_q: 4.433019\n",
            " 79316/100000: episode: 7835, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.840584, mean_q: 4.278610\n",
            " 79325/100000: episode: 7836, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000082, mae: 3.006839, mean_q: 4.461973\n",
            " 79334/100000: episode: 7837, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000091, mae: 2.997740, mean_q: 4.440913\n",
            " 79342/100000: episode: 7838, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 3.025406, mean_q: 4.479520\n",
            " 79351/100000: episode: 7839, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000041, mae: 2.795684, mean_q: 4.227981\n",
            " 79359/100000: episode: 7840, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.815653, mean_q: 4.249371\n",
            " 79368/100000: episode: 7841, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000042, mae: 2.864304, mean_q: 4.298971\n",
            " 79376/100000: episode: 7842, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000116, mae: 2.867610, mean_q: 4.299093\n",
            " 79388/100000: episode: 7843, duration: 0.172s, episode steps:  12, steps per second:  70, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 2.917 [0.000, 7.000],  loss: 0.000077, mae: 2.992172, mean_q: 4.439530\n",
            " 79396/100000: episode: 7844, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 2.980848, mean_q: 4.425426\n",
            " 79404/100000: episode: 7845, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000080, mae: 2.965090, mean_q: 4.404535\n",
            " 79412/100000: episode: 7846, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 2.834728, mean_q: 4.258526\n",
            " 79421/100000: episode: 7847, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000166, mae: 2.901957, mean_q: 4.339480\n",
            " 79431/100000: episode: 7848, duration: 0.127s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000094, mae: 3.080263, mean_q: 4.548671\n",
            " 79441/100000: episode: 7849, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000079, mae: 2.887979, mean_q: 4.317197\n",
            " 79449/100000: episode: 7850, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.908747, mean_q: 4.344642\n",
            " 79458/100000: episode: 7851, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000082, mae: 2.920051, mean_q: 4.361813\n",
            " 79466/100000: episode: 7852, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.900465, mean_q: 4.343987\n",
            " 79476/100000: episode: 7853, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000079, mae: 2.603657, mean_q: 3.989454\n",
            " 79485/100000: episode: 7854, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000048, mae: 2.984911, mean_q: 4.434656\n",
            " 79493/100000: episode: 7855, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.077326, mean_q: 4.543758\n",
            " 79501/100000: episode: 7856, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.967278, mean_q: 4.423191\n",
            " 79510/100000: episode: 7857, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000044, mae: 2.949143, mean_q: 4.398247\n",
            " 79518/100000: episode: 7858, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000062, mae: 3.009362, mean_q: 4.460755\n",
            " 79526/100000: episode: 7859, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.944227, mean_q: 4.390090\n",
            " 79537/100000: episode: 7860, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000035, mae: 2.855987, mean_q: 4.290009\n",
            " 79545/100000: episode: 7861, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000061, mae: 2.957990, mean_q: 4.409816\n",
            " 79555/100000: episode: 7862, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000086, mae: 2.961599, mean_q: 4.407176\n",
            " 79563/100000: episode: 7863, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000101, mae: 3.145889, mean_q: 4.616681\n",
            " 79572/100000: episode: 7864, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000166, mae: 2.715441, mean_q: 4.132121\n",
            " 79580/100000: episode: 7865, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000155, mae: 3.012489, mean_q: 4.467669\n",
            " 79588/100000: episode: 7866, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000187, mae: 2.828190, mean_q: 4.257936\n",
            " 79596/100000: episode: 7867, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000105, mae: 3.022503, mean_q: 4.483886\n",
            " 79606/100000: episode: 7868, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000098, mae: 2.972874, mean_q: 4.419971\n",
            " 79617/100000: episode: 7869, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000055, mae: 2.939078, mean_q: 4.382080\n",
            " 79625/100000: episode: 7870, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 3.049692, mean_q: 4.512202\n",
            " 79633/100000: episode: 7871, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 3.010071, mean_q: 4.467074\n",
            " 79642/100000: episode: 7872, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000055, mae: 2.927722, mean_q: 4.375135\n",
            " 79651/100000: episode: 7873, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000026, mae: 2.860514, mean_q: 4.294098\n",
            " 79661/100000: episode: 7874, duration: 0.101s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000025, mae: 2.991124, mean_q: 4.448639\n",
            " 79670/100000: episode: 7875, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000045, mae: 2.981065, mean_q: 4.436697\n",
            " 79679/100000: episode: 7876, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000050, mae: 3.099754, mean_q: 4.573632\n",
            " 79688/100000: episode: 7877, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000077, mae: 3.174877, mean_q: 4.664694\n",
            " 79697/100000: episode: 7878, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000053, mae: 3.053009, mean_q: 4.516348\n",
            " 79705/100000: episode: 7879, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.026274, mean_q: 4.482760\n",
            " 79714/100000: episode: 7880, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000019, mae: 2.987485, mean_q: 4.435984\n",
            " 79724/100000: episode: 7881, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000011, mae: 2.785913, mean_q: 4.207664\n",
            " 79733/100000: episode: 7882, duration: 0.182s, episode steps:   9, steps per second:  50, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000048, mae: 2.915900, mean_q: 4.359692\n",
            " 79742/100000: episode: 7883, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000044, mae: 2.949073, mean_q: 4.402524\n",
            " 79750/100000: episode: 7884, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.892026, mean_q: 4.331995\n",
            " 79758/100000: episode: 7885, duration: 0.120s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000052, mae: 3.011897, mean_q: 4.463998\n",
            " 79766/100000: episode: 7886, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.883077, mean_q: 4.325263\n",
            " 79774/100000: episode: 7887, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.966671, mean_q: 4.411229\n",
            " 79783/100000: episode: 7888, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000017, mae: 2.825893, mean_q: 4.251596\n",
            " 79791/100000: episode: 7889, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.844888, mean_q: 4.273489\n",
            " 79800/100000: episode: 7890, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000008, mae: 2.950059, mean_q: 4.397593\n",
            " 79809/100000: episode: 7891, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.930717, mean_q: 4.379675\n",
            " 79819/100000: episode: 7892, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.016392, mean_q: 4.476546\n",
            " 79827/100000: episode: 7893, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.724048, mean_q: 4.137717\n",
            " 79835/100000: episode: 7894, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.849915, mean_q: 4.286274\n",
            " 79843/100000: episode: 7895, duration: 0.139s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.961106, mean_q: 4.413812\n",
            " 79851/100000: episode: 7896, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.167878, mean_q: 4.652853\n",
            " 79859/100000: episode: 7897, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.942399, mean_q: 4.394062\n",
            " 79868/100000: episode: 7898, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 3.078671, mean_q: 4.544985\n",
            " 79876/100000: episode: 7899, duration: 0.172s, episode steps:   8, steps per second:  46, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.982777, mean_q: 4.435432\n",
            " 79886/100000: episode: 7900, duration: 0.202s, episode steps:  10, steps per second:  49, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000002, mae: 2.740687, mean_q: 4.157804\n",
            " 79894/100000: episode: 7901, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.042557, mean_q: 4.509062\n",
            " 79902/100000: episode: 7902, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.921147, mean_q: 4.362518\n",
            " 79911/100000: episode: 7903, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000012, mae: 2.945150, mean_q: 4.394248\n",
            " 79919/100000: episode: 7904, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.952258, mean_q: 4.400018\n",
            " 79928/100000: episode: 7905, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.701985, mean_q: 4.112049\n",
            " 79936/100000: episode: 7906, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.692575, mean_q: 4.099960\n",
            " 79944/100000: episode: 7907, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.860615, mean_q: 4.297841\n",
            " 79954/100000: episode: 7908, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000007, mae: 2.862210, mean_q: 4.297990\n",
            " 79962/100000: episode: 7909, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.960165, mean_q: 4.412029\n",
            " 79972/100000: episode: 7910, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000009, mae: 2.828426, mean_q: 4.261252\n",
            " 79981/100000: episode: 7911, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000008, mae: 2.927761, mean_q: 4.372090\n",
            " 79989/100000: episode: 7912, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.039538, mean_q: 4.505405\n",
            " 79998/100000: episode: 7913, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000004, mae: 2.870024, mean_q: 4.306726\n",
            " 80006/100000: episode: 7914, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.931892, mean_q: 4.381684\n",
            " 80014/100000: episode: 7915, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.932319, mean_q: 4.377211\n",
            " 80023/100000: episode: 7916, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.899194, mean_q: 4.337236\n",
            " 80032/100000: episode: 7917, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.954143, mean_q: 4.403141\n",
            " 80043/100000: episode: 7918, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000003, mae: 3.026066, mean_q: 4.484917\n",
            " 80051/100000: episode: 7919, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.959578, mean_q: 4.413708\n",
            " 80059/100000: episode: 7920, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.925835, mean_q: 4.375134\n",
            " 80068/100000: episode: 7921, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 3.098006, mean_q: 4.569065\n",
            " 80076/100000: episode: 7922, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.835434, mean_q: 4.269719\n",
            " 80087/100000: episode: 7923, duration: 0.115s, episode steps:  11, steps per second:  95, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000016, mae: 3.022956, mean_q: 4.481739\n",
            " 80097/100000: episode: 7924, duration: 0.113s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000016, mae: 2.868547, mean_q: 4.306325\n",
            " 80105/100000: episode: 7925, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.981605, mean_q: 4.438437\n",
            " 80115/100000: episode: 7926, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000020, mae: 2.987637, mean_q: 4.444737\n",
            " 80124/100000: episode: 7927, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 2.887435, mean_q: 4.324481\n",
            " 80133/100000: episode: 7928, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000008, mae: 2.974064, mean_q: 4.423323\n",
            " 80141/100000: episode: 7929, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.127046, mean_q: 4.604527\n",
            " 80149/100000: episode: 7930, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.917272, mean_q: 4.363953\n",
            " 80157/100000: episode: 7931, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.658738, mean_q: 4.063200\n",
            " 80166/100000: episode: 7932, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.732106, mean_q: 4.143665\n",
            " 80175/100000: episode: 7933, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000020, mae: 3.026551, mean_q: 4.484312\n",
            " 80184/100000: episode: 7934, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000033, mae: 2.839495, mean_q: 4.267324\n",
            " 80193/100000: episode: 7935, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000021, mae: 3.013826, mean_q: 4.473233\n",
            " 80205/100000: episode: 7936, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000020, mae: 3.073038, mean_q: 4.537969\n",
            " 80214/100000: episode: 7937, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000020, mae: 2.761685, mean_q: 4.175479\n",
            " 80225/100000: episode: 7938, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000051, mae: 2.976045, mean_q: 4.420413\n",
            " 80234/100000: episode: 7939, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000152, mae: 3.000818, mean_q: 4.451634\n",
            " 80242/100000: episode: 7940, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000386, mae: 2.811409, mean_q: 4.232705\n",
            " 80250/100000: episode: 7941, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000335, mae: 2.774696, mean_q: 4.190441\n",
            " 80258/100000: episode: 7942, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000184, mae: 2.911593, mean_q: 4.346560\n",
            " 80266/100000: episode: 7943, duration: 0.087s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000133, mae: 2.947955, mean_q: 4.391758\n",
            " 80277/100000: episode: 7944, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000085, mae: 2.960779, mean_q: 4.399295\n",
            " 80285/100000: episode: 7945, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000086, mae: 2.938204, mean_q: 4.378184\n",
            " 80295/100000: episode: 7946, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000086, mae: 2.888222, mean_q: 4.320209\n",
            " 80305/100000: episode: 7947, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000086, mae: 2.937221, mean_q: 4.377923\n",
            " 80314/100000: episode: 7948, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000104, mae: 2.712500, mean_q: 4.119853\n",
            " 80323/100000: episode: 7949, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000093, mae: 2.944922, mean_q: 4.387866\n",
            " 80332/100000: episode: 7950, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000100, mae: 2.848464, mean_q: 4.272472\n",
            " 80340/100000: episode: 7951, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000133, mae: 2.912489, mean_q: 4.346737\n",
            " 80348/100000: episode: 7952, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000123, mae: 2.716142, mean_q: 4.129225\n",
            " 80356/100000: episode: 7953, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 2.787015, mean_q: 4.205667\n",
            " 80364/100000: episode: 7954, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000096, mae: 2.865721, mean_q: 4.296344\n",
            " 80373/100000: episode: 7955, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000043, mae: 2.947011, mean_q: 4.389997\n",
            " 80381/100000: episode: 7956, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.050074, mean_q: 4.513871\n",
            " 80391/100000: episode: 7957, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000298, mae: 2.987584, mean_q: 4.433377\n",
            " 80400/100000: episode: 7958, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000250, mae: 2.836468, mean_q: 4.258646\n",
            " 80408/100000: episode: 7959, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000707, mae: 3.037028, mean_q: 4.498890\n",
            " 80416/100000: episode: 7960, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000688, mae: 2.779689, mean_q: 4.197959\n",
            " 80424/100000: episode: 7961, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000639, mae: 2.918466, mean_q: 4.361036\n",
            " 80432/100000: episode: 7962, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000648, mae: 2.936860, mean_q: 4.366353\n",
            " 80440/100000: episode: 7963, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000526, mae: 2.773961, mean_q: 4.166978\n",
            " 80448/100000: episode: 7964, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000499, mae: 2.913432, mean_q: 4.332321\n",
            " 80457/100000: episode: 7965, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000520, mae: 2.903749, mean_q: 4.333517\n",
            " 80466/100000: episode: 7966, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000465, mae: 2.912585, mean_q: 4.344046\n",
            " 80475/100000: episode: 7967, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000146, mae: 3.044052, mean_q: 4.501507\n",
            " 80484/100000: episode: 7968, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000210, mae: 2.971449, mean_q: 4.411672\n",
            " 80494/100000: episode: 7969, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000195, mae: 3.057173, mean_q: 4.507048\n",
            " 80504/100000: episode: 7970, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000137, mae: 2.883791, mean_q: 4.310396\n",
            " 80514/100000: episode: 7971, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000117, mae: 2.897396, mean_q: 4.346877\n",
            " 80524/100000: episode: 7972, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000125, mae: 2.840569, mean_q: 4.278895\n",
            " 80532/100000: episode: 7973, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.645254, mean_q: 4.048262\n",
            " 80542/100000: episode: 7974, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000047, mae: 2.846278, mean_q: 4.283963\n",
            " 80550/100000: episode: 7975, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 3.141962, mean_q: 4.612773\n",
            " 80558/100000: episode: 7976, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.926355, mean_q: 4.371315\n",
            " 80566/100000: episode: 7977, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.848361, mean_q: 4.278046\n",
            " 80574/100000: episode: 7978, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.011782, mean_q: 4.478967\n",
            " 80582/100000: episode: 7979, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.080368, mean_q: 4.553556\n",
            " 80590/100000: episode: 7980, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.820528, mean_q: 4.255174\n",
            " 80598/100000: episode: 7981, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.035671, mean_q: 4.493595\n",
            " 80608/100000: episode: 7982, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.093876, mean_q: 4.566068\n",
            " 80616/100000: episode: 7983, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.783043, mean_q: 4.206172\n",
            " 80624/100000: episode: 7984, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.962271, mean_q: 4.414843\n",
            " 80635/100000: episode: 7985, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000005, mae: 2.930491, mean_q: 4.378015\n",
            " 80644/100000: episode: 7986, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 3.080411, mean_q: 4.549309\n",
            " 80653/100000: episode: 7987, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 2.761030, mean_q: 4.179537\n",
            " 80661/100000: episode: 7988, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.969982, mean_q: 4.419544\n",
            " 80669/100000: episode: 7989, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.980448, mean_q: 4.432508\n",
            " 80677/100000: episode: 7990, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.088113, mean_q: 4.557477\n",
            " 80685/100000: episode: 7991, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.044849, mean_q: 4.512382\n",
            " 80696/100000: episode: 7992, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000006, mae: 2.874006, mean_q: 4.309323\n",
            " 80705/100000: episode: 7993, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.986689, mean_q: 4.444334\n",
            " 80714/100000: episode: 7994, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 3.149811, mean_q: 4.631624\n",
            " 80722/100000: episode: 7995, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.111822, mean_q: 4.586935\n",
            " 80731/100000: episode: 7996, duration: 0.144s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.897720, mean_q: 4.339559\n",
            " 80739/100000: episode: 7997, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.895958, mean_q: 4.340529\n",
            " 80749/100000: episode: 7998, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000002, mae: 3.109791, mean_q: 4.585353\n",
            " 80758/100000: episode: 7999, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.906940, mean_q: 4.350248\n",
            " 80767/100000: episode: 8000, duration: 0.201s, episode steps:   9, steps per second:  45, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 3.140062, mean_q: 4.618371\n",
            " 80778/100000: episode: 8001, duration: 0.214s, episode steps:  11, steps per second:  52, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000001, mae: 2.871293, mean_q: 4.308915\n",
            " 80788/100000: episode: 8002, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 2.894219, mean_q: 4.336974\n",
            " 80796/100000: episode: 8003, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.927617, mean_q: 4.373198\n",
            " 80804/100000: episode: 8004, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.786377, mean_q: 4.209031\n",
            " 80812/100000: episode: 8005, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.053965, mean_q: 4.524766\n",
            " 80820/100000: episode: 8006, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.786152, mean_q: 4.207725\n",
            " 80829/100000: episode: 8007, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 3.061053, mean_q: 4.530980\n",
            " 80837/100000: episode: 8008, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.964703, mean_q: 4.417785\n",
            " 80847/100000: episode: 8009, duration: 0.138s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 2.775241, mean_q: 4.196428\n",
            " 80855/100000: episode: 8010, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.960892, mean_q: 4.408237\n",
            " 80863/100000: episode: 8011, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.992489, mean_q: 4.449625\n",
            " 80872/100000: episode: 8012, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 2.897130, mean_q: 4.338380\n",
            " 80880/100000: episode: 8013, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.886004, mean_q: 4.323036\n",
            " 80889/100000: episode: 8014, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.080351, mean_q: 4.552871\n",
            " 80900/100000: episode: 8015, duration: 0.202s, episode steps:  11, steps per second:  55, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000001, mae: 3.192890, mean_q: 4.681089\n",
            " 80910/100000: episode: 8016, duration: 0.174s, episode steps:  10, steps per second:  58, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 3.236874, mean_q: 4.731761\n",
            " 80918/100000: episode: 8017, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.971465, mean_q: 4.426555\n",
            " 80927/100000: episode: 8018, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000000, mae: 3.210286, mean_q: 4.703660\n",
            " 80935/100000: episode: 8019, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.935635, mean_q: 4.383419\n",
            " 80944/100000: episode: 8020, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000000, mae: 3.030579, mean_q: 4.494531\n",
            " 80952/100000: episode: 8021, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.069708, mean_q: 4.543498\n",
            " 80960/100000: episode: 8022, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.731371, mean_q: 4.142973\n",
            " 80968/100000: episode: 8023, duration: 0.158s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.109090, mean_q: 4.582619\n",
            " 80976/100000: episode: 8024, duration: 0.139s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.095498, mean_q: 4.569063\n",
            " 80985/100000: episode: 8025, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.857451, mean_q: 4.292482\n",
            " 80993/100000: episode: 8026, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.897770, mean_q: 4.340766\n",
            " 81004/100000: episode: 8027, duration: 0.182s, episode steps:  11, steps per second:  61, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000004, mae: 3.035267, mean_q: 4.497709\n",
            " 81012/100000: episode: 8028, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.180735, mean_q: 4.667385\n",
            " 81020/100000: episode: 8029, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.871861, mean_q: 4.305628\n",
            " 81028/100000: episode: 8030, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.790494, mean_q: 4.214442\n",
            " 81037/100000: episode: 8031, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 2.934899, mean_q: 4.380161\n",
            " 81045/100000: episode: 8032, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.008588, mean_q: 4.467121\n",
            " 81053/100000: episode: 8033, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.027299, mean_q: 4.485976\n",
            " 81062/100000: episode: 8034, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.915001, mean_q: 4.361677\n",
            " 81070/100000: episode: 8035, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.075913, mean_q: 4.544625\n",
            " 81078/100000: episode: 8036, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.945614, mean_q: 4.393682\n",
            " 81086/100000: episode: 8037, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.013530, mean_q: 4.476841\n",
            " 81094/100000: episode: 8038, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.034141, mean_q: 4.496342\n",
            " 81103/100000: episode: 8039, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 3.053126, mean_q: 4.518280\n",
            " 81111/100000: episode: 8040, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.928587, mean_q: 4.370912\n",
            " 81121/100000: episode: 8041, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.910655, mean_q: 4.355928\n",
            " 81130/100000: episode: 8042, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.902564, mean_q: 4.341657\n",
            " 81139/100000: episode: 8043, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.946687, mean_q: 4.395292\n",
            " 81147/100000: episode: 8044, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.783156, mean_q: 4.210810\n",
            " 81155/100000: episode: 8045, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.876301, mean_q: 4.316578\n",
            " 81163/100000: episode: 8046, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.070468, mean_q: 4.541893\n",
            " 81171/100000: episode: 8047, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.066177, mean_q: 4.534826\n",
            " 81180/100000: episode: 8048, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.875278, mean_q: 4.315733\n",
            " 81188/100000: episode: 8049, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.897985, mean_q: 4.338238\n",
            " 81197/100000: episode: 8050, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000000, mae: 2.886030, mean_q: 4.328852\n",
            " 81207/100000: episode: 8051, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000001, mae: 2.891284, mean_q: 4.332558\n",
            " 81215/100000: episode: 8052, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.970184, mean_q: 4.421948\n",
            " 81224/100000: episode: 8053, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.061053, mean_q: 4.525243\n",
            " 81233/100000: episode: 8054, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.921384, mean_q: 4.368702\n",
            " 81241/100000: episode: 8055, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.966928, mean_q: 4.420974\n",
            " 81250/100000: episode: 8056, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.024529, mean_q: 4.487496\n",
            " 81260/100000: episode: 8057, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 3.189113, mean_q: 4.678086\n",
            " 81269/100000: episode: 8058, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 3.121415, mean_q: 4.595350\n",
            " 81278/100000: episode: 8059, duration: 0.099s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 2.964685, mean_q: 4.417146\n",
            " 81286/100000: episode: 8060, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.061828, mean_q: 4.526479\n",
            " 81295/100000: episode: 8061, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 2.930732, mean_q: 4.383807\n",
            " 81303/100000: episode: 8062, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.925710, mean_q: 4.371471\n",
            " 81312/100000: episode: 8063, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.224361, mean_q: 4.715595\n",
            " 81322/100000: episode: 8064, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000002, mae: 2.974272, mean_q: 4.429622\n",
            " 81331/100000: episode: 8065, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.951885, mean_q: 4.402812\n",
            " 81340/100000: episode: 8066, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.816267, mean_q: 4.244503\n",
            " 81351/100000: episode: 8067, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000010, mae: 2.970843, mean_q: 4.425850\n",
            " 81361/100000: episode: 8068, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000019, mae: 2.994838, mean_q: 4.445159\n",
            " 81371/100000: episode: 8069, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000031, mae: 2.778585, mean_q: 4.203099\n",
            " 81380/100000: episode: 8070, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000026, mae: 2.836058, mean_q: 4.268138\n",
            " 81388/100000: episode: 8071, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.062904, mean_q: 4.535003\n",
            " 81397/100000: episode: 8072, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000025, mae: 3.037188, mean_q: 4.500326\n",
            " 81407/100000: episode: 8073, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000058, mae: 2.840736, mean_q: 4.266393\n",
            " 81416/100000: episode: 8074, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000021, mae: 2.892415, mean_q: 4.329618\n",
            " 81424/100000: episode: 8075, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.841129, mean_q: 4.269989\n",
            " 81432/100000: episode: 8076, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.993712, mean_q: 4.449666\n",
            " 81441/100000: episode: 8077, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 2.973973, mean_q: 4.428401\n",
            " 81450/100000: episode: 8078, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000020, mae: 2.916784, mean_q: 4.365201\n",
            " 81458/100000: episode: 8079, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.082748, mean_q: 4.553043\n",
            " 81466/100000: episode: 8080, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.747592, mean_q: 4.163174\n",
            " 81476/100000: episode: 8081, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000019, mae: 2.901685, mean_q: 4.338881\n",
            " 81484/100000: episode: 8082, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.901376, mean_q: 4.346556\n",
            " 81492/100000: episode: 8083, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.764435, mean_q: 4.189000\n",
            " 81500/100000: episode: 8084, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.132737, mean_q: 4.611038\n",
            " 81508/100000: episode: 8085, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.827608, mean_q: 4.262561\n",
            " 81518/100000: episode: 8086, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000020, mae: 3.071110, mean_q: 4.534366\n",
            " 81526/100000: episode: 8087, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 3.011801, mean_q: 4.467101\n",
            " 81534/100000: episode: 8088, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.902024, mean_q: 4.342942\n",
            " 81543/100000: episode: 8089, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000075, mae: 2.915875, mean_q: 4.360433\n",
            " 81552/100000: episode: 8090, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000063, mae: 3.061538, mean_q: 4.523445\n",
            " 81561/100000: episode: 8091, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000072, mae: 2.865129, mean_q: 4.291889\n",
            " 81569/100000: episode: 8092, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 3.075812, mean_q: 4.547235\n",
            " 81577/100000: episode: 8093, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000118, mae: 2.928836, mean_q: 4.371461\n",
            " 81586/100000: episode: 8094, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000310, mae: 2.970093, mean_q: 4.413195\n",
            " 81595/100000: episode: 8095, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000375, mae: 2.761498, mean_q: 4.167531\n",
            " 81604/100000: episode: 8096, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000312, mae: 3.013109, mean_q: 4.472278\n",
            " 81612/100000: episode: 8097, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000506, mae: 2.585705, mean_q: 3.962422\n",
            " 81621/100000: episode: 8098, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000351, mae: 2.969311, mean_q: 4.414061\n",
            " 81629/100000: episode: 8099, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000334, mae: 3.008041, mean_q: 4.458102\n",
            " 81642/100000: episode: 8100, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 3.231 [0.000, 7.000],  loss: 0.000285, mae: 2.969158, mean_q: 4.413085\n",
            " 81651/100000: episode: 8101, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000255, mae: 3.014429, mean_q: 4.474541\n",
            " 81661/100000: episode: 8102, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000206, mae: 2.925045, mean_q: 4.343939\n",
            " 81669/100000: episode: 8103, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000411, mae: 3.022618, mean_q: 4.474813\n",
            " 81677/100000: episode: 8104, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000173, mae: 2.872617, mean_q: 4.307686\n",
            " 81687/100000: episode: 8105, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000427, mae: 2.937598, mean_q: 4.368487\n",
            " 81696/100000: episode: 8106, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000423, mae: 3.095917, mean_q: 4.547083\n",
            " 81706/100000: episode: 8107, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000221, mae: 2.862024, mean_q: 4.286059\n",
            " 81714/100000: episode: 8108, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000320, mae: 3.128568, mean_q: 4.583599\n",
            " 81723/100000: episode: 8109, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000176, mae: 3.009749, mean_q: 4.452021\n",
            " 81731/100000: episode: 8110, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000136, mae: 3.088551, mean_q: 4.550668\n",
            " 81739/100000: episode: 8111, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000306, mae: 2.950770, mean_q: 4.386826\n",
            " 81748/100000: episode: 8112, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000270, mae: 2.905814, mean_q: 4.339796\n",
            " 81757/100000: episode: 8113, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000091, mae: 3.185891, mean_q: 4.660780\n",
            " 81767/100000: episode: 8114, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000108, mae: 2.907794, mean_q: 4.339216\n",
            " 81777/100000: episode: 8115, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000063, mae: 2.903392, mean_q: 4.339564\n",
            " 81785/100000: episode: 8116, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.957487, mean_q: 4.408066\n",
            " 81793/100000: episode: 8117, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000108, mae: 2.957030, mean_q: 4.400184\n",
            " 81801/100000: episode: 8118, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000130, mae: 3.229469, mean_q: 4.705079\n",
            " 81809/100000: episode: 8119, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000274, mae: 2.802908, mean_q: 4.224711\n",
            " 81819/100000: episode: 8120, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000148, mae: 2.952773, mean_q: 4.397817\n",
            " 81828/100000: episode: 8121, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000126, mae: 3.054671, mean_q: 4.502132\n",
            " 81837/100000: episode: 8122, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000151, mae: 2.888793, mean_q: 4.317983\n",
            " 81845/100000: episode: 8123, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000340, mae: 2.974648, mean_q: 4.424891\n",
            " 81853/100000: episode: 8124, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000293, mae: 3.107732, mean_q: 4.582936\n",
            " 81861/100000: episode: 8125, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000154, mae: 2.898317, mean_q: 4.326093\n",
            " 81869/100000: episode: 8126, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 3.011242, mean_q: 4.458060\n",
            " 81877/100000: episode: 8127, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 3.049580, mean_q: 4.500707\n",
            " 81887/100000: episode: 8128, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000019, mae: 3.051217, mean_q: 4.514800\n",
            " 81895/100000: episode: 8129, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.896231, mean_q: 4.333626\n",
            " 81904/100000: episode: 8130, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000017, mae: 3.031731, mean_q: 4.496874\n",
            " 81912/100000: episode: 8131, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.019713, mean_q: 4.483433\n",
            " 81921/100000: episode: 8132, duration: 0.191s, episode steps:   9, steps per second:  47, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000020, mae: 3.178180, mean_q: 4.662715\n",
            " 81929/100000: episode: 8133, duration: 0.184s, episode steps:   8, steps per second:  44, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.919276, mean_q: 4.367029\n",
            " 81938/100000: episode: 8134, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000019, mae: 2.890000, mean_q: 4.332214\n",
            " 81951/100000: episode: 8135, duration: 0.179s, episode steps:  13, steps per second:  73, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 3.923 [0.000, 7.000],  loss: 0.000012, mae: 2.973376, mean_q: 4.424186\n",
            " 81961/100000: episode: 8136, duration: 0.148s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000004, mae: 2.833131, mean_q: 4.265724\n",
            " 81969/100000: episode: 8137, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.792577, mean_q: 4.212052\n",
            " 81977/100000: episode: 8138, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.876499, mean_q: 4.313769\n",
            " 81986/100000: episode: 8139, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000006, mae: 3.028993, mean_q: 4.491426\n",
            " 81994/100000: episode: 8140, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.094581, mean_q: 4.567491\n",
            " 82002/100000: episode: 8141, duration: 0.158s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.654970, mean_q: 4.058187\n",
            " 82011/100000: episode: 8142, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000008, mae: 3.086844, mean_q: 4.554595\n",
            " 82019/100000: episode: 8143, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.993338, mean_q: 4.452490\n",
            " 82028/100000: episode: 8144, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.987669, mean_q: 4.445064\n",
            " 82037/100000: episode: 8145, duration: 0.199s, episode steps:   9, steps per second:  45, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 2.961682, mean_q: 4.411005\n",
            " 82047/100000: episode: 8146, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000004, mae: 3.236035, mean_q: 4.729358\n",
            " 82057/100000: episode: 8147, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000004, mae: 2.924611, mean_q: 4.372564\n",
            " 82065/100000: episode: 8148, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.002170, mean_q: 4.458605\n",
            " 82075/100000: episode: 8149, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000003, mae: 2.923192, mean_q: 4.370501\n",
            " 82083/100000: episode: 8150, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.081608, mean_q: 4.551397\n",
            " 82091/100000: episode: 8151, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.805466, mean_q: 4.234908\n",
            " 82101/100000: episode: 8152, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000002, mae: 2.840389, mean_q: 4.274502\n",
            " 82109/100000: episode: 8153, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.909912, mean_q: 4.355360\n",
            " 82118/100000: episode: 8154, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.841187, mean_q: 4.273118\n",
            " 82126/100000: episode: 8155, duration: 0.165s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.958497, mean_q: 4.408303\n",
            " 82134/100000: episode: 8156, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.916648, mean_q: 4.361254\n",
            " 82143/100000: episode: 8157, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 3.000026, mean_q: 4.455009\n",
            " 82152/100000: episode: 8158, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.844171, mean_q: 4.274143\n",
            " 82160/100000: episode: 8159, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.068342, mean_q: 4.531923\n",
            " 82169/100000: episode: 8160, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.888416, mean_q: 4.326602\n",
            " 82179/100000: episode: 8161, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 3.018065, mean_q: 4.479779\n",
            " 82188/100000: episode: 8162, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.183767, mean_q: 4.670408\n",
            " 82196/100000: episode: 8163, duration: 0.094s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.935496, mean_q: 4.379735\n",
            " 82204/100000: episode: 8164, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.010560, mean_q: 4.471557\n",
            " 82212/100000: episode: 8165, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.115460, mean_q: 4.590032\n",
            " 82221/100000: episode: 8166, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.072179, mean_q: 4.541324\n",
            " 82231/100000: episode: 8167, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000001, mae: 2.970150, mean_q: 4.418675\n",
            " 82240/100000: episode: 8168, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.952594, mean_q: 4.398185\n",
            " 82249/100000: episode: 8169, duration: 0.168s, episode steps:   9, steps per second:  53, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.939629, mean_q: 4.390192\n",
            " 82257/100000: episode: 8170, duration: 0.157s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.872059, mean_q: 4.311645\n",
            " 82265/100000: episode: 8171, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.943141, mean_q: 4.391497\n",
            " 82275/100000: episode: 8172, duration: 0.109s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000000, mae: 2.896550, mean_q: 4.340937\n",
            " 82283/100000: episode: 8173, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.743334, mean_q: 4.165593\n",
            " 82293/100000: episode: 8174, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000001, mae: 3.030080, mean_q: 4.494216\n",
            " 82302/100000: episode: 8175, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.118299, mean_q: 4.596332\n",
            " 82310/100000: episode: 8176, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.928775, mean_q: 4.375755\n",
            " 82318/100000: episode: 8177, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.942004, mean_q: 4.395136\n",
            " 82327/100000: episode: 8178, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.818707, mean_q: 4.245175\n",
            " 82336/100000: episode: 8179, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.985346, mean_q: 4.444042\n",
            " 82347/100000: episode: 8180, duration: 0.124s, episode steps:  11, steps per second:  89, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000001, mae: 3.043000, mean_q: 4.508834\n",
            " 82356/100000: episode: 8181, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 3.091193, mean_q: 4.561579\n",
            " 82364/100000: episode: 8182, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.993570, mean_q: 4.451447\n",
            " 82373/100000: episode: 8183, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 2.885734, mean_q: 4.328713\n",
            " 82382/100000: episode: 8184, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.843803, mean_q: 4.278930\n",
            " 82391/100000: episode: 8185, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 3.074271, mean_q: 4.545813\n",
            " 82400/100000: episode: 8186, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.862123, mean_q: 4.299402\n",
            " 82408/100000: episode: 8187, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.893411, mean_q: 4.337543\n",
            " 82416/100000: episode: 8188, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.798608, mean_q: 4.220425\n",
            " 82425/100000: episode: 8189, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.965218, mean_q: 4.414875\n",
            " 82434/100000: episode: 8190, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.974899, mean_q: 4.432215\n",
            " 82442/100000: episode: 8191, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.885847, mean_q: 4.325462\n",
            " 82452/100000: episode: 8192, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 2.889670, mean_q: 4.329984\n",
            " 82461/100000: episode: 8193, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.921257, mean_q: 4.370344\n",
            " 82470/100000: episode: 8194, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 3.028704, mean_q: 4.497356\n",
            " 82479/100000: episode: 8195, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 2.943308, mean_q: 4.393719\n",
            " 82487/100000: episode: 8196, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.927205, mean_q: 4.370546\n",
            " 82497/100000: episode: 8197, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 3.074934, mean_q: 4.546906\n",
            " 82505/100000: episode: 8198, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.923262, mean_q: 4.368212\n",
            " 82514/100000: episode: 8199, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000000, mae: 3.025187, mean_q: 4.486116\n",
            " 82522/100000: episode: 8200, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.784564, mean_q: 4.209232\n",
            " 82530/100000: episode: 8201, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.987679, mean_q: 4.444215\n",
            " 82539/100000: episode: 8202, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.098029, mean_q: 4.568995\n",
            " 82548/100000: episode: 8203, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.993103, mean_q: 4.452966\n",
            " 82557/100000: episode: 8204, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 3.069310, mean_q: 4.541741\n",
            " 82566/100000: episode: 8205, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.904531, mean_q: 4.347738\n",
            " 82574/100000: episode: 8206, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.343263, mean_q: 4.855030\n",
            " 82584/100000: episode: 8207, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000001, mae: 2.882266, mean_q: 4.325240\n",
            " 82592/100000: episode: 8208, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.936485, mean_q: 4.383972\n",
            " 82600/100000: episode: 8209, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.145495, mean_q: 4.622334\n",
            " 82608/100000: episode: 8210, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.965939, mean_q: 4.421413\n",
            " 82616/100000: episode: 8211, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.055170, mean_q: 4.521667\n",
            " 82625/100000: episode: 8212, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.891937, mean_q: 4.331851\n",
            " 82633/100000: episode: 8213, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.871966, mean_q: 4.305712\n",
            " 82641/100000: episode: 8214, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.539669, mean_q: 3.929596\n",
            " 82649/100000: episode: 8215, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.698374, mean_q: 4.108044\n",
            " 82658/100000: episode: 8216, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000004, mae: 3.183258, mean_q: 4.671208\n",
            " 82668/100000: episode: 8217, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000003, mae: 2.796811, mean_q: 4.224977\n",
            " 82676/100000: episode: 8218, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.137995, mean_q: 4.611205\n",
            " 82684/100000: episode: 8219, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.978502, mean_q: 4.437528\n",
            " 82692/100000: episode: 8220, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.131384, mean_q: 4.609848\n",
            " 82701/100000: episode: 8221, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 3.081623, mean_q: 4.552132\n",
            " 82709/100000: episode: 8222, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.967028, mean_q: 4.417962\n",
            " 82718/100000: episode: 8223, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000049, mae: 2.949863, mean_q: 4.400053\n",
            " 82726/100000: episode: 8224, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.014774, mean_q: 4.475921\n",
            " 82734/100000: episode: 8225, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 2.850809, mean_q: 4.282472\n",
            " 82742/100000: episode: 8226, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.919584, mean_q: 4.362573\n",
            " 82751/100000: episode: 8227, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000034, mae: 2.973475, mean_q: 4.426944\n",
            " 82759/100000: episode: 8228, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 2.923199, mean_q: 4.366416\n",
            " 82769/100000: episode: 8229, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000186, mae: 2.977947, mean_q: 4.428491\n",
            " 82777/100000: episode: 8230, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000268, mae: 2.961191, mean_q: 4.407290\n",
            " 82785/100000: episode: 8231, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.001007, mae: 2.857920, mean_q: 4.283443\n",
            " 82794/100000: episode: 8232, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000722, mae: 2.974006, mean_q: 4.420876\n",
            " 82802/100000: episode: 8233, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000712, mae: 3.007825, mean_q: 4.442019\n",
            " 82811/100000: episode: 8234, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000954, mae: 2.821134, mean_q: 4.220133\n",
            " 82821/100000: episode: 8235, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000402, mae: 3.106324, mean_q: 4.561006\n",
            " 82829/100000: episode: 8236, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000342, mae: 3.082083, mean_q: 4.544351\n",
            " 82839/100000: episode: 8237, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000325, mae: 2.757324, mean_q: 4.168645\n",
            " 82849/100000: episode: 8238, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000149, mae: 2.993444, mean_q: 4.438419\n",
            " 82857/100000: episode: 8239, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000116, mae: 2.935850, mean_q: 4.376049\n",
            " 82865/100000: episode: 8240, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000089, mae: 3.000863, mean_q: 4.459658\n",
            " 82875/100000: episode: 8241, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000052, mae: 3.094733, mean_q: 4.562539\n",
            " 82883/100000: episode: 8242, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.996787, mean_q: 4.444273\n",
            " 82891/100000: episode: 8243, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.015282, mean_q: 4.473363\n",
            " 82901/100000: episode: 8244, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.839493, mean_q: 4.271066\n",
            " 82909/100000: episode: 8245, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.789306, mean_q: 4.208590\n",
            " 82917/100000: episode: 8246, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.914950, mean_q: 4.357172\n",
            " 82925/100000: episode: 8247, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.741934, mean_q: 4.156025\n",
            " 82933/100000: episode: 8248, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.988822, mean_q: 4.441031\n",
            " 82943/100000: episode: 8249, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000007, mae: 3.065775, mean_q: 4.536136\n",
            " 82952/100000: episode: 8250, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000011, mae: 2.983353, mean_q: 4.436907\n",
            " 82960/100000: episode: 8251, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.991218, mean_q: 4.445988\n",
            " 82969/100000: episode: 8252, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000018, mae: 3.049958, mean_q: 4.517335\n",
            " 82977/100000: episode: 8253, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.109365, mean_q: 4.573704\n",
            " 82987/100000: episode: 8254, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000013, mae: 2.845229, mean_q: 4.277913\n",
            " 82995/100000: episode: 8255, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.751659, mean_q: 4.170597\n",
            " 83003/100000: episode: 8256, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.971297, mean_q: 4.417517\n",
            " 83013/100000: episode: 8257, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000012, mae: 2.941266, mean_q: 4.386146\n",
            " 83022/100000: episode: 8258, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000016, mae: 3.038639, mean_q: 4.499028\n",
            " 83031/100000: episode: 8259, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000012, mae: 2.971064, mean_q: 4.415904\n",
            " 83040/100000: episode: 8260, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000013, mae: 2.850368, mean_q: 4.282834\n",
            " 83048/100000: episode: 8261, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.795458, mean_q: 4.223587\n",
            " 83058/100000: episode: 8262, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000010, mae: 2.824952, mean_q: 4.256984\n",
            " 83067/100000: episode: 8263, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 2.790273, mean_q: 4.213089\n",
            " 83075/100000: episode: 8264, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.039087, mean_q: 4.503580\n",
            " 83086/100000: episode: 8265, duration: 0.171s, episode steps:  11, steps per second:  64, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000011, mae: 2.866671, mean_q: 4.303068\n",
            " 83095/100000: episode: 8266, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000007, mae: 2.917532, mean_q: 4.360807\n",
            " 83103/100000: episode: 8267, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.790416, mean_q: 4.213205\n",
            " 83113/100000: episode: 8268, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000005, mae: 2.877648, mean_q: 4.317134\n",
            " 83123/100000: episode: 8269, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000005, mae: 2.941714, mean_q: 4.392252\n",
            " 83132/100000: episode: 8270, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 2.991664, mean_q: 4.444532\n",
            " 83142/100000: episode: 8271, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000005, mae: 3.119934, mean_q: 4.593912\n",
            " 83150/100000: episode: 8272, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.994478, mean_q: 4.452679\n",
            " 83158/100000: episode: 8273, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.640165, mean_q: 4.036851\n",
            " 83166/100000: episode: 8274, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.791183, mean_q: 4.216504\n",
            " 83176/100000: episode: 8275, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000002, mae: 2.937771, mean_q: 4.384241\n",
            " 83185/100000: episode: 8276, duration: 0.171s, episode steps:   9, steps per second:  53, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.946625, mean_q: 4.398264\n",
            " 83194/100000: episode: 8277, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.073529, mean_q: 4.542003\n",
            " 83204/100000: episode: 8278, duration: 0.191s, episode steps:  10, steps per second:  52, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000002, mae: 3.059585, mean_q: 4.523788\n",
            " 83212/100000: episode: 8279, duration: 0.195s, episode steps:   8, steps per second:  41, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.148589, mean_q: 4.628393\n",
            " 83221/100000: episode: 8280, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 2.931996, mean_q: 4.378072\n",
            " 83230/100000: episode: 8281, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000005, mae: 2.982410, mean_q: 4.440253\n",
            " 83239/100000: episode: 8282, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.885900, mean_q: 4.323900\n",
            " 83247/100000: episode: 8283, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.864359, mean_q: 4.299041\n",
            " 83255/100000: episode: 8284, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.811713, mean_q: 4.234858\n",
            " 83265/100000: episode: 8285, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.116312, mean_q: 4.590838\n",
            " 83274/100000: episode: 8286, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.952887, mean_q: 4.404389\n",
            " 83283/100000: episode: 8287, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.794480, mean_q: 4.223349\n",
            " 83292/100000: episode: 8288, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.917811, mean_q: 4.361847\n",
            " 83300/100000: episode: 8289, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.044122, mean_q: 4.511991\n",
            " 83310/100000: episode: 8290, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000003, mae: 2.929908, mean_q: 4.376779\n",
            " 83319/100000: episode: 8291, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000004, mae: 2.924055, mean_q: 4.366723\n",
            " 83328/100000: episode: 8292, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000005, mae: 2.793378, mean_q: 4.221961\n",
            " 83336/100000: episode: 8293, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.806156, mean_q: 4.232850\n",
            " 83345/100000: episode: 8294, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000011, mae: 2.827602, mean_q: 4.252344\n",
            " 83355/100000: episode: 8295, duration: 0.140s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000005, mae: 3.006464, mean_q: 4.466690\n",
            " 83364/100000: episode: 8296, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.853500, mean_q: 4.286025\n",
            " 83373/100000: episode: 8297, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 3.058272, mean_q: 4.523975\n",
            " 83382/100000: episode: 8298, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.974446, mean_q: 4.429318\n",
            " 83392/100000: episode: 8299, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 2.778279, mean_q: 4.203985\n",
            " 83400/100000: episode: 8300, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.856483, mean_q: 4.296152\n",
            " 83408/100000: episode: 8301, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.916639, mean_q: 4.362211\n",
            " 83417/100000: episode: 8302, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.999383, mean_q: 4.455393\n",
            " 83425/100000: episode: 8303, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.794378, mean_q: 4.215849\n",
            " 83435/100000: episode: 8304, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.969125, mean_q: 4.419514\n",
            " 83445/100000: episode: 8305, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 2.911232, mean_q: 4.355528\n",
            " 83454/100000: episode: 8306, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.969866, mean_q: 4.426890\n",
            " 83463/100000: episode: 8307, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.886294, mean_q: 4.322582\n",
            " 83471/100000: episode: 8308, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.983488, mean_q: 4.438957\n",
            " 83479/100000: episode: 8309, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.784281, mean_q: 4.210272\n",
            " 83487/100000: episode: 8310, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.012283, mean_q: 4.474259\n",
            " 83495/100000: episode: 8311, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.157346, mean_q: 4.641866\n",
            " 83503/100000: episode: 8312, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.873135, mean_q: 4.314180\n",
            " 83512/100000: episode: 8313, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.917850, mean_q: 4.360673\n",
            " 83520/100000: episode: 8314, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.984669, mean_q: 4.440425\n",
            " 83529/100000: episode: 8315, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000003, mae: 2.753749, mean_q: 4.172263\n",
            " 83537/100000: episode: 8316, duration: 0.132s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.143741, mean_q: 4.622572\n",
            " 83546/100000: episode: 8317, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 3.008096, mean_q: 4.467312\n",
            " 83554/100000: episode: 8318, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.881185, mean_q: 4.321370\n",
            " 83565/100000: episode: 8319, duration: 0.103s, episode steps:  11, steps per second: 107, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000004, mae: 2.750788, mean_q: 4.170403\n",
            " 83574/100000: episode: 8320, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.811557, mean_q: 4.240088\n",
            " 83582/100000: episode: 8321, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.937047, mean_q: 4.387301\n",
            " 83591/100000: episode: 8322, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000015, mae: 2.892449, mean_q: 4.332694\n",
            " 83599/100000: episode: 8323, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.149008, mean_q: 4.623885\n",
            " 83609/100000: episode: 8324, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000006, mae: 2.925430, mean_q: 4.370056\n",
            " 83618/100000: episode: 8325, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.964096, mean_q: 4.416689\n",
            " 83626/100000: episode: 8326, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.996787, mean_q: 4.457886\n",
            " 83634/100000: episode: 8327, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.220775, mean_q: 4.711391\n",
            " 83644/100000: episode: 8328, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000008, mae: 3.013542, mean_q: 4.471579\n",
            " 83654/100000: episode: 8329, duration: 0.105s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000010, mae: 2.950672, mean_q: 4.404038\n",
            " 83663/100000: episode: 8330, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000019, mae: 3.059190, mean_q: 4.524805\n",
            " 83671/100000: episode: 8331, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.788274, mean_q: 4.214912\n",
            " 83681/100000: episode: 8332, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000019, mae: 2.901149, mean_q: 4.344678\n",
            " 83689/100000: episode: 8333, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.816035, mean_q: 4.245344\n",
            " 83697/100000: episode: 8334, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.057513, mean_q: 4.527858\n",
            " 83706/100000: episode: 8335, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000018, mae: 2.991163, mean_q: 4.446691\n",
            " 83714/100000: episode: 8336, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.836198, mean_q: 4.272720\n",
            " 83722/100000: episode: 8337, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.961458, mean_q: 4.414968\n",
            " 83730/100000: episode: 8338, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 2.931469, mean_q: 4.375150\n",
            " 83738/100000: episode: 8339, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000128, mae: 3.234711, mean_q: 4.723361\n",
            " 83747/100000: episode: 8340, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000099, mae: 2.968963, mean_q: 4.418270\n",
            " 83755/100000: episode: 8341, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000079, mae: 2.930342, mean_q: 4.377134\n",
            " 83765/100000: episode: 8342, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000061, mae: 2.955282, mean_q: 4.405183\n",
            " 83773/100000: episode: 8343, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 2.906645, mean_q: 4.350667\n",
            " 83781/100000: episode: 8344, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000052, mae: 2.914107, mean_q: 4.357690\n",
            " 83789/100000: episode: 8345, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.979440, mean_q: 4.435637\n",
            " 83797/100000: episode: 8346, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.865480, mean_q: 4.304969\n",
            " 83806/100000: episode: 8347, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000054, mae: 2.730320, mean_q: 4.148105\n",
            " 83816/100000: episode: 8348, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000036, mae: 2.902833, mean_q: 4.343551\n",
            " 83826/100000: episode: 8349, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000035, mae: 2.918827, mean_q: 4.364568\n",
            " 83834/100000: episode: 8350, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.932418, mean_q: 4.382648\n",
            " 83842/100000: episode: 8351, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 2.845984, mean_q: 4.270439\n",
            " 83850/100000: episode: 8352, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.905105, mean_q: 4.342688\n",
            " 83858/100000: episode: 8353, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.012510, mean_q: 4.470132\n",
            " 83866/100000: episode: 8354, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 3.262853, mean_q: 4.758983\n",
            " 83876/100000: episode: 8355, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000035, mae: 3.056857, mean_q: 4.522193\n",
            " 83884/100000: episode: 8356, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 3.074254, mean_q: 4.532202\n",
            " 83892/100000: episode: 8357, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 3.056603, mean_q: 4.523687\n",
            " 83900/100000: episode: 8358, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.881059, mean_q: 4.321613\n",
            " 83908/100000: episode: 8359, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.851738, mean_q: 4.282495\n",
            " 83917/100000: episode: 8360, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000146, mae: 3.125305, mean_q: 4.592497\n",
            " 83925/100000: episode: 8361, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 3.195644, mean_q: 4.681347\n",
            " 83934/100000: episode: 8362, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000201, mae: 2.807100, mean_q: 4.225976\n",
            " 83944/100000: episode: 8363, duration: 0.113s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000262, mae: 2.973577, mean_q: 4.413751\n",
            " 83953/100000: episode: 8364, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000259, mae: 2.911112, mean_q: 4.345293\n",
            " 83961/100000: episode: 8365, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000163, mae: 2.745611, mean_q: 4.160353\n",
            " 83970/100000: episode: 8366, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000170, mae: 2.838551, mean_q: 4.266410\n",
            " 83979/100000: episode: 8367, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000159, mae: 2.947938, mean_q: 4.386910\n",
            " 83988/100000: episode: 8368, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000138, mae: 3.063997, mean_q: 4.528775\n",
            " 83996/100000: episode: 8369, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000209, mae: 3.227175, mean_q: 4.719386\n",
            " 84008/100000: episode: 8370, duration: 0.126s, episode steps:  12, steps per second:  96, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000134, mae: 2.990707, mean_q: 4.450112\n",
            " 84017/100000: episode: 8371, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000121, mae: 2.929510, mean_q: 4.371162\n",
            " 84026/100000: episode: 8372, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000120, mae: 2.652656, mean_q: 4.047923\n",
            " 84034/100000: episode: 8373, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000079, mae: 3.062080, mean_q: 4.523430\n",
            " 84042/100000: episode: 8374, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 2.871684, mean_q: 4.303114\n",
            " 84050/100000: episode: 8375, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 2.739800, mean_q: 4.159953\n",
            " 84058/100000: episode: 8376, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 3.112350, mean_q: 4.590337\n",
            " 84067/100000: episode: 8377, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000042, mae: 2.997458, mean_q: 4.450578\n",
            " 84075/100000: episode: 8378, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.928937, mean_q: 4.364813\n",
            " 84083/100000: episode: 8379, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 2.810338, mean_q: 4.229530\n",
            " 84091/100000: episode: 8380, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 3.015108, mean_q: 4.472012\n",
            " 84101/100000: episode: 8381, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000041, mae: 2.863330, mean_q: 4.294251\n",
            " 84109/100000: episode: 8382, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.985020, mean_q: 4.442675\n",
            " 84117/100000: episode: 8383, duration: 0.147s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 2.896838, mean_q: 4.331461\n",
            " 84126/100000: episode: 8384, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000048, mae: 3.046363, mean_q: 4.505084\n",
            " 84135/100000: episode: 8385, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000032, mae: 2.775270, mean_q: 4.196999\n",
            " 84143/100000: episode: 8386, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.994090, mean_q: 4.451728\n",
            " 84152/100000: episode: 8387, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000011, mae: 3.034131, mean_q: 4.497754\n",
            " 84160/100000: episode: 8388, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.910000, mean_q: 4.350648\n",
            " 84168/100000: episode: 8389, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.845401, mean_q: 4.280457\n",
            " 84178/100000: episode: 8390, duration: 0.146s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 2.872696, mean_q: 4.309557\n",
            " 84187/100000: episode: 8391, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.955446, mean_q: 4.404519\n",
            " 84196/100000: episode: 8392, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 2.751288, mean_q: 4.167898\n",
            " 84204/100000: episode: 8393, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.853105, mean_q: 4.285649\n",
            " 84214/100000: episode: 8394, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000007, mae: 3.034293, mean_q: 4.496509\n",
            " 84223/100000: episode: 8395, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000008, mae: 2.978380, mean_q: 4.433908\n",
            " 84232/100000: episode: 8396, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000022, mae: 3.033939, mean_q: 4.492383\n",
            " 84242/100000: episode: 8397, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000025, mae: 2.985725, mean_q: 4.437861\n",
            " 84252/100000: episode: 8398, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000007, mae: 2.933026, mean_q: 4.373838\n",
            " 84260/100000: episode: 8399, duration: 0.168s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.889660, mean_q: 4.329082\n",
            " 84269/100000: episode: 8400, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 2.901124, mean_q: 4.343957\n",
            " 84277/100000: episode: 8401, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.056785, mean_q: 4.524088\n",
            " 84287/100000: episode: 8402, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000008, mae: 3.026113, mean_q: 4.486504\n",
            " 84295/100000: episode: 8403, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.021859, mean_q: 4.482622\n",
            " 84303/100000: episode: 8404, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.847039, mean_q: 4.281356\n",
            " 84311/100000: episode: 8405, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.847049, mean_q: 4.280085\n",
            " 84319/100000: episode: 8406, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.932450, mean_q: 4.374483\n",
            " 84328/100000: episode: 8407, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000005, mae: 2.908577, mean_q: 4.347997\n",
            " 84337/100000: episode: 8408, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 2.823183, mean_q: 4.254441\n",
            " 84346/100000: episode: 8409, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 2.812994, mean_q: 4.235877\n",
            " 84355/100000: episode: 8410, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 2.863103, mean_q: 4.302594\n",
            " 84364/100000: episode: 8411, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 2.784368, mean_q: 4.204335\n",
            " 84373/100000: episode: 8412, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000013, mae: 2.954990, mean_q: 4.408193\n",
            " 84381/100000: episode: 8413, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.918797, mean_q: 4.365594\n",
            " 84390/100000: episode: 8414, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000014, mae: 3.008482, mean_q: 4.466513\n",
            " 84398/100000: episode: 8415, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.250563, mean_q: 4.748394\n",
            " 84406/100000: episode: 8416, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.827103, mean_q: 4.257516\n",
            " 84414/100000: episode: 8417, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.743318, mean_q: 4.159369\n",
            " 84422/100000: episode: 8418, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.980070, mean_q: 4.434202\n",
            " 84430/100000: episode: 8419, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000052, mae: 2.998041, mean_q: 4.452726\n",
            " 84439/100000: episode: 8420, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000024, mae: 2.941258, mean_q: 4.387712\n",
            " 84447/100000: episode: 8421, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.929545, mean_q: 4.376837\n",
            " 84457/100000: episode: 8422, duration: 0.142s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000010, mae: 2.936547, mean_q: 4.383123\n",
            " 84465/100000: episode: 8423, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.062035, mean_q: 4.526608\n",
            " 84473/100000: episode: 8424, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.855322, mean_q: 4.292091\n",
            " 84484/100000: episode: 8425, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000014, mae: 2.932570, mean_q: 4.380625\n",
            " 84492/100000: episode: 8426, duration: 0.126s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 3.085183, mean_q: 4.557797\n",
            " 84500/100000: episode: 8427, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000085, mae: 2.819755, mean_q: 4.245476\n",
            " 84509/100000: episode: 8428, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000068, mae: 2.822451, mean_q: 4.251634\n",
            " 84517/100000: episode: 8429, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 2.954204, mean_q: 4.402304\n",
            " 84525/100000: episode: 8430, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.897030, mean_q: 4.323763\n",
            " 84534/100000: episode: 8431, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000079, mae: 2.931666, mean_q: 4.382182\n",
            " 84543/100000: episode: 8432, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000199, mae: 2.884734, mean_q: 4.307062\n",
            " 84553/100000: episode: 8433, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000181, mae: 2.981127, mean_q: 4.409657\n",
            " 84561/100000: episode: 8434, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000431, mae: 2.971271, mean_q: 4.407174\n",
            " 84570/100000: episode: 8435, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000517, mae: 2.901934, mean_q: 4.334836\n",
            " 84578/100000: episode: 8436, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000392, mae: 3.096099, mean_q: 4.547241\n",
            " 84586/100000: episode: 8437, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000342, mae: 2.949339, mean_q: 4.370892\n",
            " 84598/100000: episode: 8438, duration: 0.125s, episode steps:  12, steps per second:  96, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.417 [0.000, 7.000],  loss: 0.000405, mae: 2.929824, mean_q: 4.366170\n",
            " 84606/100000: episode: 8439, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000235, mae: 3.194664, mean_q: 4.681442\n",
            " 84615/100000: episode: 8440, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000158, mae: 2.999367, mean_q: 4.453412\n",
            " 84625/100000: episode: 8441, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000155, mae: 2.826336, mean_q: 4.253887\n",
            " 84633/100000: episode: 8442, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000107, mae: 3.106988, mean_q: 4.568876\n",
            " 84643/100000: episode: 8443, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000120, mae: 3.113058, mean_q: 4.587493\n",
            " 84652/100000: episode: 8444, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000077, mae: 3.064592, mean_q: 4.513376\n",
            " 84660/100000: episode: 8445, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000074, mae: 2.985354, mean_q: 4.435398\n",
            " 84668/100000: episode: 8446, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.976881, mean_q: 4.433337\n",
            " 84676/100000: episode: 8447, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 2.639982, mean_q: 4.040841\n",
            " 84685/100000: episode: 8448, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000055, mae: 2.824367, mean_q: 4.250720\n",
            " 84696/100000: episode: 8449, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000033, mae: 3.083494, mean_q: 4.542642\n",
            " 84704/100000: episode: 8450, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 3.061781, mean_q: 4.523817\n",
            " 84714/100000: episode: 8451, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000024, mae: 3.007614, mean_q: 4.461889\n",
            " 84722/100000: episode: 8452, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 2.741781, mean_q: 4.146421\n",
            " 84730/100000: episode: 8453, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000113, mae: 2.800643, mean_q: 4.219323\n",
            " 84739/100000: episode: 8454, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000057, mae: 3.055240, mean_q: 4.517048\n",
            " 84747/100000: episode: 8455, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000063, mae: 2.859628, mean_q: 4.283329\n",
            " 84755/100000: episode: 8456, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000062, mae: 3.015421, mean_q: 4.468793\n",
            " 84764/100000: episode: 8457, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000105, mae: 2.783063, mean_q: 4.197705\n",
            " 84772/100000: episode: 8458, duration: 0.077s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 3.203816, mean_q: 4.682483\n",
            " 84780/100000: episode: 8459, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 2.944205, mean_q: 4.384974\n",
            " 84788/100000: episode: 8460, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.903672, mean_q: 4.339492\n",
            " 84797/100000: episode: 8461, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000067, mae: 3.028672, mean_q: 4.486935\n",
            " 84805/100000: episode: 8462, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000160, mae: 2.955739, mean_q: 4.402786\n",
            " 84813/100000: episode: 8463, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000190, mae: 2.935705, mean_q: 4.382967\n",
            " 84821/100000: episode: 8464, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000248, mae: 3.037663, mean_q: 4.490082\n",
            " 84830/100000: episode: 8465, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000077, mae: 2.878118, mean_q: 4.310205\n",
            " 84839/100000: episode: 8466, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000043, mae: 2.998802, mean_q: 4.456151\n",
            " 84849/100000: episode: 8467, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000043, mae: 2.967183, mean_q: 4.416281\n",
            " 84857/100000: episode: 8468, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 3.088445, mean_q: 4.558891\n",
            " 84866/100000: episode: 8469, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000046, mae: 3.098329, mean_q: 4.575755\n",
            " 84876/100000: episode: 8470, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000046, mae: 2.843027, mean_q: 4.272012\n",
            " 84886/100000: episode: 8471, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000068, mae: 2.938471, mean_q: 4.382832\n",
            " 84894/100000: episode: 8472, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.007988, mean_q: 4.465966\n",
            " 84904/100000: episode: 8473, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000036, mae: 2.922657, mean_q: 4.363875\n",
            " 84913/100000: episode: 8474, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000045, mae: 2.940790, mean_q: 4.388262\n",
            " 84921/100000: episode: 8475, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.897506, mean_q: 4.335326\n",
            " 84930/100000: episode: 8476, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000058, mae: 3.157904, mean_q: 4.635953\n",
            " 84938/100000: episode: 8477, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 3.054862, mean_q: 4.523391\n",
            " 84946/100000: episode: 8478, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.874782, mean_q: 4.304185\n",
            " 84955/100000: episode: 8479, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 3.075296, mean_q: 4.543816\n",
            " 84963/100000: episode: 8480, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.773926, mean_q: 4.191164\n",
            " 84971/100000: episode: 8481, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.885537, mean_q: 4.318688\n",
            " 84981/100000: episode: 8482, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000019, mae: 2.882292, mean_q: 4.320035\n",
            " 84991/100000: episode: 8483, duration: 0.095s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000014, mae: 3.063368, mean_q: 4.540891\n",
            " 85001/100000: episode: 8484, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000008, mae: 3.034800, mean_q: 4.499111\n",
            " 85011/100000: episode: 8485, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000012, mae: 2.989607, mean_q: 4.447996\n",
            " 85020/100000: episode: 8486, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000008, mae: 3.048447, mean_q: 4.513790\n",
            " 85029/100000: episode: 8487, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000012, mae: 2.783613, mean_q: 4.206314\n",
            " 85037/100000: episode: 8488, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.909032, mean_q: 4.348399\n",
            " 85045/100000: episode: 8489, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.040366, mean_q: 4.505076\n",
            " 85054/100000: episode: 8490, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000011, mae: 2.855424, mean_q: 4.289187\n",
            " 85062/100000: episode: 8491, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.977919, mean_q: 4.435714\n",
            " 85071/100000: episode: 8492, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 3.025537, mean_q: 4.487596\n",
            " 85079/100000: episode: 8493, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.970748, mean_q: 4.423112\n",
            " 85087/100000: episode: 8494, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.012570, mean_q: 4.473124\n",
            " 85098/100000: episode: 8495, duration: 0.116s, episode steps:  11, steps per second:  95, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000005, mae: 2.988609, mean_q: 4.444337\n",
            " 85106/100000: episode: 8496, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.814584, mean_q: 4.247214\n",
            " 85115/100000: episode: 8497, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.934347, mean_q: 4.386492\n",
            " 85123/100000: episode: 8498, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.072713, mean_q: 4.537609\n",
            " 85131/100000: episode: 8499, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.088811, mean_q: 4.566665\n",
            " 85140/100000: episode: 8500, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.849599, mean_q: 4.282969\n",
            " 85149/100000: episode: 8501, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.961924, mean_q: 4.417285\n",
            " 85158/100000: episode: 8502, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 2.895837, mean_q: 4.334779\n",
            " 85167/100000: episode: 8503, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 2.755727, mean_q: 4.181036\n",
            " 85175/100000: episode: 8504, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.965929, mean_q: 4.420617\n",
            " 85183/100000: episode: 8505, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.926999, mean_q: 4.371177\n",
            " 85192/100000: episode: 8506, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 2.923440, mean_q: 4.362900\n",
            " 85200/100000: episode: 8507, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.802167, mean_q: 4.226928\n",
            " 85210/100000: episode: 8508, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000014, mae: 2.960541, mean_q: 4.413214\n",
            " 85219/100000: episode: 8509, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000009, mae: 2.962093, mean_q: 4.421844\n",
            " 85227/100000: episode: 8510, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.011089, mean_q: 4.471433\n",
            " 85238/100000: episode: 8511, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000011, mae: 2.934288, mean_q: 4.380625\n",
            " 85248/100000: episode: 8512, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000017, mae: 2.799670, mean_q: 4.223109\n",
            " 85257/100000: episode: 8513, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000039, mae: 2.962226, mean_q: 4.415754\n",
            " 85265/100000: episode: 8514, duration: 0.152s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.888712, mean_q: 4.323081\n",
            " 85274/100000: episode: 8515, duration: 0.151s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000040, mae: 2.754932, mean_q: 4.168164\n",
            " 85284/100000: episode: 8516, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000032, mae: 3.082263, mean_q: 4.544743\n",
            " 85292/100000: episode: 8517, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.944869, mean_q: 4.393197\n",
            " 85301/100000: episode: 8518, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 3.161795, mean_q: 4.643217\n",
            " 85310/100000: episode: 8519, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 2.905200, mean_q: 4.346201\n",
            " 85319/100000: episode: 8520, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000019, mae: 2.845124, mean_q: 4.275730\n",
            " 85327/100000: episode: 8521, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.012766, mean_q: 4.467554\n",
            " 85335/100000: episode: 8522, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.005566, mean_q: 4.462163\n",
            " 85344/100000: episode: 8523, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000009, mae: 3.053182, mean_q: 4.521714\n",
            " 85352/100000: episode: 8524, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.759311, mean_q: 4.177767\n",
            " 85360/100000: episode: 8525, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.054795, mean_q: 4.520015\n",
            " 85368/100000: episode: 8526, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 3.063223, mean_q: 4.526211\n",
            " 85378/100000: episode: 8527, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000066, mae: 2.901309, mean_q: 4.337628\n",
            " 85387/100000: episode: 8528, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000081, mae: 2.859375, mean_q: 4.287195\n",
            " 85396/100000: episode: 8529, duration: 0.171s, episode steps:   9, steps per second:  53, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000053, mae: 2.836796, mean_q: 4.256674\n",
            " 85404/100000: episode: 8530, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 2.973586, mean_q: 4.418230\n",
            " 85412/100000: episode: 8531, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000068, mae: 3.120781, mean_q: 4.593908\n",
            " 85423/100000: episode: 8532, duration: 0.195s, episode steps:  11, steps per second:  57, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.455 [0.000, 7.000],  loss: 0.000034, mae: 2.843601, mean_q: 4.276903\n",
            " 85432/100000: episode: 8533, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000043, mae: 2.895888, mean_q: 4.334108\n",
            " 85440/100000: episode: 8534, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.834062, mean_q: 4.263772\n",
            " 85448/100000: episode: 8535, duration: 0.162s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.923456, mean_q: 4.371243\n",
            " 85456/100000: episode: 8536, duration: 0.169s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.791570, mean_q: 4.211248\n",
            " 85464/100000: episode: 8537, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 3.181498, mean_q: 4.662576\n",
            " 85473/100000: episode: 8538, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000018, mae: 2.983898, mean_q: 4.443784\n",
            " 85482/100000: episode: 8539, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000014, mae: 3.150448, mean_q: 4.629759\n",
            " 85491/100000: episode: 8540, duration: 0.176s, episode steps:   9, steps per second:  51, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000006, mae: 2.957648, mean_q: 4.409580\n",
            " 85499/100000: episode: 8541, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.970659, mean_q: 4.426102\n",
            " 85507/100000: episode: 8542, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.974496, mean_q: 4.427675\n",
            " 85516/100000: episode: 8543, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000020, mae: 2.698606, mean_q: 4.108516\n",
            " 85525/100000: episode: 8544, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 2.949596, mean_q: 4.402875\n",
            " 85534/100000: episode: 8545, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000007, mae: 3.024771, mean_q: 4.492541\n",
            " 85542/100000: episode: 8546, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.811506, mean_q: 4.234300\n",
            " 85551/100000: episode: 8547, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000006, mae: 2.981757, mean_q: 4.436480\n",
            " 85559/100000: episode: 8548, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.817065, mean_q: 4.243121\n",
            " 85567/100000: episode: 8549, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.729628, mean_q: 4.142029\n",
            " 85575/100000: episode: 8550, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.859099, mean_q: 4.293699\n",
            " 85583/100000: episode: 8551, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.794283, mean_q: 4.232042\n",
            " 85593/100000: episode: 8552, duration: 0.121s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000037, mae: 2.794019, mean_q: 4.226925\n",
            " 85602/100000: episode: 8553, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000055, mae: 3.041416, mean_q: 4.500669\n",
            " 85610/100000: episode: 8554, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.960772, mean_q: 4.410524\n",
            " 85618/100000: episode: 8555, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.698430, mean_q: 4.106199\n",
            " 85626/100000: episode: 8556, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000092, mae: 3.120890, mean_q: 4.587668\n",
            " 85635/100000: episode: 8557, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000092, mae: 2.832742, mean_q: 4.264606\n",
            " 85645/100000: episode: 8558, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000067, mae: 3.055379, mean_q: 4.510718\n",
            " 85654/100000: episode: 8559, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000092, mae: 3.029982, mean_q: 4.471797\n",
            " 85663/100000: episode: 8560, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000098, mae: 2.913328, mean_q: 4.350360\n",
            " 85671/100000: episode: 8561, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000273, mae: 3.111376, mean_q: 4.574240\n",
            " 85679/100000: episode: 8562, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000221, mae: 3.015431, mean_q: 4.453791\n",
            " 85689/100000: episode: 8563, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000365, mae: 2.877946, mean_q: 4.312256\n",
            " 85699/100000: episode: 8564, duration: 0.113s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000452, mae: 2.990872, mean_q: 4.437676\n",
            " 85707/100000: episode: 8565, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000251, mae: 2.972736, mean_q: 4.421257\n",
            " 85716/100000: episode: 8566, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000242, mae: 3.005597, mean_q: 4.470773\n",
            " 85724/100000: episode: 8567, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000283, mae: 2.804631, mean_q: 4.211245\n",
            " 85733/100000: episode: 8568, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000134, mae: 2.965170, mean_q: 4.420239\n",
            " 85744/100000: episode: 8569, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000097, mae: 2.898044, mean_q: 4.346232\n",
            " 85752/100000: episode: 8570, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000110, mae: 2.974074, mean_q: 4.422069\n",
            " 85760/100000: episode: 8571, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.811984, mean_q: 4.228770\n",
            " 85768/100000: episode: 8572, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.862737, mean_q: 4.293355\n",
            " 85776/100000: episode: 8573, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.782302, mean_q: 4.201811\n",
            " 85784/100000: episode: 8574, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 3.007413, mean_q: 4.459558\n",
            " 85792/100000: episode: 8575, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.897314, mean_q: 4.331437\n",
            " 85801/100000: episode: 8576, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 3.099328, mean_q: 4.577243\n",
            " 85811/100000: episode: 8577, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000031, mae: 2.873414, mean_q: 4.305825\n",
            " 85819/100000: episode: 8578, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.995272, mean_q: 4.449204\n",
            " 85832/100000: episode: 8579, duration: 0.147s, episode steps:  13, steps per second:  88, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 3.692 [0.000, 7.000],  loss: 0.000036, mae: 2.899016, mean_q: 4.336467\n",
            " 85841/100000: episode: 8580, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000065, mae: 2.918723, mean_q: 4.362755\n",
            " 85850/100000: episode: 8581, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000036, mae: 2.918315, mean_q: 4.357168\n",
            " 85860/100000: episode: 8582, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000026, mae: 2.966230, mean_q: 4.412118\n",
            " 85869/100000: episode: 8583, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000033, mae: 2.967729, mean_q: 4.414206\n",
            " 85877/100000: episode: 8584, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.017963, mean_q: 4.471975\n",
            " 85885/100000: episode: 8585, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.786098, mean_q: 4.206785\n",
            " 85893/100000: episode: 8586, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.968438, mean_q: 4.416455\n",
            " 85901/100000: episode: 8587, duration: 0.087s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.963492, mean_q: 4.413134\n",
            " 85910/100000: episode: 8588, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000012, mae: 2.866159, mean_q: 4.301207\n",
            " 85919/100000: episode: 8589, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000017, mae: 2.896672, mean_q: 4.336981\n",
            " 85928/100000: episode: 8590, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000026, mae: 3.041723, mean_q: 4.502997\n",
            " 85936/100000: episode: 8591, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.892441, mean_q: 4.336545\n",
            " 85945/100000: episode: 8592, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000016, mae: 3.113563, mean_q: 4.591393\n",
            " 85953/100000: episode: 8593, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.059865, mean_q: 4.526138\n",
            " 85961/100000: episode: 8594, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.006080, mean_q: 4.464690\n",
            " 85969/100000: episode: 8595, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.989253, mean_q: 4.442281\n",
            " 85977/100000: episode: 8596, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.942904, mean_q: 4.390912\n",
            " 85986/100000: episode: 8597, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000062, mae: 2.960658, mean_q: 4.408684\n",
            " 85994/100000: episode: 8598, duration: 0.095s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000086, mae: 2.757830, mean_q: 4.177138\n",
            " 86002/100000: episode: 8599, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000089, mae: 2.992502, mean_q: 4.441906\n",
            " 86010/100000: episode: 8600, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 3.194810, mean_q: 4.680337\n",
            " 86018/100000: episode: 8601, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.987881, mean_q: 4.436373\n",
            " 86026/100000: episode: 8602, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.875834, mean_q: 4.309072\n",
            " 86034/100000: episode: 8603, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.930612, mean_q: 4.378222\n",
            " 86045/100000: episode: 8604, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000014, mae: 2.851597, mean_q: 4.283921\n",
            " 86057/100000: episode: 8605, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000012, mae: 2.970756, mean_q: 4.424599\n",
            " 86067/100000: episode: 8606, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000022, mae: 3.027849, mean_q: 4.493823\n",
            " 86077/100000: episode: 8607, duration: 0.097s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000029, mae: 3.182846, mean_q: 4.670542\n",
            " 86085/100000: episode: 8608, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.948737, mean_q: 4.398985\n",
            " 86093/100000: episode: 8609, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.174514, mean_q: 4.658708\n",
            " 86101/100000: episode: 8610, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.856780, mean_q: 4.293791\n",
            " 86110/100000: episode: 8611, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000022, mae: 2.977860, mean_q: 4.427164\n",
            " 86118/100000: episode: 8612, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 2.954347, mean_q: 4.401173\n",
            " 86126/100000: episode: 8613, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000188, mae: 3.093522, mean_q: 4.561275\n",
            " 86134/100000: episode: 8614, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000162, mae: 3.004488, mean_q: 4.450280\n",
            " 86142/100000: episode: 8615, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000062, mae: 3.161644, mean_q: 4.643926\n",
            " 86152/100000: episode: 8616, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000089, mae: 2.921804, mean_q: 4.360526\n",
            " 86161/100000: episode: 8617, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000121, mae: 2.947974, mean_q: 4.382136\n",
            " 86171/100000: episode: 8618, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000153, mae: 2.763102, mean_q: 4.180789\n",
            " 86183/100000: episode: 8619, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.583 [0.000, 7.000],  loss: 0.000098, mae: 3.017757, mean_q: 4.462416\n",
            " 86193/100000: episode: 8620, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000075, mae: 2.872439, mean_q: 4.297174\n",
            " 86201/100000: episode: 8621, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.850003, mean_q: 4.277585\n",
            " 86209/100000: episode: 8622, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 3.113895, mean_q: 4.600123\n",
            " 86217/100000: episode: 8623, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.927723, mean_q: 4.370625\n",
            " 86227/100000: episode: 8624, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000026, mae: 2.951428, mean_q: 4.397273\n",
            " 86237/100000: episode: 8625, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000054, mae: 2.850293, mean_q: 4.284575\n",
            " 86245/100000: episode: 8626, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 3.036785, mean_q: 4.491699\n",
            " 86254/100000: episode: 8627, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000039, mae: 2.997107, mean_q: 4.446344\n",
            " 86262/100000: episode: 8628, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.814034, mean_q: 4.234897\n",
            " 86271/100000: episode: 8629, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000025, mae: 3.027773, mean_q: 4.491004\n",
            " 86279/100000: episode: 8630, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.078826, mean_q: 4.546214\n",
            " 86287/100000: episode: 8631, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.037353, mean_q: 4.505364\n",
            " 86295/100000: episode: 8632, duration: 0.158s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.796564, mean_q: 4.219386\n",
            " 86304/100000: episode: 8633, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000010, mae: 2.757765, mean_q: 4.174677\n",
            " 86313/100000: episode: 8634, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000004, mae: 3.255027, mean_q: 4.752333\n",
            " 86322/100000: episode: 8635, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000053, mae: 2.806429, mean_q: 4.232880\n",
            " 86330/100000: episode: 8636, duration: 0.139s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 3.070724, mean_q: 4.543893\n",
            " 86339/100000: episode: 8637, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000022, mae: 2.831820, mean_q: 4.268677\n",
            " 86348/100000: episode: 8638, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 2.943844, mean_q: 4.396329\n",
            " 86357/100000: episode: 8639, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000028, mae: 2.964892, mean_q: 4.416403\n",
            " 86366/100000: episode: 8640, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000051, mae: 3.026830, mean_q: 4.488928\n",
            " 86374/100000: episode: 8641, duration: 0.132s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.969061, mean_q: 4.423007\n",
            " 86383/100000: episode: 8642, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000027, mae: 2.967048, mean_q: 4.415088\n",
            " 86391/100000: episode: 8643, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.864040, mean_q: 4.293156\n",
            " 86399/100000: episode: 8644, duration: 0.156s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.003217, mean_q: 4.465356\n",
            " 86409/100000: episode: 8645, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000034, mae: 2.966856, mean_q: 4.412585\n",
            " 86417/100000: episode: 8646, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.751779, mean_q: 4.162515\n",
            " 86426/100000: episode: 8647, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000013, mae: 3.138612, mean_q: 4.615641\n",
            " 86434/100000: episode: 8648, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.013875, mean_q: 4.471551\n",
            " 86443/100000: episode: 8649, duration: 0.154s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000049, mae: 2.867981, mean_q: 4.301898\n",
            " 86453/100000: episode: 8650, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000017, mae: 2.809408, mean_q: 4.235290\n",
            " 86461/100000: episode: 8651, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.081226, mean_q: 4.549245\n",
            " 86471/100000: episode: 8652, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000007, mae: 3.019464, mean_q: 4.483056\n",
            " 86479/100000: episode: 8653, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.851914, mean_q: 4.286633\n",
            " 86488/100000: episode: 8654, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 3.029441, mean_q: 4.493910\n",
            " 86497/100000: episode: 8655, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000011, mae: 2.913818, mean_q: 4.360186\n",
            " 86505/100000: episode: 8656, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.963152, mean_q: 4.411228\n",
            " 86513/100000: episode: 8657, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.841889, mean_q: 4.268180\n",
            " 86521/100000: episode: 8658, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.027711, mean_q: 4.485304\n",
            " 86529/100000: episode: 8659, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.959555, mean_q: 4.416821\n",
            " 86538/100000: episode: 8660, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000027, mae: 2.837768, mean_q: 4.274032\n",
            " 86549/100000: episode: 8661, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000039, mae: 2.995444, mean_q: 4.447939\n",
            " 86559/100000: episode: 8662, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000069, mae: 2.866015, mean_q: 4.297534\n",
            " 86569/100000: episode: 8663, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000225, mae: 2.900011, mean_q: 4.338598\n",
            " 86578/100000: episode: 8664, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000181, mae: 3.142725, mean_q: 4.618323\n",
            " 86586/100000: episode: 8665, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.859952, mean_q: 4.279844\n",
            " 86595/100000: episode: 8666, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000352, mae: 3.212411, mean_q: 4.697604\n",
            " 86605/100000: episode: 8667, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000275, mae: 2.908486, mean_q: 4.347171\n",
            " 86613/100000: episode: 8668, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000273, mae: 3.089697, mean_q: 4.556038\n",
            " 86621/100000: episode: 8669, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000117, mae: 2.971138, mean_q: 4.431731\n",
            " 86629/100000: episode: 8670, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.795438, mean_q: 4.217329\n",
            " 86637/100000: episode: 8671, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.975757, mean_q: 4.421613\n",
            " 86647/100000: episode: 8672, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000042, mae: 3.146935, mean_q: 4.625600\n",
            " 86656/100000: episode: 8673, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000054, mae: 2.801733, mean_q: 4.225142\n",
            " 86665/100000: episode: 8674, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000035, mae: 2.956167, mean_q: 4.406406\n",
            " 86674/100000: episode: 8675, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000030, mae: 2.917439, mean_q: 4.357924\n",
            " 86682/100000: episode: 8676, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 3.147618, mean_q: 4.620558\n",
            " 86690/100000: episode: 8677, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.908048, mean_q: 4.349701\n",
            " 86698/100000: episode: 8678, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.775266, mean_q: 4.194573\n",
            " 86710/100000: episode: 8679, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000015, mae: 2.848866, mean_q: 4.278895\n",
            " 86718/100000: episode: 8680, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.998925, mean_q: 4.454337\n",
            " 86726/100000: episode: 8681, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 3.115433, mean_q: 4.588885\n",
            " 86735/100000: episode: 8682, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000028, mae: 2.863998, mean_q: 4.292120\n",
            " 86744/100000: episode: 8683, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000025, mae: 2.959519, mean_q: 4.408532\n",
            " 86752/100000: episode: 8684, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.943010, mean_q: 4.390115\n",
            " 86760/100000: episode: 8685, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.996452, mean_q: 4.452222\n",
            " 86771/100000: episode: 8686, duration: 0.124s, episode steps:  11, steps per second:  89, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000006, mae: 2.888732, mean_q: 4.328344\n",
            " 86779/100000: episode: 8687, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.937270, mean_q: 4.384485\n",
            " 86787/100000: episode: 8688, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.045860, mean_q: 4.513093\n",
            " 86795/100000: episode: 8689, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.837225, mean_q: 4.267409\n",
            " 86805/100000: episode: 8690, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000033, mae: 2.969126, mean_q: 4.424031\n",
            " 86813/100000: episode: 8691, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.841537, mean_q: 4.270980\n",
            " 86821/100000: episode: 8692, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000074, mae: 2.908752, mean_q: 4.352048\n",
            " 86831/100000: episode: 8693, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.940018, mean_q: 4.387108\n",
            " 86839/100000: episode: 8694, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.051784, mean_q: 4.515081\n",
            " 86848/100000: episode: 8695, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.730740, mean_q: 4.138693\n",
            " 86856/100000: episode: 8696, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.702533, mean_q: 4.111201\n",
            " 86864/100000: episode: 8697, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.071212, mean_q: 4.543612\n",
            " 86873/100000: episode: 8698, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000050, mae: 2.997335, mean_q: 4.453788\n",
            " 86882/100000: episode: 8699, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000079, mae: 3.152209, mean_q: 4.623458\n",
            " 86890/100000: episode: 8700, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000119, mae: 2.960055, mean_q: 4.411528\n",
            " 86898/100000: episode: 8701, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000080, mae: 2.910838, mean_q: 4.352759\n",
            " 86907/100000: episode: 8702, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000068, mae: 3.157164, mean_q: 4.635777\n",
            " 86915/100000: episode: 8703, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000149, mae: 3.122561, mean_q: 4.592486\n",
            " 86925/100000: episode: 8704, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000102, mae: 3.017785, mean_q: 4.461804\n",
            " 86934/100000: episode: 8705, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000024, mae: 2.857886, mean_q: 4.292181\n",
            " 86942/100000: episode: 8706, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.816089, mean_q: 4.236590\n",
            " 86950/100000: episode: 8707, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 3.077713, mean_q: 4.548556\n",
            " 86959/100000: episode: 8708, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000023, mae: 2.892824, mean_q: 4.335371\n",
            " 86968/100000: episode: 8709, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000024, mae: 2.962336, mean_q: 4.418608\n",
            " 86977/100000: episode: 8710, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000027, mae: 2.932192, mean_q: 4.375746\n",
            " 86987/100000: episode: 8711, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000057, mae: 3.109668, mean_q: 4.576945\n",
            " 86995/100000: episode: 8712, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 2.893212, mean_q: 4.330717\n",
            " 87003/100000: episode: 8713, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 3.092664, mean_q: 4.549035\n",
            " 87011/100000: episode: 8714, duration: 0.094s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000164, mae: 2.829879, mean_q: 4.238443\n",
            " 87021/100000: episode: 8715, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000275, mae: 2.811019, mean_q: 4.237163\n",
            " 87029/100000: episode: 8716, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000270, mae: 3.051332, mean_q: 4.511238\n",
            " 87037/100000: episode: 8717, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000261, mae: 3.104097, mean_q: 4.560562\n",
            " 87045/100000: episode: 8718, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000273, mae: 3.101650, mean_q: 4.568913\n",
            " 87053/100000: episode: 8719, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000149, mae: 2.824576, mean_q: 4.253734\n",
            " 87061/100000: episode: 8720, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000105, mae: 2.757505, mean_q: 4.179071\n",
            " 87069/100000: episode: 8721, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 2.944877, mean_q: 4.392215\n",
            " 87079/100000: episode: 8722, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.042450, mean_q: 4.500034\n",
            " 87087/100000: episode: 8723, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.973743, mean_q: 4.423765\n",
            " 87095/100000: episode: 8724, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 3.019375, mean_q: 4.479482\n",
            " 87103/100000: episode: 8725, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.851216, mean_q: 4.289323\n",
            " 87111/100000: episode: 8726, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.960164, mean_q: 4.413653\n",
            " 87119/100000: episode: 8727, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.931515, mean_q: 4.376452\n",
            " 87127/100000: episode: 8728, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.894182, mean_q: 4.331712\n",
            " 87136/100000: episode: 8729, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000039, mae: 2.918437, mean_q: 4.364690\n",
            " 87144/100000: episode: 8730, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.763493, mean_q: 4.187567\n",
            " 87152/100000: episode: 8731, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 2.978572, mean_q: 4.435214\n",
            " 87160/100000: episode: 8732, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.929867, mean_q: 4.370438\n",
            " 87168/100000: episode: 8733, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.888405, mean_q: 4.324921\n",
            " 87179/100000: episode: 8734, duration: 0.122s, episode steps:  11, steps per second:  90, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000025, mae: 2.864204, mean_q: 4.300305\n",
            " 87187/100000: episode: 8735, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.884875, mean_q: 4.317286\n",
            " 87195/100000: episode: 8736, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.715627, mean_q: 4.128712\n",
            " 87205/100000: episode: 8737, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000020, mae: 2.930591, mean_q: 4.371887\n",
            " 87214/100000: episode: 8738, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000015, mae: 2.871302, mean_q: 4.308268\n",
            " 87222/100000: episode: 8739, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.183230, mean_q: 4.670341\n",
            " 87230/100000: episode: 8740, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.900441, mean_q: 4.340433\n",
            " 87238/100000: episode: 8741, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.842055, mean_q: 4.276662\n",
            " 87248/100000: episode: 8742, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000021, mae: 3.168376, mean_q: 4.647819\n",
            " 87256/100000: episode: 8743, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.872548, mean_q: 4.308575\n",
            " 87265/100000: episode: 8744, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000012, mae: 3.149573, mean_q: 4.637322\n",
            " 87273/100000: episode: 8745, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.105167, mean_q: 4.575407\n",
            " 87283/100000: episode: 8746, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000033, mae: 3.066496, mean_q: 4.531946\n",
            " 87292/100000: episode: 8747, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000036, mae: 2.889720, mean_q: 4.330416\n",
            " 87300/100000: episode: 8748, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 3.099606, mean_q: 4.566330\n",
            " 87308/100000: episode: 8749, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 3.060582, mean_q: 4.529440\n",
            " 87317/100000: episode: 8750, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000042, mae: 2.877711, mean_q: 4.315989\n",
            " 87326/100000: episode: 8751, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000028, mae: 2.892039, mean_q: 4.333434\n",
            " 87336/100000: episode: 8752, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000022, mae: 3.075940, mean_q: 4.547390\n",
            " 87344/100000: episode: 8753, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.943994, mean_q: 4.389018\n",
            " 87353/100000: episode: 8754, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000025, mae: 2.964683, mean_q: 4.420794\n",
            " 87363/100000: episode: 8755, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000022, mae: 3.098981, mean_q: 4.567151\n",
            " 87372/100000: episode: 8756, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000029, mae: 2.934939, mean_q: 4.375832\n",
            " 87380/100000: episode: 8757, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 3.000516, mean_q: 4.455249\n",
            " 87389/100000: episode: 8758, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000039, mae: 2.838069, mean_q: 4.273800\n",
            " 87398/100000: episode: 8759, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000072, mae: 3.051639, mean_q: 4.518841\n",
            " 87408/100000: episode: 8760, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 3.144955, mean_q: 4.615926\n",
            " 87417/100000: episode: 8761, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000305, mae: 2.992787, mean_q: 4.435123\n",
            " 87425/100000: episode: 8762, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000724, mae: 3.072637, mean_q: 4.531781\n",
            " 87435/100000: episode: 8763, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000266, mae: 2.825625, mean_q: 4.237040\n",
            " 87443/100000: episode: 8764, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000217, mae: 2.959440, mean_q: 4.411061\n",
            " 87451/100000: episode: 8765, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 3.102165, mean_q: 4.569052\n",
            " 87459/100000: episode: 8766, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.802171, mean_q: 4.223258\n",
            " 87468/100000: episode: 8767, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000037, mae: 3.050032, mean_q: 4.513214\n",
            " 87476/100000: episode: 8768, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.952179, mean_q: 4.397888\n",
            " 87485/100000: episode: 8769, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000080, mae: 2.767220, mean_q: 4.182966\n",
            " 87496/100000: episode: 8770, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.818 [0.000, 7.000],  loss: 0.000083, mae: 2.949824, mean_q: 4.393529\n",
            " 87505/100000: episode: 8771, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000054, mae: 2.955186, mean_q: 4.402572\n",
            " 87513/100000: episode: 8772, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 3.100439, mean_q: 4.571886\n",
            " 87521/100000: episode: 8773, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.997452, mean_q: 4.450086\n",
            " 87530/100000: episode: 8774, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000020, mae: 2.908153, mean_q: 4.348461\n",
            " 87538/100000: episode: 8775, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.969558, mean_q: 4.427317\n",
            " 87547/100000: episode: 8776, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000010, mae: 3.079623, mean_q: 4.552743\n",
            " 87555/100000: episode: 8777, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.716920, mean_q: 4.128112\n",
            " 87564/100000: episode: 8778, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000014, mae: 3.123095, mean_q: 4.594110\n",
            " 87575/100000: episode: 8779, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000017, mae: 2.983163, mean_q: 4.434891\n",
            " 87585/100000: episode: 8780, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000037, mae: 2.801197, mean_q: 4.226411\n",
            " 87594/100000: episode: 8781, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 3.038614, mean_q: 4.502205\n",
            " 87603/100000: episode: 8782, duration: 0.128s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000009, mae: 2.964970, mean_q: 4.416991\n",
            " 87612/100000: episode: 8783, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 2.829684, mean_q: 4.257105\n",
            " 87621/100000: episode: 8784, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 2.796673, mean_q: 4.214828\n",
            " 87630/100000: episode: 8785, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 2.701126, mean_q: 4.104517\n",
            " 87639/100000: episode: 8786, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000017, mae: 2.807063, mean_q: 4.235310\n",
            " 87648/100000: episode: 8787, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000020, mae: 2.987201, mean_q: 4.440442\n",
            " 87657/100000: episode: 8788, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000016, mae: 3.006129, mean_q: 4.465145\n",
            " 87668/100000: episode: 8789, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000009, mae: 2.894087, mean_q: 4.334698\n",
            " 87676/100000: episode: 8790, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.750515, mean_q: 4.170228\n",
            " 87684/100000: episode: 8791, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.923269, mean_q: 4.370711\n",
            " 87692/100000: episode: 8792, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.989035, mean_q: 4.444621\n",
            " 87700/100000: episode: 8793, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.842892, mean_q: 4.279413\n",
            " 87709/100000: episode: 8794, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000008, mae: 2.915087, mean_q: 4.367034\n",
            " 87717/100000: episode: 8795, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.897099, mean_q: 4.341524\n",
            " 87725/100000: episode: 8796, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.036733, mean_q: 4.502110\n",
            " 87733/100000: episode: 8797, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.815091, mean_q: 4.239323\n",
            " 87743/100000: episode: 8798, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000008, mae: 2.944199, mean_q: 4.389192\n",
            " 87751/100000: episode: 8799, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.820415, mean_q: 4.249857\n",
            " 87760/100000: episode: 8800, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000006, mae: 2.872812, mean_q: 4.313513\n",
            " 87769/100000: episode: 8801, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 3.016805, mean_q: 4.477023\n",
            " 87777/100000: episode: 8802, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.044506, mean_q: 4.505434\n",
            " 87788/100000: episode: 8803, duration: 0.120s, episode steps:  11, steps per second:  91, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000004, mae: 3.038632, mean_q: 4.501181\n",
            " 87797/100000: episode: 8804, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000006, mae: 3.023946, mean_q: 4.488569\n",
            " 87806/100000: episode: 8805, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000024, mae: 2.769981, mean_q: 4.192006\n",
            " 87814/100000: episode: 8806, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.671260, mean_q: 4.075235\n",
            " 87824/100000: episode: 8807, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000034, mae: 2.938249, mean_q: 4.387969\n",
            " 87833/100000: episode: 8808, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000029, mae: 2.902336, mean_q: 4.343421\n",
            " 87841/100000: episode: 8809, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000035, mae: 2.790672, mean_q: 4.211718\n",
            " 87849/100000: episode: 8810, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.887409, mean_q: 4.320578\n",
            " 87857/100000: episode: 8811, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.123883, mean_q: 4.600412\n",
            " 87866/100000: episode: 8812, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000017, mae: 3.158554, mean_q: 4.633249\n",
            " 87875/100000: episode: 8813, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000016, mae: 3.107933, mean_q: 4.581095\n",
            " 87885/100000: episode: 8814, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000029, mae: 2.959458, mean_q: 4.402705\n",
            " 87893/100000: episode: 8815, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.797514, mean_q: 4.220139\n",
            " 87902/100000: episode: 8816, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000042, mae: 2.909034, mean_q: 4.349277\n",
            " 87910/100000: episode: 8817, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.999632, mean_q: 4.455904\n",
            " 87919/100000: episode: 8818, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000091, mae: 2.900377, mean_q: 4.342178\n",
            " 87927/100000: episode: 8819, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000067, mae: 3.023236, mean_q: 4.481943\n",
            " 87935/100000: episode: 8820, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000118, mae: 2.993881, mean_q: 4.446824\n",
            " 87943/100000: episode: 8821, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000088, mae: 2.859579, mean_q: 4.293808\n",
            " 87951/100000: episode: 8822, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.894323, mean_q: 4.330472\n",
            " 87960/100000: episode: 8823, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000070, mae: 2.958215, mean_q: 4.404909\n",
            " 87968/100000: episode: 8824, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000068, mae: 2.810030, mean_q: 4.227007\n",
            " 87976/100000: episode: 8825, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000050, mae: 2.803564, mean_q: 4.223415\n",
            " 87987/100000: episode: 8826, duration: 0.120s, episode steps:  11, steps per second:  92, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000043, mae: 3.047850, mean_q: 4.515010\n",
            " 87995/100000: episode: 8827, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.827243, mean_q: 4.252819\n",
            " 88003/100000: episode: 8828, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.723063, mean_q: 4.128661\n",
            " 88013/100000: episode: 8829, duration: 0.106s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000043, mae: 3.068146, mean_q: 4.534926\n",
            " 88021/100000: episode: 8830, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.053625, mean_q: 4.518302\n",
            " 88029/100000: episode: 8831, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.846469, mean_q: 4.282389\n",
            " 88039/100000: episode: 8832, duration: 0.104s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000015, mae: 3.029575, mean_q: 4.488265\n",
            " 88048/100000: episode: 8833, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 2.970257, mean_q: 4.421663\n",
            " 88056/100000: episode: 8834, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.113857, mean_q: 4.588095\n",
            " 88064/100000: episode: 8835, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.979662, mean_q: 4.437384\n",
            " 88072/100000: episode: 8836, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.862404, mean_q: 4.299119\n",
            " 88083/100000: episode: 8837, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000008, mae: 2.805648, mean_q: 4.231289\n",
            " 88091/100000: episode: 8838, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.990483, mean_q: 4.443040\n",
            " 88099/100000: episode: 8839, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.874776, mean_q: 4.305208\n",
            " 88109/100000: episode: 8840, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000004, mae: 3.029260, mean_q: 4.490562\n",
            " 88117/100000: episode: 8841, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.862652, mean_q: 4.301277\n",
            " 88125/100000: episode: 8842, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.890701, mean_q: 4.331674\n",
            " 88133/100000: episode: 8843, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.888943, mean_q: 4.327112\n",
            " 88141/100000: episode: 8844, duration: 0.169s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.936057, mean_q: 4.377029\n",
            " 88151/100000: episode: 8845, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000002, mae: 3.089993, mean_q: 4.563537\n",
            " 88159/100000: episode: 8846, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.882556, mean_q: 4.325513\n",
            " 88168/100000: episode: 8847, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.887814, mean_q: 4.329826\n",
            " 88176/100000: episode: 8848, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.847807, mean_q: 4.284643\n",
            " 88186/100000: episode: 8849, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 2.824318, mean_q: 4.258183\n",
            " 88195/100000: episode: 8850, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 3.041595, mean_q: 4.511658\n",
            " 88204/100000: episode: 8851, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000005, mae: 2.961091, mean_q: 4.411902\n",
            " 88213/100000: episode: 8852, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000004, mae: 2.887281, mean_q: 4.331189\n",
            " 88223/100000: episode: 8853, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000004, mae: 2.894165, mean_q: 4.336398\n",
            " 88231/100000: episode: 8854, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.845426, mean_q: 4.280767\n",
            " 88240/100000: episode: 8855, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.862811, mean_q: 4.301434\n",
            " 88248/100000: episode: 8856, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.951692, mean_q: 4.400881\n",
            " 88257/100000: episode: 8857, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.988719, mean_q: 4.443844\n",
            " 88266/100000: episode: 8858, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 3.117139, mean_q: 4.590515\n",
            " 88274/100000: episode: 8859, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.077042, mean_q: 4.552557\n",
            " 88282/100000: episode: 8860, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.952676, mean_q: 4.406533\n",
            " 88290/100000: episode: 8861, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.039090, mean_q: 4.499997\n",
            " 88298/100000: episode: 8862, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.057734, mean_q: 4.525312\n",
            " 88307/100000: episode: 8863, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000009, mae: 2.996808, mean_q: 4.456177\n",
            " 88316/100000: episode: 8864, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000010, mae: 2.929773, mean_q: 4.374019\n",
            " 88325/100000: episode: 8865, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 3.050710, mean_q: 4.516611\n",
            " 88336/100000: episode: 8866, duration: 0.124s, episode steps:  11, steps per second:  89, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.636 [0.000, 7.000],  loss: 0.000010, mae: 2.743275, mean_q: 4.157033\n",
            " 88344/100000: episode: 8867, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 3.039446, mean_q: 4.501125\n",
            " 88352/100000: episode: 8868, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.977509, mean_q: 4.432261\n",
            " 88360/100000: episode: 8869, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.939328, mean_q: 4.388210\n",
            " 88368/100000: episode: 8870, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.876314, mean_q: 4.305991\n",
            " 88376/100000: episode: 8871, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.771980, mean_q: 4.183798\n",
            " 88385/100000: episode: 8872, duration: 0.144s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000023, mae: 2.851079, mean_q: 4.294480\n",
            " 88393/100000: episode: 8873, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.218574, mean_q: 4.704537\n",
            " 88401/100000: episode: 8874, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.830488, mean_q: 4.262001\n",
            " 88410/100000: episode: 8875, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000075, mae: 2.900458, mean_q: 4.348129\n",
            " 88418/100000: episode: 8876, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000108, mae: 3.092556, mean_q: 4.550689\n",
            " 88426/100000: episode: 8877, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000094, mae: 2.967796, mean_q: 4.420035\n",
            " 88434/100000: episode: 8878, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 2.966687, mean_q: 4.415021\n",
            " 88443/100000: episode: 8879, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000042, mae: 3.066663, mean_q: 4.529627\n",
            " 88451/100000: episode: 8880, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 2.983438, mean_q: 4.436858\n",
            " 88459/100000: episode: 8881, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 3.042590, mean_q: 4.499239\n",
            " 88468/100000: episode: 8882, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000033, mae: 2.940366, mean_q: 4.392904\n",
            " 88476/100000: episode: 8883, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 2.819701, mean_q: 4.252167\n",
            " 88484/100000: episode: 8884, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000060, mae: 3.025301, mean_q: 4.488401\n",
            " 88492/100000: episode: 8885, duration: 0.144s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.859976, mean_q: 4.293412\n",
            " 88500/100000: episode: 8886, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000081, mae: 2.992122, mean_q: 4.440633\n",
            " 88509/100000: episode: 8887, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000053, mae: 3.117228, mean_q: 4.589693\n",
            " 88517/100000: episode: 8888, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 2.961205, mean_q: 4.413370\n",
            " 88526/100000: episode: 8889, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000031, mae: 3.036613, mean_q: 4.496656\n",
            " 88536/100000: episode: 8890, duration: 0.140s, episode steps:  10, steps per second:  72, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000012, mae: 3.003806, mean_q: 4.461516\n",
            " 88545/100000: episode: 8891, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000026, mae: 3.168299, mean_q: 4.652112\n",
            " 88554/100000: episode: 8892, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000023, mae: 2.947967, mean_q: 4.402125\n",
            " 88563/100000: episode: 8893, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000039, mae: 3.094196, mean_q: 4.562086\n",
            " 88571/100000: episode: 8894, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 3.120682, mean_q: 4.591468\n",
            " 88579/100000: episode: 8895, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 2.916777, mean_q: 4.354825\n",
            " 88588/100000: episode: 8896, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000082, mae: 2.951016, mean_q: 4.404926\n",
            " 88598/100000: episode: 8897, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000276, mae: 2.907438, mean_q: 4.340211\n",
            " 88606/100000: episode: 8898, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000333, mae: 3.055470, mean_q: 4.512620\n",
            " 88616/100000: episode: 8899, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000353, mae: 3.006562, mean_q: 4.457229\n",
            " 88625/100000: episode: 8900, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000465, mae: 2.886953, mean_q: 4.314539\n",
            " 88634/100000: episode: 8901, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000403, mae: 2.969648, mean_q: 4.414477\n",
            " 88644/100000: episode: 8902, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000307, mae: 2.802218, mean_q: 4.217096\n",
            " 88652/100000: episode: 8903, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000156, mae: 3.060462, mean_q: 4.522757\n",
            " 88660/100000: episode: 8904, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000145, mae: 3.022334, mean_q: 4.471145\n",
            " 88668/100000: episode: 8905, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000092, mae: 3.076937, mean_q: 4.537145\n",
            " 88676/100000: episode: 8906, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.835653, mean_q: 4.258685\n",
            " 88686/100000: episode: 8907, duration: 0.149s, episode steps:  10, steps per second:  67, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000028, mae: 2.894603, mean_q: 4.330564\n",
            " 88696/100000: episode: 8908, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000045, mae: 2.839660, mean_q: 4.266177\n",
            " 88705/100000: episode: 8909, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000050, mae: 2.959761, mean_q: 4.398427\n",
            " 88713/100000: episode: 8910, duration: 0.175s, episode steps:   8, steps per second:  46, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 2.846251, mean_q: 4.276937\n",
            " 88723/100000: episode: 8911, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000064, mae: 2.980662, mean_q: 4.435335\n",
            " 88732/100000: episode: 8912, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000035, mae: 2.896405, mean_q: 4.336344\n",
            " 88740/100000: episode: 8913, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 3.013539, mean_q: 4.467873\n",
            " 88749/100000: episode: 8914, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000025, mae: 2.958731, mean_q: 4.411214\n",
            " 88757/100000: episode: 8915, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 3.053896, mean_q: 4.518692\n",
            " 88766/100000: episode: 8916, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000023, mae: 2.953181, mean_q: 4.404051\n",
            " 88774/100000: episode: 8917, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.816293, mean_q: 4.246755\n",
            " 88783/100000: episode: 8918, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000082, mae: 2.923092, mean_q: 4.370588\n",
            " 88792/100000: episode: 8919, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000061, mae: 3.197619, mean_q: 4.681035\n",
            " 88800/100000: episode: 8920, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 2.818336, mean_q: 4.241503\n",
            " 88810/100000: episode: 8921, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.947828, mean_q: 4.389942\n",
            " 88819/100000: episode: 8922, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000051, mae: 3.012889, mean_q: 4.475721\n",
            " 88827/100000: episode: 8923, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000060, mae: 2.946735, mean_q: 4.397769\n",
            " 88836/100000: episode: 8924, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000040, mae: 2.849716, mean_q: 4.277653\n",
            " 88844/100000: episode: 8925, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.979134, mean_q: 4.432221\n",
            " 88853/100000: episode: 8926, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000094, mae: 3.003716, mean_q: 4.465625\n",
            " 88861/100000: episode: 8927, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000074, mae: 3.048273, mean_q: 4.505601\n",
            " 88869/100000: episode: 8928, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000057, mae: 2.844446, mean_q: 4.271600\n",
            " 88877/100000: episode: 8929, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.819332, mean_q: 4.252564\n",
            " 88888/100000: episode: 8930, duration: 0.113s, episode steps:  11, steps per second:  97, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000028, mae: 2.847886, mean_q: 4.291296\n",
            " 88898/100000: episode: 8931, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000029, mae: 2.993238, mean_q: 4.446683\n",
            " 88907/100000: episode: 8932, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000054, mae: 3.022941, mean_q: 4.472965\n",
            " 88915/100000: episode: 8933, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.918355, mean_q: 4.367264\n",
            " 88923/100000: episode: 8934, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.928417, mean_q: 4.376335\n",
            " 88931/100000: episode: 8935, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 3.077431, mean_q: 4.546764\n",
            " 88940/100000: episode: 8936, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000060, mae: 2.980211, mean_q: 4.429377\n",
            " 88949/100000: episode: 8937, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000049, mae: 2.992544, mean_q: 4.453112\n",
            " 88958/100000: episode: 8938, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000052, mae: 2.760355, mean_q: 4.181748\n",
            " 88966/100000: episode: 8939, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.763372, mean_q: 4.187772\n",
            " 88975/100000: episode: 8940, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000021, mae: 2.920347, mean_q: 4.364538\n",
            " 88983/100000: episode: 8941, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.811131, mean_q: 4.234033\n",
            " 88991/100000: episode: 8942, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.935966, mean_q: 4.377236\n",
            " 88999/100000: episode: 8943, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.129063, mean_q: 4.608734\n",
            " 89008/100000: episode: 8944, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000016, mae: 3.043904, mean_q: 4.514659\n",
            " 89016/100000: episode: 8945, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.914760, mean_q: 4.356833\n",
            " 89025/100000: episode: 8946, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000026, mae: 3.035693, mean_q: 4.492798\n",
            " 89033/100000: episode: 8947, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.940445, mean_q: 4.384695\n",
            " 89044/100000: episode: 8948, duration: 0.114s, episode steps:  11, steps per second:  96, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.909 [0.000, 7.000],  loss: 0.000018, mae: 2.902782, mean_q: 4.351453\n",
            " 89053/100000: episode: 8949, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000017, mae: 3.061185, mean_q: 4.526071\n",
            " 89062/100000: episode: 8950, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000013, mae: 2.873207, mean_q: 4.310842\n",
            " 89071/100000: episode: 8951, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000011, mae: 3.015760, mean_q: 4.473469\n",
            " 89080/100000: episode: 8952, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.944803, mean_q: 4.391409\n",
            " 89089/100000: episode: 8953, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 2.897610, mean_q: 4.342197\n",
            " 89098/100000: episode: 8954, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000008, mae: 3.023623, mean_q: 4.484994\n",
            " 89108/100000: episode: 8955, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000003, mae: 2.858242, mean_q: 4.291350\n",
            " 89116/100000: episode: 8956, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.000523, mean_q: 4.459212\n",
            " 89125/100000: episode: 8957, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000007, mae: 2.924984, mean_q: 4.372492\n",
            " 89134/100000: episode: 8958, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.860734, mean_q: 4.294306\n",
            " 89144/100000: episode: 8959, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000008, mae: 3.100893, mean_q: 4.575472\n",
            " 89152/100000: episode: 8960, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.009892, mean_q: 4.468526\n",
            " 89160/100000: episode: 8961, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.024565, mean_q: 4.487351\n",
            " 89168/100000: episode: 8962, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.019062, mean_q: 4.476210\n",
            " 89177/100000: episode: 8963, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 2.932375, mean_q: 4.379568\n",
            " 89185/100000: episode: 8964, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.850294, mean_q: 4.283181\n",
            " 89193/100000: episode: 8965, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.071690, mean_q: 4.542391\n",
            " 89203/100000: episode: 8966, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 3.083663, mean_q: 4.557514\n",
            " 89211/100000: episode: 8967, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.059889, mean_q: 4.524003\n",
            " 89219/100000: episode: 8968, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.103067, mean_q: 4.571581\n",
            " 89227/100000: episode: 8969, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.182057, mean_q: 4.663791\n",
            " 89235/100000: episode: 8970, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.049381, mean_q: 4.516575\n",
            " 89243/100000: episode: 8971, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.869077, mean_q: 4.309373\n",
            " 89252/100000: episode: 8972, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.992444, mean_q: 4.453679\n",
            " 89260/100000: episode: 8973, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.056304, mean_q: 4.525172\n",
            " 89268/100000: episode: 8974, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.723700, mean_q: 4.134605\n",
            " 89277/100000: episode: 8975, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000004, mae: 2.918871, mean_q: 4.365473\n",
            " 89286/100000: episode: 8976, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 2.821059, mean_q: 4.245584\n",
            " 89294/100000: episode: 8977, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.979260, mean_q: 4.432438\n",
            " 89302/100000: episode: 8978, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.984914, mean_q: 4.442570\n",
            " 89311/100000: episode: 8979, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000009, mae: 3.090400, mean_q: 4.564853\n",
            " 89320/100000: episode: 8980, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000012, mae: 3.176156, mean_q: 4.663770\n",
            " 89328/100000: episode: 8981, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.028858, mean_q: 4.492104\n",
            " 89337/100000: episode: 8982, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 2.923586, mean_q: 4.368963\n",
            " 89345/100000: episode: 8983, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.970163, mean_q: 4.426066\n",
            " 89353/100000: episode: 8984, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.989157, mean_q: 4.446071\n",
            " 89362/100000: episode: 8985, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000019, mae: 2.996640, mean_q: 4.450606\n",
            " 89370/100000: episode: 8986, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 3.123872, mean_q: 4.599241\n",
            " 89379/100000: episode: 8987, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000029, mae: 2.827108, mean_q: 4.254074\n",
            " 89388/100000: episode: 8988, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000037, mae: 2.783697, mean_q: 4.199102\n",
            " 89397/100000: episode: 8989, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000040, mae: 3.060415, mean_q: 4.528063\n",
            " 89405/100000: episode: 8990, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.162130, mean_q: 4.643547\n",
            " 89415/100000: episode: 8991, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000022, mae: 3.033167, mean_q: 4.497218\n",
            " 89423/100000: episode: 8992, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.970431, mean_q: 4.427016\n",
            " 89432/100000: episode: 8993, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000013, mae: 3.013713, mean_q: 4.479213\n",
            " 89440/100000: episode: 8994, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.841067, mean_q: 4.272574\n",
            " 89449/100000: episode: 8995, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000091, mae: 3.082403, mean_q: 4.541798\n",
            " 89457/100000: episode: 8996, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000117, mae: 3.005426, mean_q: 4.455704\n",
            " 89466/100000: episode: 8997, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000172, mae: 2.900565, mean_q: 4.334546\n",
            " 89475/100000: episode: 8998, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000184, mae: 3.028843, mean_q: 4.474967\n",
            " 89483/100000: episode: 8999, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000094, mae: 3.209961, mean_q: 4.680795\n",
            " 89492/100000: episode: 9000, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000096, mae: 3.021213, mean_q: 4.462636\n",
            " 89501/100000: episode: 9001, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000172, mae: 2.817183, mean_q: 4.226798\n",
            " 89509/100000: episode: 9002, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000411, mae: 3.059664, mean_q: 4.520173\n",
            " 89521/100000: episode: 9003, duration: 0.117s, episode steps:  12, steps per second: 103, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 2.917 [0.000, 7.000],  loss: 0.000364, mae: 3.014476, mean_q: 4.469615\n",
            " 89530/100000: episode: 9004, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000467, mae: 2.903875, mean_q: 4.337270\n",
            " 89538/100000: episode: 9005, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000834, mae: 2.807284, mean_q: 4.220311\n",
            " 89546/100000: episode: 9006, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000443, mae: 2.762892, mean_q: 4.182065\n",
            " 89555/100000: episode: 9007, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000186, mae: 3.076574, mean_q: 4.535985\n",
            " 89563/100000: episode: 9008, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000319, mae: 2.920840, mean_q: 4.359624\n",
            " 89571/100000: episode: 9009, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000191, mae: 2.877119, mean_q: 4.313066\n",
            " 89579/100000: episode: 9010, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000110, mae: 3.102648, mean_q: 4.576784\n",
            " 89588/100000: episode: 9011, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000082, mae: 2.819337, mean_q: 4.245182\n",
            " 89597/100000: episode: 9012, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000064, mae: 2.920747, mean_q: 4.365348\n",
            " 89605/100000: episode: 9013, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000068, mae: 2.994876, mean_q: 4.449087\n",
            " 89613/100000: episode: 9014, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 3.266725, mean_q: 4.762319\n",
            " 89621/100000: episode: 9015, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 3.052933, mean_q: 4.514159\n",
            " 89631/100000: episode: 9016, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000036, mae: 2.857762, mean_q: 4.291459\n",
            " 89639/100000: episode: 9017, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.068069, mean_q: 4.544104\n",
            " 89647/100000: episode: 9018, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.940813, mean_q: 4.393238\n",
            " 89655/100000: episode: 9019, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.901661, mean_q: 4.341537\n",
            " 89664/100000: episode: 9020, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000010, mae: 2.910995, mean_q: 4.352100\n",
            " 89675/100000: episode: 9021, duration: 0.158s, episode steps:  11, steps per second:  70, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000008, mae: 3.135074, mean_q: 4.614235\n",
            " 89684/100000: episode: 9022, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.917397, mean_q: 4.367465\n",
            " 89692/100000: episode: 9023, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.974838, mean_q: 4.429560\n",
            " 89700/100000: episode: 9024, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.787956, mean_q: 4.214717\n",
            " 89710/100000: episode: 9025, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000004, mae: 2.876556, mean_q: 4.314632\n",
            " 89719/100000: episode: 9026, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 2.927827, mean_q: 4.372561\n",
            " 89727/100000: episode: 9027, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.037310, mean_q: 4.505163\n",
            " 89736/100000: episode: 9028, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.852385, mean_q: 4.284643\n",
            " 89744/100000: episode: 9029, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.020423, mean_q: 4.480093\n",
            " 89753/100000: episode: 9030, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.993184, mean_q: 4.452939\n",
            " 89762/100000: episode: 9031, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.994834, mean_q: 4.449483\n",
            " 89770/100000: episode: 9032, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.081013, mean_q: 4.554135\n",
            " 89778/100000: episode: 9033, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.796390, mean_q: 4.226260\n",
            " 89788/100000: episode: 9034, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000002, mae: 2.863944, mean_q: 4.301733\n",
            " 89797/100000: episode: 9035, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.877288, mean_q: 4.320461\n",
            " 89805/100000: episode: 9036, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.077251, mean_q: 4.544227\n",
            " 89814/100000: episode: 9037, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000004, mae: 2.666474, mean_q: 4.074749\n",
            " 89822/100000: episode: 9038, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.877879, mean_q: 4.317141\n",
            " 89830/100000: episode: 9039, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.673324, mean_q: 4.082539\n",
            " 89839/100000: episode: 9040, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.808522, mean_q: 4.238149\n",
            " 89847/100000: episode: 9041, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.151010, mean_q: 4.633327\n",
            " 89856/100000: episode: 9042, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.957614, mean_q: 4.403779\n",
            " 89866/100000: episode: 9043, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000001, mae: 2.891636, mean_q: 4.332816\n",
            " 89874/100000: episode: 9044, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.081246, mean_q: 4.552010\n",
            " 89882/100000: episode: 9045, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.015926, mean_q: 4.475492\n",
            " 89891/100000: episode: 9046, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.670606, mean_q: 4.075826\n",
            " 89901/100000: episode: 9047, duration: 0.089s, episode steps:  10, steps per second: 112, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.921590, mean_q: 4.366837\n",
            " 89910/100000: episode: 9048, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000003, mae: 2.879491, mean_q: 4.315764\n",
            " 89920/100000: episode: 9049, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000003, mae: 2.826689, mean_q: 4.254847\n",
            " 89928/100000: episode: 9050, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.870680, mean_q: 4.303626\n",
            " 89936/100000: episode: 9051, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.961796, mean_q: 4.415148\n",
            " 89944/100000: episode: 9052, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.008373, mean_q: 4.465208\n",
            " 89953/100000: episode: 9053, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 2.821846, mean_q: 4.254129\n",
            " 89962/100000: episode: 9054, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 3.042917, mean_q: 4.508575\n",
            " 89971/100000: episode: 9055, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000014, mae: 3.043151, mean_q: 4.504709\n",
            " 89979/100000: episode: 9056, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.913251, mean_q: 4.361421\n",
            " 89988/100000: episode: 9057, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.963797, mean_q: 4.413080\n",
            " 89997/100000: episode: 9058, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000007, mae: 2.955733, mean_q: 4.401339\n",
            " 90005/100000: episode: 9059, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.033163, mean_q: 4.493569\n",
            " 90013/100000: episode: 9060, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.739970, mean_q: 4.156051\n",
            " 90022/100000: episode: 9061, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 3.056370, mean_q: 4.526011\n",
            " 90030/100000: episode: 9062, duration: 0.072s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.911095, mean_q: 4.357319\n",
            " 90043/100000: episode: 9063, duration: 0.143s, episode steps:  13, steps per second:  91, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 3.538 [0.000, 7.000],  loss: 0.000003, mae: 2.929459, mean_q: 4.377498\n",
            " 90051/100000: episode: 9064, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.821984, mean_q: 4.247879\n",
            " 90059/100000: episode: 9065, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.082911, mean_q: 4.556237\n",
            " 90068/100000: episode: 9066, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.869131, mean_q: 4.304822\n",
            " 90078/100000: episode: 9067, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.904963, mean_q: 4.349233\n",
            " 90086/100000: episode: 9068, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.925252, mean_q: 4.369630\n",
            " 90095/100000: episode: 9069, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.940406, mean_q: 4.388990\n",
            " 90103/100000: episode: 9070, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.869232, mean_q: 4.301876\n",
            " 90111/100000: episode: 9071, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.906518, mean_q: 4.348771\n",
            " 90119/100000: episode: 9072, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.821571, mean_q: 4.254447\n",
            " 90127/100000: episode: 9073, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.822988, mean_q: 4.249343\n",
            " 90135/100000: episode: 9074, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.019985, mean_q: 4.475814\n",
            " 90143/100000: episode: 9075, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.040339, mean_q: 4.510155\n",
            " 90151/100000: episode: 9076, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.907152, mean_q: 4.348909\n",
            " 90159/100000: episode: 9077, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.914109, mean_q: 4.358877\n",
            " 90168/100000: episode: 9078, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000027, mae: 2.996943, mean_q: 4.456871\n",
            " 90176/100000: episode: 9079, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 2.963880, mean_q: 4.415074\n",
            " 90186/100000: episode: 9080, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000059, mae: 3.056175, mean_q: 4.516235\n",
            " 90194/100000: episode: 9081, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 3.012495, mean_q: 4.468224\n",
            " 90203/100000: episode: 9082, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000051, mae: 2.937866, mean_q: 4.379847\n",
            " 90211/100000: episode: 9083, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 3.054962, mean_q: 4.521475\n",
            " 90224/100000: episode: 9084, duration: 0.135s, episode steps:  13, steps per second:  96, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 4.308 [0.000, 7.000],  loss: 0.000045, mae: 2.825955, mean_q: 4.247857\n",
            " 90235/100000: episode: 9085, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000052, mae: 3.031533, mean_q: 4.489423\n",
            " 90244/100000: episode: 9086, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000035, mae: 2.775946, mean_q: 4.201773\n",
            " 90254/100000: episode: 9087, duration: 0.118s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000082, mae: 2.975606, mean_q: 4.424838\n",
            " 90263/100000: episode: 9088, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000049, mae: 3.042167, mean_q: 4.494639\n",
            " 90273/100000: episode: 9089, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000051, mae: 3.020532, mean_q: 4.477606\n",
            " 90281/100000: episode: 9090, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 2.909405, mean_q: 4.356470\n",
            " 90289/100000: episode: 9091, duration: 0.158s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 3.121784, mean_q: 4.596757\n",
            " 90297/100000: episode: 9092, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 2.776214, mean_q: 4.198500\n",
            " 90305/100000: episode: 9093, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.972423, mean_q: 4.422532\n",
            " 90313/100000: episode: 9094, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 3.111799, mean_q: 4.576907\n",
            " 90322/100000: episode: 9095, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000070, mae: 2.843077, mean_q: 4.278869\n",
            " 90332/100000: episode: 9096, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.883266, mean_q: 4.321332\n",
            " 90341/100000: episode: 9097, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000072, mae: 2.863764, mean_q: 4.295384\n",
            " 90349/100000: episode: 9098, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000060, mae: 3.028836, mean_q: 4.495825\n",
            " 90357/100000: episode: 9099, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 3.060065, mean_q: 4.524721\n",
            " 90367/100000: episode: 9100, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000074, mae: 2.930318, mean_q: 4.363991\n",
            " 90376/100000: episode: 9101, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000096, mae: 2.932282, mean_q: 4.374225\n",
            " 90385/100000: episode: 9102, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000068, mae: 3.100187, mean_q: 4.570818\n",
            " 90393/100000: episode: 9103, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000084, mae: 2.826654, mean_q: 4.254516\n",
            " 90401/100000: episode: 9104, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000103, mae: 2.910594, mean_q: 4.348969\n",
            " 90411/100000: episode: 9105, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000164, mae: 3.095733, mean_q: 4.561391\n",
            " 90422/100000: episode: 9106, duration: 0.121s, episode steps:  11, steps per second:  91, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 2.818 [0.000, 7.000],  loss: 0.000121, mae: 2.891815, mean_q: 4.333497\n",
            " 90430/100000: episode: 9107, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000080, mae: 3.007781, mean_q: 4.470349\n",
            " 90439/100000: episode: 9108, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000264, mae: 3.024417, mean_q: 4.480152\n",
            " 90447/100000: episode: 9109, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000239, mae: 2.786978, mean_q: 4.188865\n",
            " 90456/100000: episode: 9110, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000239, mae: 2.974929, mean_q: 4.411510\n",
            " 90464/100000: episode: 9111, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000214, mae: 3.107274, mean_q: 4.573699\n",
            " 90472/100000: episode: 9112, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000265, mae: 2.982530, mean_q: 4.433916\n",
            " 90481/100000: episode: 9113, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000181, mae: 2.935497, mean_q: 4.371527\n",
            " 90489/100000: episode: 9114, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000124, mae: 3.048732, mean_q: 4.510352\n",
            " 90497/100000: episode: 9115, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000087, mae: 3.078625, mean_q: 4.552152\n",
            " 90507/100000: episode: 9116, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000088, mae: 3.038067, mean_q: 4.495691\n",
            " 90516/100000: episode: 9117, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000094, mae: 2.923295, mean_q: 4.367967\n",
            " 90526/100000: episode: 9118, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000087, mae: 2.530614, mean_q: 3.919296\n",
            " 90534/100000: episode: 9119, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000073, mae: 3.061155, mean_q: 4.522924\n",
            " 90543/100000: episode: 9120, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000050, mae: 2.815857, mean_q: 4.240381\n",
            " 90551/100000: episode: 9121, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.811059, mean_q: 4.236377\n",
            " 90559/100000: episode: 9122, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.796217, mean_q: 4.223052\n",
            " 90570/100000: episode: 9123, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000024, mae: 2.774827, mean_q: 4.200716\n",
            " 90578/100000: episode: 9124, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.891500, mean_q: 4.336665\n",
            " 90587/100000: episode: 9125, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000014, mae: 2.891063, mean_q: 4.337652\n",
            " 90595/100000: episode: 9126, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.989781, mean_q: 4.450985\n",
            " 90605/100000: episode: 9127, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000006, mae: 2.834496, mean_q: 4.265924\n",
            " 90616/100000: episode: 9128, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000005, mae: 2.780706, mean_q: 4.205425\n",
            " 90625/100000: episode: 9129, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.811757, mean_q: 4.243734\n",
            " 90634/100000: episode: 9130, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.878114, mean_q: 4.314642\n",
            " 90644/100000: episode: 9131, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000008, mae: 2.893555, mean_q: 4.334194\n",
            " 90652/100000: episode: 9132, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.913358, mean_q: 4.356491\n",
            " 90660/100000: episode: 9133, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.923792, mean_q: 4.364902\n",
            " 90669/100000: episode: 9134, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000016, mae: 2.802400, mean_q: 4.228179\n",
            " 90678/100000: episode: 9135, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.933394, mean_q: 4.380938\n",
            " 90688/100000: episode: 9136, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000008, mae: 2.910765, mean_q: 4.354688\n",
            " 90697/100000: episode: 9137, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.929592, mean_q: 4.375353\n",
            " 90707/100000: episode: 9138, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 2.940111, mean_q: 4.392387\n",
            " 90715/100000: episode: 9139, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.986948, mean_q: 4.438101\n",
            " 90723/100000: episode: 9140, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.994740, mean_q: 4.452863\n",
            " 90732/100000: episode: 9141, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.895438, mean_q: 4.335689\n",
            " 90741/100000: episode: 9142, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.908365, mean_q: 4.351377\n",
            " 90749/100000: episode: 9143, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.150959, mean_q: 4.630331\n",
            " 90757/100000: episode: 9144, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.881060, mean_q: 4.317361\n",
            " 90767/100000: episode: 9145, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000001, mae: 2.972185, mean_q: 4.429389\n",
            " 90779/100000: episode: 9146, duration: 0.111s, episode steps:  12, steps per second: 108, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.250 [0.000, 7.000],  loss: 0.000002, mae: 3.019251, mean_q: 4.481503\n",
            " 90787/100000: episode: 9147, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.904167, mean_q: 4.342411\n",
            " 90797/100000: episode: 9148, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000001, mae: 2.802417, mean_q: 4.227410\n",
            " 90805/100000: episode: 9149, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.078829, mean_q: 4.546984\n",
            " 90814/100000: episode: 9150, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.116479, mean_q: 4.586562\n",
            " 90822/100000: episode: 9151, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.793298, mean_q: 4.218086\n",
            " 90830/100000: episode: 9152, duration: 0.132s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.809939, mean_q: 4.234268\n",
            " 90838/100000: episode: 9153, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.826101, mean_q: 4.257179\n",
            " 90846/100000: episode: 9154, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.124936, mean_q: 4.605819\n",
            " 90856/100000: episode: 9155, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 2.850980, mean_q: 4.287125\n",
            " 90864/100000: episode: 9156, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.970955, mean_q: 4.419693\n",
            " 90872/100000: episode: 9157, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.889272, mean_q: 4.328266\n",
            " 90880/100000: episode: 9158, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.039135, mean_q: 4.501995\n",
            " 90888/100000: episode: 9159, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.966012, mean_q: 4.416584\n",
            " 90897/100000: episode: 9160, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 2.853104, mean_q: 4.285716\n",
            " 90905/100000: episode: 9161, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.903803, mean_q: 4.343863\n",
            " 90913/100000: episode: 9162, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.949472, mean_q: 4.394686\n",
            " 90922/100000: episode: 9163, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 3.154029, mean_q: 4.632166\n",
            " 90930/100000: episode: 9164, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.950220, mean_q: 4.405632\n",
            " 90938/100000: episode: 9165, duration: 0.120s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.722997, mean_q: 4.141115\n",
            " 90947/100000: episode: 9166, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.950030, mean_q: 4.400973\n",
            " 90956/100000: episode: 9167, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000007, mae: 2.885814, mean_q: 4.326347\n",
            " 90964/100000: episode: 9168, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.661136, mean_q: 4.066838\n",
            " 90974/100000: episode: 9169, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000017, mae: 2.910578, mean_q: 4.355717\n",
            " 90983/100000: episode: 9170, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000014, mae: 2.917546, mean_q: 4.367167\n",
            " 90992/100000: episode: 9171, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000027, mae: 2.939906, mean_q: 4.392581\n",
            " 91002/100000: episode: 9172, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000043, mae: 3.047247, mean_q: 4.508513\n",
            " 91011/100000: episode: 9173, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000055, mae: 2.967739, mean_q: 4.418116\n",
            " 91021/100000: episode: 9174, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000047, mae: 2.841644, mean_q: 4.267828\n",
            " 91029/100000: episode: 9175, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000115, mae: 2.822911, mean_q: 4.249043\n",
            " 91039/100000: episode: 9176, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000086, mae: 2.819503, mean_q: 4.249219\n",
            " 91048/100000: episode: 9177, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000051, mae: 2.953954, mean_q: 4.397749\n",
            " 91056/100000: episode: 9178, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.870781, mean_q: 4.308939\n",
            " 91066/100000: episode: 9179, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000114, mae: 3.018687, mean_q: 4.476438\n",
            " 91074/100000: episode: 9180, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000155, mae: 2.929920, mean_q: 4.377060\n",
            " 91083/100000: episode: 9181, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000058, mae: 3.054451, mean_q: 4.517795\n",
            " 91091/100000: episode: 9182, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000109, mae: 2.971935, mean_q: 4.422274\n",
            " 91099/100000: episode: 9183, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000108, mae: 2.856421, mean_q: 4.292717\n",
            " 91107/100000: episode: 9184, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000135, mae: 2.855425, mean_q: 4.286724\n",
            " 91117/100000: episode: 9185, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000067, mae: 2.987964, mean_q: 4.432612\n",
            " 91126/100000: episode: 9186, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000077, mae: 3.053366, mean_q: 4.515558\n",
            " 91136/100000: episode: 9187, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 3.059791, mean_q: 4.522804\n",
            " 91144/100000: episode: 9188, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.902583, mean_q: 4.342206\n",
            " 91153/100000: episode: 9189, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000025, mae: 2.784354, mean_q: 4.203650\n",
            " 91165/100000: episode: 9190, duration: 0.129s, episode steps:  12, steps per second:  93, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.583 [0.000, 7.000],  loss: 0.000080, mae: 3.047111, mean_q: 4.508616\n",
            " 91175/100000: episode: 9191, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000052, mae: 2.745524, mean_q: 4.162297\n",
            " 91185/100000: episode: 9192, duration: 0.112s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000086, mae: 2.856617, mean_q: 4.279359\n",
            " 91194/100000: episode: 9193, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000072, mae: 3.022574, mean_q: 4.483093\n",
            " 91202/100000: episode: 9194, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 2.923524, mean_q: 4.366753\n",
            " 91210/100000: episode: 9195, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000067, mae: 3.003856, mean_q: 4.456232\n",
            " 91220/100000: episode: 9196, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000049, mae: 2.983449, mean_q: 4.434751\n",
            " 91228/100000: episode: 9197, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 3.101800, mean_q: 4.569620\n",
            " 91236/100000: episode: 9198, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.885153, mean_q: 4.325042\n",
            " 91244/100000: episode: 9199, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.738166, mean_q: 4.153240\n",
            " 91253/100000: episode: 9200, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000057, mae: 2.963202, mean_q: 4.418772\n",
            " 91262/100000: episode: 9201, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000099, mae: 2.987304, mean_q: 4.445036\n",
            " 91270/100000: episode: 9202, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 2.908573, mean_q: 4.348930\n",
            " 91282/100000: episode: 9203, duration: 0.132s, episode steps:  12, steps per second:  91, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000226, mae: 2.990797, mean_q: 4.444075\n",
            " 91291/100000: episode: 9204, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000418, mae: 3.136223, mean_q: 4.608432\n",
            " 91300/100000: episode: 9205, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000257, mae: 2.843609, mean_q: 4.248424\n",
            " 91308/100000: episode: 9206, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000783, mae: 2.853449, mean_q: 4.268848\n",
            " 91316/100000: episode: 9207, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.001531, mae: 2.816896, mean_q: 4.232866\n",
            " 91324/100000: episode: 9208, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000916, mae: 3.048177, mean_q: 4.453940\n",
            " 91332/100000: episode: 9209, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.001065, mae: 3.009668, mean_q: 4.415889\n",
            " 91341/100000: episode: 9210, duration: 0.081s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.001048, mae: 2.981863, mean_q: 4.403659\n",
            " 91350/100000: episode: 9211, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000941, mae: 2.930942, mean_q: 4.336146\n",
            " 91358/100000: episode: 9212, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.001245, mae: 2.846926, mean_q: 4.227854\n",
            " 91366/100000: episode: 9213, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000305, mae: 3.084548, mean_q: 4.538736\n",
            " 91374/100000: episode: 9214, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000564, mae: 2.771261, mean_q: 4.177606\n",
            " 91383/100000: episode: 9215, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000217, mae: 2.840275, mean_q: 4.271835\n",
            " 91391/100000: episode: 9216, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000096, mae: 2.788082, mean_q: 4.215906\n",
            " 91401/100000: episode: 9217, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000055, mae: 2.941363, mean_q: 4.387941\n",
            " 91409/100000: episode: 9218, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.843328, mean_q: 4.274544\n",
            " 91419/100000: episode: 9219, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000031, mae: 2.927313, mean_q: 4.366155\n",
            " 91428/100000: episode: 9220, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000027, mae: 3.024622, mean_q: 4.480689\n",
            " 91436/100000: episode: 9221, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.953032, mean_q: 4.410216\n",
            " 91445/100000: episode: 9222, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000015, mae: 2.918781, mean_q: 4.368903\n",
            " 91453/100000: episode: 9223, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.037254, mean_q: 4.496538\n",
            " 91462/100000: episode: 9224, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000011, mae: 3.043843, mean_q: 4.503419\n",
            " 91470/100000: episode: 9225, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.954180, mean_q: 4.400021\n",
            " 91478/100000: episode: 9226, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.096735, mean_q: 4.564236\n",
            " 91487/100000: episode: 9227, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000011, mae: 2.933295, mean_q: 4.380053\n",
            " 91496/100000: episode: 9228, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000010, mae: 2.996080, mean_q: 4.459642\n",
            " 91504/100000: episode: 9229, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.937115, mean_q: 4.390493\n",
            " 91513/100000: episode: 9230, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000005, mae: 2.874668, mean_q: 4.321054\n",
            " 91521/100000: episode: 9231, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.936613, mean_q: 4.384395\n",
            " 91529/100000: episode: 9232, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.986858, mean_q: 4.441600\n",
            " 91537/100000: episode: 9233, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.960919, mean_q: 4.407814\n",
            " 91547/100000: episode: 9234, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.986773, mean_q: 4.442123\n",
            " 91557/100000: episode: 9235, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 3.101139, mean_q: 4.583644\n",
            " 91566/100000: episode: 9236, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000003, mae: 2.855670, mean_q: 4.296988\n",
            " 91576/100000: episode: 9237, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000006, mae: 2.862296, mean_q: 4.295229\n",
            " 91585/100000: episode: 9238, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000006, mae: 2.961242, mean_q: 4.410975\n",
            " 91594/100000: episode: 9239, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000005, mae: 3.102506, mean_q: 4.581614\n",
            " 91603/100000: episode: 9240, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000007, mae: 2.599688, mean_q: 3.989707\n",
            " 91613/100000: episode: 9241, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000006, mae: 2.837731, mean_q: 4.268136\n",
            " 91622/100000: episode: 9242, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 3.214240, mean_q: 4.701710\n",
            " 91630/100000: episode: 9243, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.996102, mean_q: 4.451297\n",
            " 91638/100000: episode: 9244, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.880494, mean_q: 4.317228\n",
            " 91647/100000: episode: 9245, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.960534, mean_q: 4.412367\n",
            " 91655/100000: episode: 9246, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.016427, mean_q: 4.480151\n",
            " 91665/100000: episode: 9247, duration: 0.121s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000002, mae: 2.816013, mean_q: 4.244628\n",
            " 91674/100000: episode: 9248, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000003, mae: 2.833211, mean_q: 4.269379\n",
            " 91682/100000: episode: 9249, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.755915, mean_q: 4.176883\n",
            " 91691/100000: episode: 9250, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.812110, mean_q: 4.241745\n",
            " 91699/100000: episode: 9251, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.930943, mean_q: 4.381060\n",
            " 91708/100000: episode: 9252, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.920679, mean_q: 4.364107\n",
            " 91716/100000: episode: 9253, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.878790, mean_q: 4.318883\n",
            " 91724/100000: episode: 9254, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.217847, mean_q: 4.706706\n",
            " 91733/100000: episode: 9255, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.184930, mean_q: 4.673306\n",
            " 91742/100000: episode: 9256, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 2.893758, mean_q: 4.333547\n",
            " 91750/100000: episode: 9257, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.003439, mean_q: 4.461321\n",
            " 91758/100000: episode: 9258, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.863546, mean_q: 4.301205\n",
            " 91767/100000: episode: 9259, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000001, mae: 3.125270, mean_q: 4.601236\n",
            " 91776/100000: episode: 9260, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 2.948257, mean_q: 4.402491\n",
            " 91785/100000: episode: 9261, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000001, mae: 2.835799, mean_q: 4.267702\n",
            " 91793/100000: episode: 9262, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.916564, mean_q: 4.361687\n",
            " 91803/100000: episode: 9263, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000002, mae: 2.768451, mean_q: 4.187472\n",
            " 91811/100000: episode: 9264, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.013424, mean_q: 4.471827\n",
            " 91820/100000: episode: 9265, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.932361, mean_q: 4.376722\n",
            " 91828/100000: episode: 9266, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.934383, mean_q: 4.384052\n",
            " 91836/100000: episode: 9267, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.055872, mean_q: 4.522446\n",
            " 91847/100000: episode: 9268, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000001, mae: 3.192157, mean_q: 4.682293\n",
            " 91855/100000: episode: 9269, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.720714, mean_q: 4.134412\n",
            " 91864/100000: episode: 9270, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000001, mae: 2.843272, mean_q: 4.277221\n",
            " 91874/100000: episode: 9271, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000001, mae: 2.872943, mean_q: 4.310016\n",
            " 91882/100000: episode: 9272, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.975763, mean_q: 4.430462\n",
            " 91890/100000: episode: 9273, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.009373, mean_q: 4.469469\n",
            " 91898/100000: episode: 9274, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.910225, mean_q: 4.354649\n",
            " 91906/100000: episode: 9275, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.026853, mean_q: 4.484680\n",
            " 91916/100000: episode: 9276, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000001, mae: 3.168603, mean_q: 4.658563\n",
            " 91924/100000: episode: 9277, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.006825, mean_q: 4.468832\n",
            " 91933/100000: episode: 9278, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.921983, mean_q: 4.363666\n",
            " 91941/100000: episode: 9279, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.238956, mean_q: 4.731009\n",
            " 91951/100000: episode: 9280, duration: 0.090s, episode steps:  10, steps per second: 112, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000001, mae: 2.808993, mean_q: 4.239360\n",
            " 91959/100000: episode: 9281, duration: 0.120s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.900898, mean_q: 4.344254\n",
            " 91968/100000: episode: 9282, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.104494, mean_q: 4.578858\n",
            " 91977/100000: episode: 9283, duration: 0.131s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000001, mae: 3.142582, mean_q: 4.622005\n",
            " 91985/100000: episode: 9284, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.093996, mean_q: 4.567959\n",
            " 91994/100000: episode: 9285, duration: 0.155s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000001, mae: 3.076020, mean_q: 4.546003\n",
            " 92003/100000: episode: 9286, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 2.876878, mean_q: 4.316565\n",
            " 92011/100000: episode: 9287, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.631663, mean_q: 4.031963\n",
            " 92020/100000: episode: 9288, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000000, mae: 2.928343, mean_q: 4.375921\n",
            " 92030/100000: episode: 9289, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.882627, mean_q: 4.323015\n",
            " 92038/100000: episode: 9290, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.122965, mean_q: 4.599022\n",
            " 92046/100000: episode: 9291, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 2.990545, mean_q: 4.446143\n",
            " 92057/100000: episode: 9292, duration: 0.156s, episode steps:  11, steps per second:  70, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000000, mae: 3.000532, mean_q: 4.463170\n",
            " 92067/100000: episode: 9293, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000000, mae: 2.885482, mean_q: 4.331004\n",
            " 92076/100000: episode: 9294, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000000, mae: 2.979111, mean_q: 4.436168\n",
            " 92084/100000: episode: 9295, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.100758, mean_q: 4.579354\n",
            " 92094/100000: episode: 9296, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 2.982978, mean_q: 4.437444\n",
            " 92103/100000: episode: 9297, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 3.037019, mean_q: 4.498338\n",
            " 92115/100000: episode: 9298, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000001, mae: 3.085991, mean_q: 4.556049\n",
            " 92125/100000: episode: 9299, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000001, mae: 3.025410, mean_q: 4.487648\n",
            " 92135/100000: episode: 9300, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000001, mae: 2.999352, mean_q: 4.454528\n",
            " 92143/100000: episode: 9301, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000000, mae: 3.042780, mean_q: 4.510142\n",
            " 92151/100000: episode: 9302, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.159015, mean_q: 4.640344\n",
            " 92163/100000: episode: 9303, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 4.083 [0.000, 7.000],  loss: 0.000001, mae: 3.003032, mean_q: 4.462954\n",
            " 92172/100000: episode: 9304, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.158372, mean_q: 4.645669\n",
            " 92180/100000: episode: 9305, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.863867, mean_q: 4.299825\n",
            " 92189/100000: episode: 9306, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 3.139019, mean_q: 4.621068\n",
            " 92198/100000: episode: 9307, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000003, mae: 2.818412, mean_q: 4.250365\n",
            " 92207/100000: episode: 9308, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000007, mae: 2.800331, mean_q: 4.222632\n",
            " 92215/100000: episode: 9309, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.898604, mean_q: 4.332522\n",
            " 92224/100000: episode: 9310, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 2.816874, mean_q: 4.247019\n",
            " 92235/100000: episode: 9311, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000007, mae: 3.031410, mean_q: 4.490732\n",
            " 92243/100000: episode: 9312, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.180712, mean_q: 4.670706\n",
            " 92253/100000: episode: 9313, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000013, mae: 2.888169, mean_q: 4.329377\n",
            " 92261/100000: episode: 9314, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.671033, mean_q: 4.076848\n",
            " 92270/100000: episode: 9315, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000004, mae: 2.931809, mean_q: 4.383295\n",
            " 92278/100000: episode: 9316, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.716564, mean_q: 4.133023\n",
            " 92286/100000: episode: 9317, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.741928, mean_q: 4.157912\n",
            " 92295/100000: episode: 9318, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 2.989941, mean_q: 4.448414\n",
            " 92304/100000: episode: 9319, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.806791, mean_q: 4.239390\n",
            " 92312/100000: episode: 9320, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.969150, mean_q: 4.425639\n",
            " 92321/100000: episode: 9321, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.891355, mean_q: 4.333630\n",
            " 92329/100000: episode: 9322, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.091184, mean_q: 4.563374\n",
            " 92338/100000: episode: 9323, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000004, mae: 2.946898, mean_q: 4.399622\n",
            " 92346/100000: episode: 9324, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.943663, mean_q: 4.387670\n",
            " 92355/100000: episode: 9325, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 2.876725, mean_q: 4.316078\n",
            " 92363/100000: episode: 9326, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.071784, mean_q: 4.534952\n",
            " 92374/100000: episode: 9327, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000023, mae: 3.043886, mean_q: 4.509472\n",
            " 92383/100000: episode: 9328, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000058, mae: 2.959915, mean_q: 4.407382\n",
            " 92393/100000: episode: 9329, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000037, mae: 3.014764, mean_q: 4.468501\n",
            " 92401/100000: episode: 9330, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.924876, mean_q: 4.362691\n",
            " 92411/100000: episode: 9331, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000053, mae: 2.913095, mean_q: 4.353709\n",
            " 92419/100000: episode: 9332, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.962442, mean_q: 4.408223\n",
            " 92427/100000: episode: 9333, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000076, mae: 2.829257, mean_q: 4.253821\n",
            " 92435/100000: episode: 9334, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 3.007100, mean_q: 4.461407\n",
            " 92443/100000: episode: 9335, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.950862, mean_q: 4.396337\n",
            " 92451/100000: episode: 9336, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.889965, mean_q: 4.333068\n",
            " 92459/100000: episode: 9337, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.965632, mean_q: 4.421582\n",
            " 92468/100000: episode: 9338, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000033, mae: 2.981666, mean_q: 4.433337\n",
            " 92476/100000: episode: 9339, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 2.998757, mean_q: 4.451496\n",
            " 92486/100000: episode: 9340, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000073, mae: 2.810884, mean_q: 4.236246\n",
            " 92494/100000: episode: 9341, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.835899, mean_q: 4.266598\n",
            " 92506/100000: episode: 9342, duration: 0.144s, episode steps:  12, steps per second:  83, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000029, mae: 2.886745, mean_q: 4.320996\n",
            " 92516/100000: episode: 9343, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000033, mae: 2.823398, mean_q: 4.248409\n",
            " 92524/100000: episode: 9344, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.008965, mean_q: 4.471556\n",
            " 92534/100000: episode: 9345, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000009, mae: 2.610248, mean_q: 4.011290\n",
            " 92543/100000: episode: 9346, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 2.931778, mean_q: 4.379389\n",
            " 92551/100000: episode: 9347, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.998347, mean_q: 4.458346\n",
            " 92559/100000: episode: 9348, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.699645, mean_q: 4.111260\n",
            " 92567/100000: episode: 9349, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.944179, mean_q: 4.392953\n",
            " 92576/100000: episode: 9350, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000006, mae: 3.069525, mean_q: 4.536333\n",
            " 92584/100000: episode: 9351, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.835104, mean_q: 4.271622\n",
            " 92592/100000: episode: 9352, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.123538, mean_q: 4.598789\n",
            " 92602/100000: episode: 9353, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000016, mae: 2.905267, mean_q: 4.344159\n",
            " 92611/100000: episode: 9354, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000010, mae: 3.120469, mean_q: 4.600402\n",
            " 92619/100000: episode: 9355, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.032079, mean_q: 4.493686\n",
            " 92630/100000: episode: 9356, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000006, mae: 3.091268, mean_q: 4.561750\n",
            " 92639/100000: episode: 9357, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 2.877897, mean_q: 4.316091\n",
            " 92649/100000: episode: 9358, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000011, mae: 2.946654, mean_q: 4.393573\n",
            " 92659/100000: episode: 9359, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000007, mae: 2.887319, mean_q: 4.325777\n",
            " 92668/100000: episode: 9360, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.898609, mean_q: 4.342756\n",
            " 92677/100000: episode: 9361, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000023, mae: 3.095569, mean_q: 4.566146\n",
            " 92687/100000: episode: 9362, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000026, mae: 2.886908, mean_q: 4.325532\n",
            " 92695/100000: episode: 9363, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 2.926278, mean_q: 4.366336\n",
            " 92705/100000: episode: 9364, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000026, mae: 2.942008, mean_q: 4.391643\n",
            " 92713/100000: episode: 9365, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.890758, mean_q: 4.334754\n",
            " 92722/100000: episode: 9366, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000044, mae: 2.919838, mean_q: 4.362287\n",
            " 92730/100000: episode: 9367, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.696900, mean_q: 4.104699\n",
            " 92739/100000: episode: 9368, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000029, mae: 3.102435, mean_q: 4.575686\n",
            " 92747/100000: episode: 9369, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.972580, mean_q: 4.422972\n",
            " 92756/100000: episode: 9370, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000050, mae: 2.944412, mean_q: 4.388746\n",
            " 92764/100000: episode: 9371, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000083, mae: 3.128136, mean_q: 4.599998\n",
            " 92773/100000: episode: 9372, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000144, mae: 2.845769, mean_q: 4.273426\n",
            " 92781/100000: episode: 9373, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000219, mae: 2.520711, mean_q: 3.890534\n",
            " 92790/100000: episode: 9374, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000214, mae: 2.811504, mean_q: 4.237608\n",
            " 92798/100000: episode: 9375, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000166, mae: 2.971782, mean_q: 4.401123\n",
            " 92807/100000: episode: 9376, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000162, mae: 2.933782, mean_q: 4.358084\n",
            " 92815/100000: episode: 9377, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000213, mae: 2.936488, mean_q: 4.377030\n",
            " 92824/100000: episode: 9378, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000168, mae: 2.821789, mean_q: 4.238186\n",
            " 92832/100000: episode: 9379, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.901211, mean_q: 4.326027\n",
            " 92840/100000: episode: 9380, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 3.030121, mean_q: 4.479875\n",
            " 92849/100000: episode: 9381, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000023, mae: 2.996430, mean_q: 4.454066\n",
            " 92860/100000: episode: 9382, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000047, mae: 2.999547, mean_q: 4.450168\n",
            " 92868/100000: episode: 9383, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.902633, mean_q: 4.345715\n",
            " 92876/100000: episode: 9384, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.954363, mean_q: 4.398510\n",
            " 92884/100000: episode: 9385, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.070504, mean_q: 4.532788\n",
            " 92893/100000: episode: 9386, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000029, mae: 2.934156, mean_q: 4.375707\n",
            " 92904/100000: episode: 9387, duration: 0.113s, episode steps:  11, steps per second:  97, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000027, mae: 3.006481, mean_q: 4.467527\n",
            " 92913/100000: episode: 9388, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000056, mae: 2.948408, mean_q: 4.401856\n",
            " 92924/100000: episode: 9389, duration: 0.123s, episode steps:  11, steps per second:  90, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.091 [0.000, 7.000],  loss: 0.000102, mae: 3.023741, mean_q: 4.474928\n",
            " 92933/100000: episode: 9390, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000146, mae: 3.103010, mean_q: 4.568233\n",
            " 92941/100000: episode: 9391, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000144, mae: 2.835113, mean_q: 4.273268\n",
            " 92949/100000: episode: 9392, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000191, mae: 2.830052, mean_q: 4.259133\n",
            " 92957/100000: episode: 9393, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000182, mae: 3.062124, mean_q: 4.508819\n",
            " 92965/100000: episode: 9394, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 2.951581, mean_q: 4.386953\n",
            " 92973/100000: episode: 9395, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000176, mae: 2.955618, mean_q: 4.411527\n",
            " 92982/100000: episode: 9396, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000293, mae: 2.930940, mean_q: 4.371438\n",
            " 92990/100000: episode: 9397, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000267, mae: 2.919645, mean_q: 4.341435\n",
            " 92999/100000: episode: 9398, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000139, mae: 2.987018, mean_q: 4.440237\n",
            " 93008/100000: episode: 9399, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000161, mae: 2.955289, mean_q: 4.406890\n",
            " 93016/100000: episode: 9400, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000161, mae: 2.825780, mean_q: 4.235029\n",
            " 93025/100000: episode: 9401, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000253, mae: 3.183122, mean_q: 4.652688\n",
            " 93033/100000: episode: 9402, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000359, mae: 2.993623, mean_q: 4.424058\n",
            " 93042/100000: episode: 9403, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000167, mae: 2.918172, mean_q: 4.346189\n",
            " 93050/100000: episode: 9404, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000166, mae: 3.217918, mean_q: 4.691908\n",
            " 93062/100000: episode: 9405, duration: 0.136s, episode steps:  12, steps per second:  88, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000151, mae: 3.012502, mean_q: 4.463159\n",
            " 93071/100000: episode: 9406, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000092, mae: 2.979676, mean_q: 4.423360\n",
            " 93079/100000: episode: 9407, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000124, mae: 2.748923, mean_q: 4.174871\n",
            " 93087/100000: episode: 9408, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000067, mae: 2.789513, mean_q: 4.217877\n",
            " 93095/100000: episode: 9409, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.842105, mean_q: 4.272939\n",
            " 93103/100000: episode: 9410, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.990080, mean_q: 4.438339\n",
            " 93111/100000: episode: 9411, duration: 0.132s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.893197, mean_q: 4.334178\n",
            " 93121/100000: episode: 9412, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000012, mae: 3.011651, mean_q: 4.471066\n",
            " 93130/100000: episode: 9413, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000010, mae: 2.909535, mean_q: 4.354919\n",
            " 93138/100000: episode: 9414, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.747473, mean_q: 4.163885\n",
            " 93147/100000: episode: 9415, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000017, mae: 2.973680, mean_q: 4.423509\n",
            " 93156/100000: episode: 9416, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000012, mae: 2.759521, mean_q: 4.177016\n",
            " 93164/100000: episode: 9417, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.864065, mean_q: 4.301138\n",
            " 93173/100000: episode: 9418, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000008, mae: 2.966216, mean_q: 4.419907\n",
            " 93182/100000: episode: 9419, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000008, mae: 3.100331, mean_q: 4.577964\n",
            " 93190/100000: episode: 9420, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.758323, mean_q: 4.178251\n",
            " 93200/100000: episode: 9421, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000006, mae: 2.943162, mean_q: 4.385959\n",
            " 93209/100000: episode: 9422, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 2.854313, mean_q: 4.288265\n",
            " 93217/100000: episode: 9423, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.847898, mean_q: 4.284504\n",
            " 93226/100000: episode: 9424, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000012, mae: 3.062765, mean_q: 4.528408\n",
            " 93234/100000: episode: 9425, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.031944, mean_q: 4.489524\n",
            " 93245/100000: episode: 9426, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000020, mae: 2.841291, mean_q: 4.265836\n",
            " 93257/100000: episode: 9427, duration: 0.178s, episode steps:  12, steps per second:  67, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.167 [0.000, 7.000],  loss: 0.000007, mae: 2.933975, mean_q: 4.384249\n",
            " 93265/100000: episode: 9428, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.129558, mean_q: 4.613638\n",
            " 93273/100000: episode: 9429, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.977543, mean_q: 4.435982\n",
            " 93281/100000: episode: 9430, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.074489, mean_q: 4.547343\n",
            " 93289/100000: episode: 9431, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.843511, mean_q: 4.280093\n",
            " 93298/100000: episode: 9432, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000015, mae: 3.154780, mean_q: 4.633004\n",
            " 93306/100000: episode: 9433, duration: 0.152s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.681880, mean_q: 4.090745\n",
            " 93316/100000: episode: 9434, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000019, mae: 2.927526, mean_q: 4.378208\n",
            " 93324/100000: episode: 9435, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.045624, mean_q: 4.509547\n",
            " 93334/100000: episode: 9436, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000026, mae: 2.878669, mean_q: 4.316694\n",
            " 93342/100000: episode: 9437, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.946337, mean_q: 4.396620\n",
            " 93350/100000: episode: 9438, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.726248, mean_q: 4.138922\n",
            " 93360/100000: episode: 9439, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000015, mae: 2.892978, mean_q: 4.328855\n",
            " 93369/100000: episode: 9440, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000010, mae: 3.148620, mean_q: 4.632211\n",
            " 93377/100000: episode: 9441, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.897080, mean_q: 4.338771\n",
            " 93385/100000: episode: 9442, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.850787, mean_q: 4.285316\n",
            " 93393/100000: episode: 9443, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.848083, mean_q: 4.284294\n",
            " 93404/100000: episode: 9444, duration: 0.122s, episode steps:  11, steps per second:  90, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000549, mae: 2.966377, mean_q: 4.398663\n",
            " 93412/100000: episode: 9445, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000269, mae: 2.896855, mean_q: 4.341482\n",
            " 93420/100000: episode: 9446, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000112, mae: 2.872175, mean_q: 4.304559\n",
            " 93428/100000: episode: 9447, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000123, mae: 3.090271, mean_q: 4.533349\n",
            " 93437/100000: episode: 9448, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000326, mae: 2.876967, mean_q: 4.299139\n",
            " 93446/100000: episode: 9449, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000358, mae: 2.994566, mean_q: 4.451760\n",
            " 93455/100000: episode: 9450, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000266, mae: 3.035898, mean_q: 4.479145\n",
            " 93465/100000: episode: 9451, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000168, mae: 3.108220, mean_q: 4.574327\n",
            " 93473/100000: episode: 9452, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000175, mae: 2.900074, mean_q: 4.336166\n",
            " 93482/100000: episode: 9453, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000172, mae: 2.860561, mean_q: 4.271769\n",
            " 93491/100000: episode: 9454, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000112, mae: 2.893099, mean_q: 4.325768\n",
            " 93500/100000: episode: 9455, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000143, mae: 2.968731, mean_q: 4.421313\n",
            " 93508/100000: episode: 9456, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000121, mae: 2.875710, mean_q: 4.302996\n",
            " 93518/100000: episode: 9457, duration: 0.113s, episode steps:  10, steps per second:  88, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000112, mae: 2.912362, mean_q: 4.347589\n",
            " 93526/100000: episode: 9458, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 3.027802, mean_q: 4.495891\n",
            " 93534/100000: episode: 9459, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000053, mae: 2.904298, mean_q: 4.342704\n",
            " 93543/100000: episode: 9460, duration: 0.131s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000046, mae: 2.850145, mean_q: 4.274465\n",
            " 93552/100000: episode: 9461, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000034, mae: 2.824696, mean_q: 4.250313\n",
            " 93561/100000: episode: 9462, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000015, mae: 3.017253, mean_q: 4.472151\n",
            " 93569/100000: episode: 9463, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.660747, mean_q: 4.060712\n",
            " 93577/100000: episode: 9464, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.845263, mean_q: 4.276579\n",
            " 93585/100000: episode: 9465, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.783299, mean_q: 4.200442\n",
            " 93593/100000: episode: 9466, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.906567, mean_q: 4.343571\n",
            " 93601/100000: episode: 9467, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.920657, mean_q: 4.357434\n",
            " 93609/100000: episode: 9468, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 3.058091, mean_q: 4.527763\n",
            " 93619/100000: episode: 9469, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000021, mae: 3.054983, mean_q: 4.519375\n",
            " 93629/100000: episode: 9470, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000015, mae: 3.064480, mean_q: 4.537762\n",
            " 93639/100000: episode: 9471, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000011, mae: 3.064803, mean_q: 4.533891\n",
            " 93648/100000: episode: 9472, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.877202, mean_q: 4.311862\n",
            " 93657/100000: episode: 9473, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000013, mae: 2.988724, mean_q: 4.446676\n",
            " 93666/100000: episode: 9474, duration: 0.131s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000010, mae: 3.018771, mean_q: 4.480207\n",
            " 93675/100000: episode: 9475, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000017, mae: 3.037669, mean_q: 4.505333\n",
            " 93683/100000: episode: 9476, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.806638, mean_q: 4.236593\n",
            " 93691/100000: episode: 9477, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.886364, mean_q: 4.325003\n",
            " 93699/100000: episode: 9478, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.961632, mean_q: 4.405063\n",
            " 93708/100000: episode: 9479, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000012, mae: 3.124019, mean_q: 4.597904\n",
            " 93717/100000: episode: 9480, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000008, mae: 3.054492, mean_q: 4.518831\n",
            " 93727/100000: episode: 9481, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000005, mae: 2.867952, mean_q: 4.303114\n",
            " 93735/100000: episode: 9482, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.846494, mean_q: 4.280344\n",
            " 93744/100000: episode: 9483, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000005, mae: 2.955141, mean_q: 4.407669\n",
            " 93753/100000: episode: 9484, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000005, mae: 3.020416, mean_q: 4.479371\n",
            " 93762/100000: episode: 9485, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 2.927999, mean_q: 4.372584\n",
            " 93771/100000: episode: 9486, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 2.993375, mean_q: 4.449301\n",
            " 93779/100000: episode: 9487, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.861079, mean_q: 4.298205\n",
            " 93787/100000: episode: 9488, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.862697, mean_q: 4.295174\n",
            " 93795/100000: episode: 9489, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.222295, mean_q: 4.714956\n",
            " 93804/100000: episode: 9490, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000011, mae: 2.969727, mean_q: 4.421764\n",
            " 93812/100000: episode: 9491, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.093730, mean_q: 4.566642\n",
            " 93821/100000: episode: 9492, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000014, mae: 2.901930, mean_q: 4.344374\n",
            " 93829/100000: episode: 9493, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.975813, mean_q: 4.430025\n",
            " 93837/100000: episode: 9494, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.078570, mean_q: 4.545448\n",
            " 93846/100000: episode: 9495, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000008, mae: 2.804949, mean_q: 4.226649\n",
            " 93854/100000: episode: 9496, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.768793, mean_q: 4.192398\n",
            " 93863/100000: episode: 9497, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000007, mae: 3.067854, mean_q: 4.534065\n",
            " 93872/100000: episode: 9498, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000009, mae: 2.993808, mean_q: 4.448496\n",
            " 93880/100000: episode: 9499, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 3.005805, mean_q: 4.466074\n",
            " 93888/100000: episode: 9500, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.834257, mean_q: 4.265681\n",
            " 93898/100000: episode: 9501, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000022, mae: 2.878758, mean_q: 4.313645\n",
            " 93907/100000: episode: 9502, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000017, mae: 3.049369, mean_q: 4.514944\n",
            " 93915/100000: episode: 9503, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.171335, mean_q: 4.659124\n",
            " 93924/100000: episode: 9504, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000022, mae: 2.922051, mean_q: 4.364985\n",
            " 93933/100000: episode: 9505, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000030, mae: 2.876564, mean_q: 4.308798\n",
            " 93941/100000: episode: 9506, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 2.610624, mean_q: 4.000373\n",
            " 93950/100000: episode: 9507, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000032, mae: 3.087961, mean_q: 4.558119\n",
            " 93960/100000: episode: 9508, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000056, mae: 2.924675, mean_q: 4.366768\n",
            " 93968/100000: episode: 9509, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000098, mae: 2.876385, mean_q: 4.314973\n",
            " 93978/100000: episode: 9510, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000045, mae: 3.020295, mean_q: 4.471673\n",
            " 93986/100000: episode: 9511, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000085, mae: 2.899910, mean_q: 4.338783\n",
            " 93994/100000: episode: 9512, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 3.031158, mean_q: 4.491129\n",
            " 94002/100000: episode: 9513, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000089, mae: 2.764014, mean_q: 4.184718\n",
            " 94012/100000: episode: 9514, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000097, mae: 2.946711, mean_q: 4.390401\n",
            " 94021/100000: episode: 9515, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000031, mae: 3.063173, mean_q: 4.531702\n",
            " 94032/100000: episode: 9516, duration: 0.099s, episode steps:  11, steps per second: 111, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000022, mae: 2.747754, mean_q: 4.166428\n",
            " 94041/100000: episode: 9517, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000017, mae: 2.808078, mean_q: 4.236236\n",
            " 94049/100000: episode: 9518, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.094522, mean_q: 4.569145\n",
            " 94058/100000: episode: 9519, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000019, mae: 3.123281, mean_q: 4.597647\n",
            " 94066/100000: episode: 9520, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.106884, mean_q: 4.580030\n",
            " 94075/100000: episode: 9521, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000012, mae: 3.029014, mean_q: 4.491924\n",
            " 94084/100000: episode: 9522, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000010, mae: 3.187129, mean_q: 4.670099\n",
            " 94092/100000: episode: 9523, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.950063, mean_q: 4.399429\n",
            " 94100/100000: episode: 9524, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.057607, mean_q: 4.524206\n",
            " 94108/100000: episode: 9525, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.992523, mean_q: 4.449339\n",
            " 94117/100000: episode: 9526, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 2.855290, mean_q: 4.287527\n",
            " 94125/100000: episode: 9527, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.837226, mean_q: 4.270999\n",
            " 94134/100000: episode: 9528, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000018, mae: 2.880296, mean_q: 4.318100\n",
            " 94144/100000: episode: 9529, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000006, mae: 3.101886, mean_q: 4.573163\n",
            " 94152/100000: episode: 9530, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.770818, mean_q: 4.191050\n",
            " 94160/100000: episode: 9531, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.704296, mean_q: 4.120152\n",
            " 94170/100000: episode: 9532, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000022, mae: 3.093152, mean_q: 4.568079\n",
            " 94179/100000: episode: 9533, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000031, mae: 2.970249, mean_q: 4.416704\n",
            " 94187/100000: episode: 9534, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.863253, mean_q: 4.296059\n",
            " 94195/100000: episode: 9535, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000090, mae: 2.994499, mean_q: 4.448536\n",
            " 94204/100000: episode: 9536, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000042, mae: 2.984889, mean_q: 4.444607\n",
            " 94214/100000: episode: 9537, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000036, mae: 3.017222, mean_q: 4.472776\n",
            " 94222/100000: episode: 9538, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000102, mae: 2.924381, mean_q: 4.359156\n",
            " 94233/100000: episode: 9539, duration: 0.181s, episode steps:  11, steps per second:  61, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000126, mae: 2.908850, mean_q: 4.348980\n",
            " 94243/100000: episode: 9540, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.728367, mean_q: 4.134151\n",
            " 94252/100000: episode: 9541, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000125, mae: 3.117502, mean_q: 4.594217\n",
            " 94261/100000: episode: 9542, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000065, mae: 3.122055, mean_q: 4.596687\n",
            " 94270/100000: episode: 9543, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000048, mae: 2.969140, mean_q: 4.418287\n",
            " 94280/100000: episode: 9544, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000067, mae: 3.037297, mean_q: 4.496889\n",
            " 94290/100000: episode: 9545, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000039, mae: 2.990601, mean_q: 4.441847\n",
            " 94299/100000: episode: 9546, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000070, mae: 2.994923, mean_q: 4.450103\n",
            " 94308/100000: episode: 9547, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000052, mae: 2.820030, mean_q: 4.245193\n",
            " 94316/100000: episode: 9548, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.827651, mean_q: 4.254414\n",
            " 94329/100000: episode: 9549, duration: 0.220s, episode steps:  13, steps per second:  59, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 4.077 [0.000, 7.000],  loss: 0.000052, mae: 2.929577, mean_q: 4.373271\n",
            " 94341/100000: episode: 9550, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000045, mae: 3.059010, mean_q: 4.515735\n",
            " 94350/100000: episode: 9551, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000053, mae: 2.861776, mean_q: 4.293021\n",
            " 94359/100000: episode: 9552, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000052, mae: 3.131232, mean_q: 4.598676\n",
            " 94368/100000: episode: 9553, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000024, mae: 2.914596, mean_q: 4.354372\n",
            " 94378/100000: episode: 9554, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000039, mae: 2.935734, mean_q: 4.372381\n",
            " 94387/100000: episode: 9555, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000034, mae: 3.133982, mean_q: 4.606565\n",
            " 94396/100000: episode: 9556, duration: 0.155s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000050, mae: 2.980708, mean_q: 4.433238\n",
            " 94405/100000: episode: 9557, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000032, mae: 3.077722, mean_q: 4.538126\n",
            " 94413/100000: episode: 9558, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.097421, mean_q: 4.563927\n",
            " 94422/100000: episode: 9559, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000044, mae: 2.905418, mean_q: 4.341835\n",
            " 94430/100000: episode: 9560, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000054, mae: 2.866131, mean_q: 4.305011\n",
            " 94438/100000: episode: 9561, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000081, mae: 2.813013, mean_q: 4.241776\n",
            " 94447/100000: episode: 9562, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000071, mae: 2.985054, mean_q: 4.424775\n",
            " 94455/100000: episode: 9563, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000119, mae: 2.962849, mean_q: 4.418736\n",
            " 94466/100000: episode: 9564, duration: 0.106s, episode steps:  11, steps per second: 104, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000327, mae: 2.945151, mean_q: 4.379504\n",
            " 94475/100000: episode: 9565, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000140, mae: 3.063286, mean_q: 4.520468\n",
            " 94483/100000: episode: 9566, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000307, mae: 2.922822, mean_q: 4.364114\n",
            " 94492/100000: episode: 9567, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000274, mae: 3.042528, mean_q: 4.499819\n",
            " 94501/100000: episode: 9568, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000206, mae: 3.078826, mean_q: 4.527384\n",
            " 94509/100000: episode: 9569, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000206, mae: 3.028170, mean_q: 4.483325\n",
            " 94518/100000: episode: 9570, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000234, mae: 3.061534, mean_q: 4.522639\n",
            " 94526/100000: episode: 9571, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000397, mae: 2.852815, mean_q: 4.269644\n",
            " 94534/100000: episode: 9572, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000167, mae: 3.196796, mean_q: 4.683025\n",
            " 94544/100000: episode: 9573, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000099, mae: 2.951984, mean_q: 4.397204\n",
            " 94552/100000: episode: 9574, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000059, mae: 3.007160, mean_q: 4.463775\n",
            " 94561/100000: episode: 9575, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000051, mae: 2.917911, mean_q: 4.361152\n",
            " 94569/100000: episode: 9576, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.853928, mean_q: 4.281634\n",
            " 94577/100000: episode: 9577, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.889474, mean_q: 4.326639\n",
            " 94587/100000: episode: 9578, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000052, mae: 2.995261, mean_q: 4.450288\n",
            " 94596/100000: episode: 9579, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000029, mae: 2.784013, mean_q: 4.204878\n",
            " 94604/100000: episode: 9580, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.146339, mean_q: 4.624967\n",
            " 94613/100000: episode: 9581, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 2.902469, mean_q: 4.341529\n",
            " 94621/100000: episode: 9582, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.928657, mean_q: 4.376800\n",
            " 94629/100000: episode: 9583, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.984640, mean_q: 4.442384\n",
            " 94637/100000: episode: 9584, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.922150, mean_q: 4.367865\n",
            " 94646/100000: episode: 9585, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000016, mae: 2.889274, mean_q: 4.329193\n",
            " 94654/100000: episode: 9586, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.044032, mean_q: 4.507538\n",
            " 94662/100000: episode: 9587, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.683369, mean_q: 4.092420\n",
            " 94670/100000: episode: 9588, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.048466, mean_q: 4.512185\n",
            " 94678/100000: episode: 9589, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.972164, mean_q: 4.427241\n",
            " 94686/100000: episode: 9590, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.743802, mean_q: 4.163434\n",
            " 94694/100000: episode: 9591, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.934056, mean_q: 4.376933\n",
            " 94702/100000: episode: 9592, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000182, mae: 3.059428, mean_q: 4.508762\n",
            " 94710/100000: episode: 9593, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000120, mae: 2.971794, mean_q: 4.413001\n",
            " 94718/100000: episode: 9594, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000091, mae: 2.997285, mean_q: 4.442698\n",
            " 94726/100000: episode: 9595, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 3.097032, mean_q: 4.562302\n",
            " 94735/100000: episode: 9596, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000125, mae: 2.972125, mean_q: 4.419818\n",
            " 94744/100000: episode: 9597, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000069, mae: 3.176175, mean_q: 4.658941\n",
            " 94754/100000: episode: 9598, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000091, mae: 2.997200, mean_q: 4.450722\n",
            " 94762/100000: episode: 9599, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000072, mae: 2.874756, mean_q: 4.307631\n",
            " 94770/100000: episode: 9600, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 2.945058, mean_q: 4.377210\n",
            " 94778/100000: episode: 9601, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000062, mae: 2.917988, mean_q: 4.358253\n",
            " 94788/100000: episode: 9602, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000078, mae: 2.989582, mean_q: 4.445573\n",
            " 94797/100000: episode: 9603, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000080, mae: 3.100585, mean_q: 4.559698\n",
            " 94806/100000: episode: 9604, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000050, mae: 2.817610, mean_q: 4.234541\n",
            " 94814/100000: episode: 9605, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000093, mae: 2.822114, mean_q: 4.262426\n",
            " 94822/100000: episode: 9606, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000120, mae: 2.990421, mean_q: 4.428916\n",
            " 94831/100000: episode: 9607, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000241, mae: 2.932685, mean_q: 4.364926\n",
            " 94839/100000: episode: 9608, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000257, mae: 2.738215, mean_q: 4.149970\n",
            " 94847/100000: episode: 9609, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 3.106932, mean_q: 4.571372\n",
            " 94855/100000: episode: 9610, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.130851, mean_q: 4.598523\n",
            " 94864/100000: episode: 9611, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000120, mae: 3.111103, mean_q: 4.579668\n",
            " 94872/100000: episode: 9612, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000106, mae: 2.655044, mean_q: 4.068930\n",
            " 94880/100000: episode: 9613, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000140, mae: 2.948059, mean_q: 4.381156\n",
            " 94890/100000: episode: 9614, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000133, mae: 2.917395, mean_q: 4.348413\n",
            " 94899/100000: episode: 9615, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000179, mae: 2.820631, mean_q: 4.243324\n",
            " 94908/100000: episode: 9616, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000147, mae: 2.906461, mean_q: 4.348872\n",
            " 94916/100000: episode: 9617, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000104, mae: 3.036041, mean_q: 4.478050\n",
            " 94928/100000: episode: 9618, duration: 0.144s, episode steps:  12, steps per second:  83, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000268, mae: 2.967800, mean_q: 4.399986\n",
            " 94937/100000: episode: 9619, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000314, mae: 3.087428, mean_q: 4.562947\n",
            " 94946/100000: episode: 9620, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000181, mae: 3.082217, mean_q: 4.544313\n",
            " 94955/100000: episode: 9621, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000146, mae: 2.980280, mean_q: 4.423982\n",
            " 94964/100000: episode: 9622, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000273, mae: 2.964861, mean_q: 4.403951\n",
            " 94973/100000: episode: 9623, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000119, mae: 2.902785, mean_q: 4.330218\n",
            " 94982/100000: episode: 9624, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000078, mae: 2.848984, mean_q: 4.280123\n",
            " 94990/100000: episode: 9625, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 2.809762, mean_q: 4.232188\n",
            " 95001/100000: episode: 9626, duration: 0.097s, episode steps:  11, steps per second: 114, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000059, mae: 2.868700, mean_q: 4.305513\n",
            " 95010/100000: episode: 9627, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000042, mae: 2.802922, mean_q: 4.231312\n",
            " 95018/100000: episode: 9628, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 3.062685, mean_q: 4.527591\n",
            " 95026/100000: episode: 9629, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.918556, mean_q: 4.355935\n",
            " 95034/100000: episode: 9630, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.766068, mean_q: 4.187721\n",
            " 95044/100000: episode: 9631, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000022, mae: 2.911825, mean_q: 4.348373\n",
            " 95052/100000: episode: 9632, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.930326, mean_q: 4.374753\n",
            " 95061/100000: episode: 9633, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000013, mae: 2.932530, mean_q: 4.375408\n",
            " 95069/100000: episode: 9634, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.916431, mean_q: 4.356577\n",
            " 95078/100000: episode: 9635, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000014, mae: 3.045054, mean_q: 4.513846\n",
            " 95086/100000: episode: 9636, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.051875, mean_q: 4.512909\n",
            " 95096/100000: episode: 9637, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000015, mae: 2.765601, mean_q: 4.187973\n",
            " 95104/100000: episode: 9638, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.063335, mean_q: 4.528539\n",
            " 95112/100000: episode: 9639, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.808529, mean_q: 4.229612\n",
            " 95120/100000: episode: 9640, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 3.076283, mean_q: 4.538882\n",
            " 95130/100000: episode: 9641, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000010, mae: 3.037779, mean_q: 4.499689\n",
            " 95138/100000: episode: 9642, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.060171, mean_q: 4.529822\n",
            " 95146/100000: episode: 9643, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.831785, mean_q: 4.269629\n",
            " 95155/100000: episode: 9644, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000011, mae: 2.980639, mean_q: 4.434885\n",
            " 95164/100000: episode: 9645, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000009, mae: 2.780982, mean_q: 4.202908\n",
            " 95175/100000: episode: 9646, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000006, mae: 2.829922, mean_q: 4.259728\n",
            " 95185/100000: episode: 9647, duration: 0.112s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000007, mae: 2.794180, mean_q: 4.220438\n",
            " 95193/100000: episode: 9648, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.230679, mean_q: 4.721753\n",
            " 95201/100000: episode: 9649, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.935092, mean_q: 4.383818\n",
            " 95212/100000: episode: 9650, duration: 0.105s, episode steps:  11, steps per second: 105, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000011, mae: 2.944118, mean_q: 4.395666\n",
            " 95221/100000: episode: 9651, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000007, mae: 2.857157, mean_q: 4.296085\n",
            " 95231/100000: episode: 9652, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000009, mae: 2.865042, mean_q: 4.302167\n",
            " 95240/100000: episode: 9653, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000006, mae: 2.832929, mean_q: 4.267002\n",
            " 95248/100000: episode: 9654, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.811364, mean_q: 4.240033\n",
            " 95256/100000: episode: 9655, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.015610, mean_q: 4.464444\n",
            " 95266/100000: episode: 9656, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000004, mae: 3.023704, mean_q: 4.484676\n",
            " 95274/100000: episode: 9657, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.130081, mean_q: 4.605843\n",
            " 95282/100000: episode: 9658, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.181761, mean_q: 4.670293\n",
            " 95290/100000: episode: 9659, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.750717, mean_q: 4.168219\n",
            " 95298/100000: episode: 9660, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.980037, mean_q: 4.430012\n",
            " 95308/100000: episode: 9661, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000006, mae: 3.095673, mean_q: 4.566284\n",
            " 95318/100000: episode: 9662, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.200 [0.000, 7.000],  loss: 0.000014, mae: 2.869645, mean_q: 4.305402\n",
            " 95326/100000: episode: 9663, duration: 0.077s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.024856, mean_q: 4.490130\n",
            " 95336/100000: episode: 9664, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.960374, mean_q: 4.412567\n",
            " 95344/100000: episode: 9665, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000015, mae: 2.951705, mean_q: 4.394853\n",
            " 95352/100000: episode: 9666, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.688464, mean_q: 4.094911\n",
            " 95363/100000: episode: 9667, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000010, mae: 2.937746, mean_q: 4.390805\n",
            " 95371/100000: episode: 9668, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.979078, mean_q: 4.431700\n",
            " 95379/100000: episode: 9669, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.770602, mean_q: 4.192638\n",
            " 95387/100000: episode: 9670, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.000323, mean_q: 4.458420\n",
            " 95396/100000: episode: 9671, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.884127, mean_q: 4.324000\n",
            " 95406/100000: episode: 9672, duration: 0.121s, episode steps:  10, steps per second:  82, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000007, mae: 3.065340, mean_q: 4.533652\n",
            " 95414/100000: episode: 9673, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.896586, mean_q: 4.339327\n",
            " 95423/100000: episode: 9674, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000014, mae: 3.117639, mean_q: 4.587414\n",
            " 95431/100000: episode: 9675, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000037, mae: 3.141247, mean_q: 4.613044\n",
            " 95439/100000: episode: 9676, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000067, mae: 2.827021, mean_q: 4.245913\n",
            " 95447/100000: episode: 9677, duration: 0.119s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000056, mae: 2.930632, mean_q: 4.382976\n",
            " 95455/100000: episode: 9678, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.911566, mean_q: 4.353257\n",
            " 95463/100000: episode: 9679, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 3.033653, mean_q: 4.494583\n",
            " 95471/100000: episode: 9680, duration: 0.126s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.861717, mean_q: 4.290947\n",
            " 95481/100000: episode: 9681, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000016, mae: 2.904294, mean_q: 4.341945\n",
            " 95490/100000: episode: 9682, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000013, mae: 2.984709, mean_q: 4.444328\n",
            " 95498/100000: episode: 9683, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.972005, mean_q: 4.427890\n",
            " 95507/100000: episode: 9684, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000017, mae: 2.913718, mean_q: 4.359854\n",
            " 95515/100000: episode: 9685, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.122221, mean_q: 4.601503\n",
            " 95523/100000: episode: 9686, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.828111, mean_q: 4.268962\n",
            " 95532/100000: episode: 9687, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000022, mae: 2.865020, mean_q: 4.306242\n",
            " 95541/100000: episode: 9688, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000018, mae: 2.996042, mean_q: 4.454160\n",
            " 95550/100000: episode: 9689, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000009, mae: 2.749421, mean_q: 4.168436\n",
            " 95559/100000: episode: 9690, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000009, mae: 3.081097, mean_q: 4.553194\n",
            " 95568/100000: episode: 9691, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000009, mae: 2.988285, mean_q: 4.442902\n",
            " 95576/100000: episode: 9692, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.957473, mean_q: 4.399249\n",
            " 95584/100000: episode: 9693, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.964627, mean_q: 4.417294\n",
            " 95592/100000: episode: 9694, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.010692, mean_q: 4.467816\n",
            " 95600/100000: episode: 9695, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.800004, mean_q: 4.224396\n",
            " 95609/100000: episode: 9696, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000013, mae: 3.090038, mean_q: 4.563208\n",
            " 95617/100000: episode: 9697, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.846651, mean_q: 4.277665\n",
            " 95627/100000: episode: 9698, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000009, mae: 3.120614, mean_q: 4.597103\n",
            " 95635/100000: episode: 9699, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.802079, mean_q: 4.227083\n",
            " 95643/100000: episode: 9700, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 2.825799, mean_q: 4.253962\n",
            " 95651/100000: episode: 9701, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.014201, mean_q: 4.478484\n",
            " 95659/100000: episode: 9702, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.897834, mean_q: 4.340530\n",
            " 95667/100000: episode: 9703, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.871496, mean_q: 4.309791\n",
            " 95675/100000: episode: 9704, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 3.057547, mean_q: 4.520506\n",
            " 95684/100000: episode: 9705, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000011, mae: 3.072014, mean_q: 4.539830\n",
            " 95693/100000: episode: 9706, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000022, mae: 2.889289, mean_q: 4.332085\n",
            " 95703/100000: episode: 9707, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000016, mae: 2.899544, mean_q: 4.339427\n",
            " 95711/100000: episode: 9708, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 3.101781, mean_q: 4.577581\n",
            " 95719/100000: episode: 9709, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.886605, mean_q: 4.324452\n",
            " 95727/100000: episode: 9710, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.886214, mean_q: 4.322572\n",
            " 95737/100000: episode: 9711, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.893533, mean_q: 4.333126\n",
            " 95748/100000: episode: 9712, duration: 0.102s, episode steps:  11, steps per second: 108, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000037, mae: 2.797617, mean_q: 4.217403\n",
            " 95757/100000: episode: 9713, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000041, mae: 2.815510, mean_q: 4.238632\n",
            " 95765/100000: episode: 9714, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.993322, mean_q: 4.445072\n",
            " 95776/100000: episode: 9715, duration: 0.097s, episode steps:  11, steps per second: 113, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000036, mae: 3.043351, mean_q: 4.504586\n",
            " 95784/100000: episode: 9716, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000075, mae: 2.933743, mean_q: 4.372646\n",
            " 95793/100000: episode: 9717, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000067, mae: 2.838834, mean_q: 4.271629\n",
            " 95803/100000: episode: 9718, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000129, mae: 2.956248, mean_q: 4.402098\n",
            " 95812/100000: episode: 9719, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000272, mae: 3.072131, mean_q: 4.534416\n",
            " 95821/100000: episode: 9720, duration: 0.105s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000158, mae: 3.127018, mean_q: 4.616149\n",
            " 95830/100000: episode: 9721, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000173, mae: 2.985045, mean_q: 4.419487\n",
            " 95839/100000: episode: 9722, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000090, mae: 3.136099, mean_q: 4.611474\n",
            " 95848/100000: episode: 9723, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000097, mae: 2.858354, mean_q: 4.288694\n",
            " 95857/100000: episode: 9724, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000150, mae: 2.753496, mean_q: 4.162372\n",
            " 95869/100000: episode: 9725, duration: 0.138s, episode steps:  12, steps per second:  87, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.750 [0.000, 7.000],  loss: 0.000115, mae: 2.967329, mean_q: 4.421411\n",
            " 95877/100000: episode: 9726, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000354, mae: 2.963440, mean_q: 4.414515\n",
            " 95888/100000: episode: 9727, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.364 [0.000, 7.000],  loss: 0.000565, mae: 2.913095, mean_q: 4.337889\n",
            " 95898/100000: episode: 9728, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000500, mae: 3.205608, mean_q: 4.694709\n",
            " 95908/100000: episode: 9729, duration: 0.114s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000310, mae: 2.972845, mean_q: 4.416540\n",
            " 95917/100000: episode: 9730, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000161, mae: 2.891664, mean_q: 4.329589\n",
            " 95925/100000: episode: 9731, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000192, mae: 2.788453, mean_q: 4.204988\n",
            " 95933/100000: episode: 9732, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000229, mae: 3.040893, mean_q: 4.484425\n",
            " 95941/100000: episode: 9733, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000250, mae: 3.095678, mean_q: 4.568570\n",
            " 95950/100000: episode: 9734, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000302, mae: 2.844289, mean_q: 4.266407\n",
            " 95959/100000: episode: 9735, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000259, mae: 3.036498, mean_q: 4.485846\n",
            " 95968/100000: episode: 9736, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000161, mae: 3.084904, mean_q: 4.554733\n",
            " 95978/100000: episode: 9737, duration: 0.090s, episode steps:  10, steps per second: 112, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000079, mae: 3.077687, mean_q: 4.538206\n",
            " 95986/100000: episode: 9738, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000080, mae: 2.865847, mean_q: 4.302687\n",
            " 95996/100000: episode: 9739, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000076, mae: 2.941211, mean_q: 4.382862\n",
            " 96006/100000: episode: 9740, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000030, mae: 2.819681, mean_q: 4.247221\n",
            " 96014/100000: episode: 9741, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.106743, mean_q: 4.577104\n",
            " 96023/100000: episode: 9742, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000012, mae: 2.834355, mean_q: 4.266856\n",
            " 96031/100000: episode: 9743, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.246781, mean_q: 4.740320\n",
            " 96039/100000: episode: 9744, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.012585, mean_q: 4.462314\n",
            " 96049/100000: episode: 9745, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000024, mae: 2.994588, mean_q: 4.442038\n",
            " 96057/100000: episode: 9746, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.944942, mean_q: 4.386274\n",
            " 96065/100000: episode: 9747, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.086804, mean_q: 4.563321\n",
            " 96074/100000: episode: 9748, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000040, mae: 2.838474, mean_q: 4.271435\n",
            " 96083/100000: episode: 9749, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000030, mae: 3.082673, mean_q: 4.549381\n",
            " 96091/100000: episode: 9750, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.933895, mean_q: 4.377486\n",
            " 96100/100000: episode: 9751, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000053, mae: 2.805641, mean_q: 4.231584\n",
            " 96109/100000: episode: 9752, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000011, mae: 3.209371, mean_q: 4.704707\n",
            " 96117/100000: episode: 9753, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 3.189919, mean_q: 4.671830\n",
            " 96125/100000: episode: 9754, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 2.833690, mean_q: 4.258719\n",
            " 96134/100000: episode: 9755, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000054, mae: 3.005721, mean_q: 4.455527\n",
            " 96142/100000: episode: 9756, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.945792, mean_q: 4.395131\n",
            " 96150/100000: episode: 9757, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000036, mae: 3.067190, mean_q: 4.530075\n",
            " 96158/100000: episode: 9758, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 3.035406, mean_q: 4.493984\n",
            " 96166/100000: episode: 9759, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 3.011703, mean_q: 4.463755\n",
            " 96174/100000: episode: 9760, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000046, mae: 2.990387, mean_q: 4.439544\n",
            " 96183/100000: episode: 9761, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000065, mae: 2.950639, mean_q: 4.399574\n",
            " 96191/100000: episode: 9762, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000098, mae: 3.028444, mean_q: 4.482982\n",
            " 96202/100000: episode: 9763, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000034, mae: 3.162484, mean_q: 4.646739\n",
            " 96212/100000: episode: 9764, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000044, mae: 3.103988, mean_q: 4.585193\n",
            " 96220/100000: episode: 9765, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.768712, mean_q: 4.188944\n",
            " 96231/100000: episode: 9766, duration: 0.126s, episode steps:  11, steps per second:  88, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.273 [0.000, 7.000],  loss: 0.000051, mae: 2.792821, mean_q: 4.207654\n",
            " 96239/100000: episode: 9767, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 3.227597, mean_q: 4.707489\n",
            " 96248/100000: episode: 9768, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000098, mae: 2.931160, mean_q: 4.366164\n",
            " 96256/100000: episode: 9769, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 3.049419, mean_q: 4.511246\n",
            " 96265/100000: episode: 9770, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000053, mae: 2.802346, mean_q: 4.225095\n",
            " 96274/100000: episode: 9771, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000026, mae: 3.036906, mean_q: 4.496242\n",
            " 96285/100000: episode: 9772, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000015, mae: 3.259167, mean_q: 4.754201\n",
            " 96293/100000: episode: 9773, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.933728, mean_q: 4.376827\n",
            " 96302/100000: episode: 9774, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000026, mae: 2.826922, mean_q: 4.256956\n",
            " 96310/100000: episode: 9775, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.748034, mean_q: 4.162329\n",
            " 96318/100000: episode: 9776, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 3.062829, mean_q: 4.526071\n",
            " 96326/100000: episode: 9777, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.904418, mean_q: 4.343064\n",
            " 96336/100000: episode: 9778, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000006, mae: 2.803689, mean_q: 4.227903\n",
            " 96344/100000: episode: 9779, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.951215, mean_q: 4.400687\n",
            " 96352/100000: episode: 9780, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 3.052434, mean_q: 4.520249\n",
            " 96361/100000: episode: 9781, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000006, mae: 3.060039, mean_q: 4.525180\n",
            " 96370/100000: episode: 9782, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000014, mae: 2.917755, mean_q: 4.363449\n",
            " 96378/100000: episode: 9783, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.691276, mean_q: 4.106057\n",
            " 96386/100000: episode: 9784, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 3.040550, mean_q: 4.504681\n",
            " 96396/100000: episode: 9785, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000024, mae: 2.939797, mean_q: 4.384097\n",
            " 96404/100000: episode: 9786, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.760348, mean_q: 4.179437\n",
            " 96412/100000: episode: 9787, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.715987, mean_q: 4.130438\n",
            " 96420/100000: episode: 9788, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.905926, mean_q: 4.349551\n",
            " 96429/100000: episode: 9789, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000004, mae: 2.889369, mean_q: 4.330004\n",
            " 96440/100000: episode: 9790, duration: 0.107s, episode steps:  11, steps per second: 103, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000005, mae: 3.130913, mean_q: 4.607393\n",
            " 96453/100000: episode: 9791, duration: 0.133s, episode steps:  13, steps per second:  98, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000005, mae: 2.892640, mean_q: 4.333924\n",
            " 96461/100000: episode: 9792, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.848276, mean_q: 4.278777\n",
            " 96471/100000: episode: 9793, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.966372, mean_q: 4.419086\n",
            " 96479/100000: episode: 9794, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.885743, mean_q: 4.323405\n",
            " 96487/100000: episode: 9795, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 2.857987, mean_q: 4.291028\n",
            " 96496/100000: episode: 9796, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000046, mae: 2.737449, mean_q: 4.157514\n",
            " 96504/100000: episode: 9797, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 3.006376, mean_q: 4.462156\n",
            " 96513/100000: episode: 9798, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000012, mae: 3.130236, mean_q: 4.607859\n",
            " 96524/100000: episode: 9799, duration: 0.130s, episode steps:  11, steps per second:  85, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.091 [0.000, 7.000],  loss: 0.000021, mae: 2.996902, mean_q: 4.451023\n",
            " 96532/100000: episode: 9800, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000017, mae: 2.925633, mean_q: 4.371035\n",
            " 96541/100000: episode: 9801, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000019, mae: 2.894025, mean_q: 4.342479\n",
            " 96549/100000: episode: 9802, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.832409, mean_q: 4.268208\n",
            " 96557/100000: episode: 9803, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.949912, mean_q: 4.404411\n",
            " 96565/100000: episode: 9804, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000022, mae: 3.020515, mean_q: 4.478973\n",
            " 96575/100000: episode: 9805, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000015, mae: 2.728398, mean_q: 4.138291\n",
            " 96583/100000: episode: 9806, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000018, mae: 2.823629, mean_q: 4.255648\n",
            " 96591/100000: episode: 9807, duration: 0.159s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.785532, mean_q: 4.202166\n",
            " 96600/100000: episode: 9808, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000067, mae: 3.111517, mean_q: 4.579548\n",
            " 96610/100000: episode: 9809, duration: 0.155s, episode steps:  10, steps per second:  65, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000052, mae: 2.871586, mean_q: 4.308562\n",
            " 96618/100000: episode: 9810, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000097, mae: 3.075614, mean_q: 4.544647\n",
            " 96628/100000: episode: 9811, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000079, mae: 2.880988, mean_q: 4.309330\n",
            " 96637/100000: episode: 9812, duration: 0.152s, episode steps:   9, steps per second:  59, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000077, mae: 2.773543, mean_q: 4.197049\n",
            " 96645/100000: episode: 9813, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 3.112929, mean_q: 4.590859\n",
            " 96654/100000: episode: 9814, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000073, mae: 2.957386, mean_q: 4.405843\n",
            " 96663/100000: episode: 9815, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000051, mae: 2.900441, mean_q: 4.349529\n",
            " 96672/100000: episode: 9816, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000063, mae: 3.048894, mean_q: 4.502298\n",
            " 96682/100000: episode: 9817, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000104, mae: 2.926164, mean_q: 4.358533\n",
            " 96690/100000: episode: 9818, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000167, mae: 3.117050, mean_q: 4.582453\n",
            " 96699/100000: episode: 9819, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000320, mae: 3.182823, mean_q: 4.658091\n",
            " 96707/100000: episode: 9820, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000202, mae: 2.779327, mean_q: 4.208109\n",
            " 96717/100000: episode: 9821, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000120, mae: 3.130640, mean_q: 4.609013\n",
            " 96725/100000: episode: 9822, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000088, mae: 2.987628, mean_q: 4.440812\n",
            " 96733/100000: episode: 9823, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 3.040504, mean_q: 4.510484\n",
            " 96741/100000: episode: 9824, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 3.070246, mean_q: 4.540557\n",
            " 96750/100000: episode: 9825, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000080, mae: 2.884139, mean_q: 4.315702\n",
            " 96760/100000: episode: 9826, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000104, mae: 2.874433, mean_q: 4.306830\n",
            " 96769/100000: episode: 9827, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000134, mae: 2.908750, mean_q: 4.336097\n",
            " 96777/100000: episode: 9828, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000089, mae: 3.071100, mean_q: 4.535006\n",
            " 96786/100000: episode: 9829, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000073, mae: 2.906464, mean_q: 4.343812\n",
            " 96794/100000: episode: 9830, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 2.914428, mean_q: 4.360446\n",
            " 96803/100000: episode: 9831, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000040, mae: 2.900173, mean_q: 4.335363\n",
            " 96811/100000: episode: 9832, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.768082, mean_q: 4.189731\n",
            " 96820/100000: episode: 9833, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000027, mae: 2.919597, mean_q: 4.366210\n",
            " 96828/100000: episode: 9834, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.903041, mean_q: 4.338538\n",
            " 96837/100000: episode: 9835, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000016, mae: 3.104419, mean_q: 4.582119\n",
            " 96845/100000: episode: 9836, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.070130, mean_q: 4.536211\n",
            " 96853/100000: episode: 9837, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 3.016932, mean_q: 4.476123\n",
            " 96862/100000: episode: 9838, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.840054, mean_q: 4.278019\n",
            " 96870/100000: episode: 9839, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.910456, mean_q: 4.353532\n",
            " 96878/100000: episode: 9840, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.230712, mean_q: 4.723194\n",
            " 96886/100000: episode: 9841, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.933441, mean_q: 4.380728\n",
            " 96895/100000: episode: 9842, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 2.922724, mean_q: 4.368749\n",
            " 96905/100000: episode: 9843, duration: 0.102s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000002, mae: 3.068305, mean_q: 4.534951\n",
            " 96914/100000: episode: 9844, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000002, mae: 2.899485, mean_q: 4.341054\n",
            " 96922/100000: episode: 9845, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.018390, mean_q: 4.480676\n",
            " 96931/100000: episode: 9846, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.899347, mean_q: 4.336790\n",
            " 96940/100000: episode: 9847, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000002, mae: 3.011213, mean_q: 4.473447\n",
            " 96949/100000: episode: 9848, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000002, mae: 3.025922, mean_q: 4.490270\n",
            " 96958/100000: episode: 9849, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.985476, mean_q: 4.442517\n",
            " 96967/100000: episode: 9850, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000006, mae: 2.989886, mean_q: 4.440938\n",
            " 96977/100000: episode: 9851, duration: 0.089s, episode steps:  10, steps per second: 113, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000004, mae: 2.910780, mean_q: 4.357866\n",
            " 96988/100000: episode: 9852, duration: 0.117s, episode steps:  11, steps per second:  94, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000003, mae: 2.913747, mean_q: 4.360464\n",
            " 96996/100000: episode: 9853, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.046644, mean_q: 4.512847\n",
            " 97008/100000: episode: 9854, duration: 0.108s, episode steps:  12, steps per second: 111, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.750 [0.000, 7.000],  loss: 0.000001, mae: 3.101582, mean_q: 4.576340\n",
            " 97016/100000: episode: 9855, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.805253, mean_q: 4.230357\n",
            " 97026/100000: episode: 9856, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.940457, mean_q: 4.390342\n",
            " 97035/100000: episode: 9857, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 3.013387, mean_q: 4.473433\n",
            " 97043/100000: episode: 9858, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.967330, mean_q: 4.420926\n",
            " 97053/100000: episode: 9859, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000003, mae: 2.787352, mean_q: 4.209434\n",
            " 97061/100000: episode: 9860, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.019341, mean_q: 4.480699\n",
            " 97070/100000: episode: 9861, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000004, mae: 2.953599, mean_q: 4.404680\n",
            " 97079/100000: episode: 9862, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000006, mae: 2.933001, mean_q: 4.383935\n",
            " 97088/100000: episode: 9863, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000006, mae: 2.925650, mean_q: 4.363199\n",
            " 97097/100000: episode: 9864, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000002, mae: 3.023381, mean_q: 4.485229\n",
            " 97108/100000: episode: 9865, duration: 0.101s, episode steps:  11, steps per second: 108, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000002, mae: 2.931047, mean_q: 4.378778\n",
            " 97119/100000: episode: 9866, duration: 0.109s, episode steps:  11, steps per second: 101, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000003, mae: 2.955573, mean_q: 4.405722\n",
            " 97127/100000: episode: 9867, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.952724, mean_q: 4.403223\n",
            " 97136/100000: episode: 9868, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000002, mae: 2.958907, mean_q: 4.408697\n",
            " 97145/100000: episode: 9869, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 3.076040, mean_q: 4.546834\n",
            " 97153/100000: episode: 9870, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.760471, mean_q: 4.182612\n",
            " 97162/100000: episode: 9871, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000010, mae: 2.927675, mean_q: 4.375545\n",
            " 97172/100000: episode: 9872, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000016, mae: 2.977943, mean_q: 4.428094\n",
            " 97181/100000: episode: 9873, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000026, mae: 2.882718, mean_q: 4.318437\n",
            " 97189/100000: episode: 9874, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000028, mae: 2.817702, mean_q: 4.245528\n",
            " 97198/100000: episode: 9875, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000032, mae: 2.904950, mean_q: 4.341249\n",
            " 97206/100000: episode: 9876, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000030, mae: 2.831949, mean_q: 4.262675\n",
            " 97214/100000: episode: 9877, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 3.052139, mean_q: 4.516600\n",
            " 97222/100000: episode: 9878, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000027, mae: 2.853967, mean_q: 4.284168\n",
            " 97231/100000: episode: 9879, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000021, mae: 3.020495, mean_q: 4.474888\n",
            " 97240/100000: episode: 9880, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000068, mae: 2.986628, mean_q: 4.442159\n",
            " 97249/100000: episode: 9881, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000057, mae: 2.931666, mean_q: 4.380951\n",
            " 97258/100000: episode: 9882, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000031, mae: 2.979540, mean_q: 4.435724\n",
            " 97267/100000: episode: 9883, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000030, mae: 3.120513, mean_q: 4.591162\n",
            " 97277/100000: episode: 9884, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000027, mae: 2.714649, mean_q: 4.123697\n",
            " 97285/100000: episode: 9885, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000069, mae: 2.813627, mean_q: 4.238036\n",
            " 97294/100000: episode: 9886, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000077, mae: 2.842574, mean_q: 4.264988\n",
            " 97302/100000: episode: 9887, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000061, mae: 2.933824, mean_q: 4.378631\n",
            " 97310/100000: episode: 9888, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.776224, mean_q: 4.204640\n",
            " 97319/100000: episode: 9889, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000026, mae: 2.806908, mean_q: 4.237082\n",
            " 97327/100000: episode: 9890, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.987205, mean_q: 4.450283\n",
            " 97336/100000: episode: 9891, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000025, mae: 2.913521, mean_q: 4.360754\n",
            " 97344/100000: episode: 9892, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000078, mae: 2.761343, mean_q: 4.187546\n",
            " 97353/100000: episode: 9893, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000128, mae: 2.781528, mean_q: 4.190156\n",
            " 97362/100000: episode: 9894, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000088, mae: 3.023205, mean_q: 4.482385\n",
            " 97371/100000: episode: 9895, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000143, mae: 3.020546, mean_q: 4.479105\n",
            " 97381/100000: episode: 9896, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 3.198521, mean_q: 4.675653\n",
            " 97391/100000: episode: 9897, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000088, mae: 2.792991, mean_q: 4.208693\n",
            " 97402/100000: episode: 9898, duration: 0.098s, episode steps:  11, steps per second: 112, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.091 [0.000, 7.000],  loss: 0.000109, mae: 2.763228, mean_q: 4.179132\n",
            " 97411/100000: episode: 9899, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000187, mae: 3.206620, mean_q: 4.700632\n",
            " 97420/100000: episode: 9900, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000177, mae: 2.987414, mean_q: 4.441236\n",
            " 97429/100000: episode: 9901, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000098, mae: 2.770135, mean_q: 4.194105\n",
            " 97438/100000: episode: 9902, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000063, mae: 2.930988, mean_q: 4.378759\n",
            " 97446/100000: episode: 9903, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000121, mae: 2.818530, mean_q: 4.237389\n",
            " 97456/100000: episode: 9904, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000088, mae: 2.847084, mean_q: 4.270601\n",
            " 97464/100000: episode: 9905, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000112, mae: 2.922574, mean_q: 4.370033\n",
            " 97472/100000: episode: 9906, duration: 0.068s, episode steps:   8, steps per second: 118, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000119, mae: 3.157503, mean_q: 4.643138\n",
            " 97482/100000: episode: 9907, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000103, mae: 3.047491, mean_q: 4.501822\n",
            " 97491/100000: episode: 9908, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000091, mae: 3.214079, mean_q: 4.696800\n",
            " 97499/100000: episode: 9909, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000439, mae: 2.743004, mean_q: 4.134283\n",
            " 97507/100000: episode: 9910, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000247, mae: 3.041451, mean_q: 4.490068\n",
            " 97515/100000: episode: 9911, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000306, mae: 3.068642, mean_q: 4.515442\n",
            " 97524/100000: episode: 9912, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000236, mae: 3.063054, mean_q: 4.521041\n",
            " 97532/100000: episode: 9913, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000322, mae: 2.973211, mean_q: 4.410933\n",
            " 97541/100000: episode: 9914, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000213, mae: 2.756708, mean_q: 4.161644\n",
            " 97550/100000: episode: 9915, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000237, mae: 3.098415, mean_q: 4.567251\n",
            " 97558/100000: episode: 9916, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000150, mae: 2.796725, mean_q: 4.229190\n",
            " 97567/100000: episode: 9917, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000120, mae: 2.937238, mean_q: 4.388989\n",
            " 97575/100000: episode: 9918, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000105, mae: 2.742949, mean_q: 4.149078\n",
            " 97583/100000: episode: 9919, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 2.950047, mean_q: 4.389577\n",
            " 97591/100000: episode: 9920, duration: 0.087s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000043, mae: 2.987620, mean_q: 4.437110\n",
            " 97599/100000: episode: 9921, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000033, mae: 2.965826, mean_q: 4.417061\n",
            " 97607/100000: episode: 9922, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.871771, mean_q: 4.314149\n",
            " 97616/100000: episode: 9923, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000048, mae: 2.887833, mean_q: 4.330868\n",
            " 97624/100000: episode: 9924, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 3.203849, mean_q: 4.693768\n",
            " 97637/100000: episode: 9925, duration: 0.173s, episode steps:  13, steps per second:  75, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 3.462 [0.000, 7.000],  loss: 0.000055, mae: 2.859415, mean_q: 4.298207\n",
            " 97645/100000: episode: 9926, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000168, mae: 2.890704, mean_q: 4.335902\n",
            " 97654/100000: episode: 9927, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000116, mae: 3.015911, mean_q: 4.453881\n",
            " 97664/100000: episode: 9928, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000293, mae: 3.026650, mean_q: 4.483184\n",
            " 97674/100000: episode: 9929, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000135, mae: 2.898585, mean_q: 4.323347\n",
            " 97683/100000: episode: 9930, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000135, mae: 3.064973, mean_q: 4.517607\n",
            " 97692/100000: episode: 9931, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000194, mae: 2.793118, mean_q: 4.220749\n",
            " 97702/100000: episode: 9932, duration: 0.092s, episode steps:  10, steps per second: 109, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000193, mae: 2.973979, mean_q: 4.409228\n",
            " 97710/100000: episode: 9933, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000334, mae: 3.060985, mean_q: 4.490455\n",
            " 97718/100000: episode: 9934, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000311, mae: 3.046486, mean_q: 4.502554\n",
            " 97729/100000: episode: 9935, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000212, mae: 3.114517, mean_q: 4.581716\n",
            " 97739/100000: episode: 9936, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000347, mae: 2.940398, mean_q: 4.371477\n",
            " 97749/100000: episode: 9937, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000146, mae: 3.098261, mean_q: 4.568620\n",
            " 97759/100000: episode: 9938, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000238, mae: 2.982280, mean_q: 4.419895\n",
            " 97768/100000: episode: 9939, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000122, mae: 3.190417, mean_q: 4.661664\n",
            " 97776/100000: episode: 9940, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000149, mae: 2.947893, mean_q: 4.397757\n",
            " 97784/100000: episode: 9941, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000170, mae: 2.849407, mean_q: 4.284981\n",
            " 97793/100000: episode: 9942, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000071, mae: 2.905258, mean_q: 4.337735\n",
            " 97802/100000: episode: 9943, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000029, mae: 2.960607, mean_q: 4.400620\n",
            " 97810/100000: episode: 9944, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.840752, mean_q: 4.270843\n",
            " 97818/100000: episode: 9945, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000023, mae: 2.972967, mean_q: 4.423645\n",
            " 97826/100000: episode: 9946, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000034, mae: 3.145871, mean_q: 4.614900\n",
            " 97835/100000: episode: 9947, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000008, mae: 3.023860, mean_q: 4.477107\n",
            " 97844/100000: episode: 9948, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000031, mae: 3.020821, mean_q: 4.475684\n",
            " 97852/100000: episode: 9949, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.833261, mean_q: 4.263780\n",
            " 97860/100000: episode: 9950, duration: 0.179s, episode steps:   8, steps per second:  45, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.887544, mean_q: 4.321387\n",
            " 97868/100000: episode: 9951, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.866741, mean_q: 4.298916\n",
            " 97877/100000: episode: 9952, duration: 0.155s, episode steps:   9, steps per second:  58, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000005, mae: 2.936454, mean_q: 4.384160\n",
            " 97885/100000: episode: 9953, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.834002, mean_q: 4.266136\n",
            " 97894/100000: episode: 9954, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000015, mae: 2.915067, mean_q: 4.360461\n",
            " 97902/100000: episode: 9955, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000009, mae: 2.914646, mean_q: 4.358841\n",
            " 97910/100000: episode: 9956, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.066501, mean_q: 4.532130\n",
            " 97918/100000: episode: 9957, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.840242, mean_q: 4.267920\n",
            " 97926/100000: episode: 9958, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 2.972031, mean_q: 4.420268\n",
            " 97938/100000: episode: 9959, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.083 [0.000, 7.000],  loss: 0.000017, mae: 2.854875, mean_q: 4.285793\n",
            " 97946/100000: episode: 9960, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.935331, mean_q: 4.382104\n",
            " 97955/100000: episode: 9961, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000007, mae: 2.839103, mean_q: 4.270987\n",
            " 97963/100000: episode: 9962, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.814369, mean_q: 4.250460\n",
            " 97971/100000: episode: 9963, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000012, mae: 2.768992, mean_q: 4.192838\n",
            " 97981/100000: episode: 9964, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.100 [0.000, 7.000],  loss: 0.000012, mae: 2.876551, mean_q: 4.310734\n",
            " 97991/100000: episode: 9965, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000011, mae: 2.928087, mean_q: 4.377629\n",
            " 98001/100000: episode: 9966, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.800 [0.000, 7.000],  loss: 0.000003, mae: 2.897847, mean_q: 4.342433\n",
            " 98009/100000: episode: 9967, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 2.948550, mean_q: 4.395838\n",
            " 98017/100000: episode: 9968, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.875466, mean_q: 4.315176\n",
            " 98026/100000: episode: 9969, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000011, mae: 2.870248, mean_q: 4.310010\n",
            " 98034/100000: episode: 9970, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.888930, mean_q: 4.327633\n",
            " 98043/100000: episode: 9971, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000027, mae: 2.978868, mean_q: 4.430850\n",
            " 98051/100000: episode: 9972, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.762874, mean_q: 4.181280\n",
            " 98061/100000: episode: 9973, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000056, mae: 2.795653, mean_q: 4.216817\n",
            " 98071/100000: episode: 9974, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000033, mae: 3.013823, mean_q: 4.465829\n",
            " 98080/100000: episode: 9975, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000017, mae: 2.948898, mean_q: 4.403313\n",
            " 98089/100000: episode: 9976, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000067, mae: 2.851124, mean_q: 4.288842\n",
            " 98097/100000: episode: 9977, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000071, mae: 2.883330, mean_q: 4.310375\n",
            " 98106/100000: episode: 9978, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000075, mae: 3.055449, mean_q: 4.513830\n",
            " 98114/100000: episode: 9979, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000061, mae: 2.754396, mean_q: 4.178859\n",
            " 98125/100000: episode: 9980, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.182 [0.000, 7.000],  loss: 0.000077, mae: 2.992430, mean_q: 4.451100\n",
            " 98133/100000: episode: 9981, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000077, mae: 2.737085, mean_q: 4.133095\n",
            " 98143/100000: episode: 9982, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000150, mae: 3.155818, mean_q: 4.624988\n",
            " 98151/100000: episode: 9983, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000143, mae: 2.981592, mean_q: 4.443894\n",
            " 98159/100000: episode: 9984, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000087, mae: 3.236844, mean_q: 4.730204\n",
            " 98167/100000: episode: 9985, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000087, mae: 3.114303, mean_q: 4.593722\n",
            " 98177/100000: episode: 9986, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 4.000 [0.000, 7.000],  loss: 0.000066, mae: 3.000452, mean_q: 4.461965\n",
            " 98186/100000: episode: 9987, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000043, mae: 2.928970, mean_q: 4.375468\n",
            " 98197/100000: episode: 9988, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000026, mae: 2.833365, mean_q: 4.265654\n",
            " 98205/100000: episode: 9989, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.901854, mean_q: 4.347754\n",
            " 98213/100000: episode: 9990, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000024, mae: 2.799208, mean_q: 4.225702\n",
            " 98222/100000: episode: 9991, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000027, mae: 3.075373, mean_q: 4.541084\n",
            " 98232/100000: episode: 9992, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000035, mae: 2.979846, mean_q: 4.432582\n",
            " 98241/100000: episode: 9993, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000029, mae: 3.046451, mean_q: 4.511176\n",
            " 98249/100000: episode: 9994, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.985697, mean_q: 4.440491\n",
            " 98258/100000: episode: 9995, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000038, mae: 2.845826, mean_q: 4.273984\n",
            " 98266/100000: episode: 9996, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.971413, mean_q: 4.419104\n",
            " 98277/100000: episode: 9997, duration: 0.127s, episode steps:  11, steps per second:  86, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.727 [0.000, 7.000],  loss: 0.000044, mae: 2.988458, mean_q: 4.442924\n",
            " 98286/100000: episode: 9998, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000018, mae: 3.157632, mean_q: 4.642794\n",
            " 98295/100000: episode: 9999, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000010, mae: 3.006027, mean_q: 4.463582\n",
            " 98306/100000: episode: 10000, duration: 0.113s, episode steps:  11, steps per second:  97, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000010, mae: 2.900558, mean_q: 4.342196\n",
            " 98314/100000: episode: 10001, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.914517, mean_q: 4.355621\n",
            " 98322/100000: episode: 10002, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000007, mae: 2.881242, mean_q: 4.322179\n",
            " 98330/100000: episode: 10003, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.992848, mean_q: 4.454618\n",
            " 98338/100000: episode: 10004, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.840558, mean_q: 4.278884\n",
            " 98346/100000: episode: 10005, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.788320, mean_q: 4.207329\n",
            " 98354/100000: episode: 10006, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.862636, mean_q: 4.301074\n",
            " 98362/100000: episode: 10007, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 3.000818, mean_q: 4.463388\n",
            " 98370/100000: episode: 10008, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.064962, mean_q: 4.532039\n",
            " 98380/100000: episode: 10009, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000002, mae: 2.777121, mean_q: 4.199472\n",
            " 98388/100000: episode: 10010, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.899231, mean_q: 4.340867\n",
            " 98397/100000: episode: 10011, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000002, mae: 2.788489, mean_q: 4.212927\n",
            " 98405/100000: episode: 10012, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.057992, mean_q: 4.524471\n",
            " 98414/100000: episode: 10013, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000001, mae: 3.156825, mean_q: 4.641333\n",
            " 98423/100000: episode: 10014, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 2.979646, mean_q: 4.430459\n",
            " 98431/100000: episode: 10015, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.778887, mean_q: 4.202942\n",
            " 98439/100000: episode: 10016, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.144928, mean_q: 4.619645\n",
            " 98449/100000: episode: 10017, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.800 [0.000, 7.000],  loss: 0.000004, mae: 3.048093, mean_q: 4.509846\n",
            " 98458/100000: episode: 10018, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000007, mae: 3.085673, mean_q: 4.551802\n",
            " 98469/100000: episode: 10019, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.000 [0.000, 7.000],  loss: 0.000004, mae: 3.056437, mean_q: 4.522477\n",
            " 98477/100000: episode: 10020, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 3.083315, mean_q: 4.553770\n",
            " 98485/100000: episode: 10021, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.875525, mean_q: 4.314294\n",
            " 98494/100000: episode: 10022, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000004, mae: 3.021941, mean_q: 4.486551\n",
            " 98502/100000: episode: 10023, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.129962, mean_q: 4.604420\n",
            " 98511/100000: episode: 10024, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000008, mae: 2.911565, mean_q: 4.356540\n",
            " 98519/100000: episode: 10025, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.915231, mean_q: 4.357205\n",
            " 98527/100000: episode: 10026, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.985526, mean_q: 4.436616\n",
            " 98535/100000: episode: 10027, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000070, mae: 2.945860, mean_q: 4.398144\n",
            " 98545/100000: episode: 10028, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000049, mae: 2.981749, mean_q: 4.432692\n",
            " 98553/100000: episode: 10029, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.879460, mean_q: 4.313467\n",
            " 98561/100000: episode: 10030, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000031, mae: 2.918517, mean_q: 4.358329\n",
            " 98570/100000: episode: 10031, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000028, mae: 2.892983, mean_q: 4.329142\n",
            " 98579/100000: episode: 10032, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000043, mae: 2.811838, mean_q: 4.233413\n",
            " 98590/100000: episode: 10033, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.909 [0.000, 7.000],  loss: 0.000127, mae: 3.034169, mean_q: 4.489142\n",
            " 98599/100000: episode: 10034, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000239, mae: 2.998255, mean_q: 4.446447\n",
            " 98608/100000: episode: 10035, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000219, mae: 2.960389, mean_q: 4.403185\n",
            " 98616/100000: episode: 10036, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000211, mae: 2.862389, mean_q: 4.298380\n",
            " 98625/100000: episode: 10037, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000211, mae: 3.028847, mean_q: 4.479727\n",
            " 98633/100000: episode: 10038, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000182, mae: 2.933486, mean_q: 4.374675\n",
            " 98641/100000: episode: 10039, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000219, mae: 2.904134, mean_q: 4.341983\n",
            " 98651/100000: episode: 10040, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000186, mae: 3.012424, mean_q: 4.463763\n",
            " 98659/100000: episode: 10041, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000163, mae: 3.094038, mean_q: 4.554573\n",
            " 98667/100000: episode: 10042, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000203, mae: 3.270115, mean_q: 4.766707\n",
            " 98675/100000: episode: 10043, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000212, mae: 2.844088, mean_q: 4.273407\n",
            " 98686/100000: episode: 10044, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.818 [0.000, 7.000],  loss: 0.000142, mae: 2.913279, mean_q: 4.355800\n",
            " 98694/100000: episode: 10045, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000065, mae: 2.890987, mean_q: 4.327035\n",
            " 98702/100000: episode: 10046, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000040, mae: 2.996294, mean_q: 4.443152\n",
            " 98713/100000: episode: 10047, duration: 0.097s, episode steps:  11, steps per second: 113, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000016, mae: 2.981996, mean_q: 4.432800\n",
            " 98721/100000: episode: 10048, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.743298, mean_q: 4.156409\n",
            " 98730/100000: episode: 10049, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000016, mae: 3.194854, mean_q: 4.672492\n",
            " 98741/100000: episode: 10050, duration: 0.101s, episode steps:  11, steps per second: 108, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000079, mae: 3.043728, mean_q: 4.506376\n",
            " 98751/100000: episode: 10051, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000082, mae: 2.909680, mean_q: 4.341925\n",
            " 98760/100000: episode: 10052, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000045, mae: 2.858035, mean_q: 4.277260\n",
            " 98768/100000: episode: 10053, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000058, mae: 3.053194, mean_q: 4.507531\n",
            " 98779/100000: episode: 10054, duration: 0.125s, episode steps:  11, steps per second:  88, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 4.182 [0.000, 7.000],  loss: 0.000020, mae: 2.922571, mean_q: 4.369061\n",
            " 98788/100000: episode: 10055, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000052, mae: 2.848315, mean_q: 4.280939\n",
            " 98796/100000: episode: 10056, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000064, mae: 2.848042, mean_q: 4.270534\n",
            " 98804/100000: episode: 10057, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000039, mae: 2.973127, mean_q: 4.415157\n",
            " 98812/100000: episode: 10058, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000104, mae: 2.842812, mean_q: 4.267272\n",
            " 98822/100000: episode: 10059, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000171, mae: 2.830698, mean_q: 4.244878\n",
            " 98831/100000: episode: 10060, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000107, mae: 3.132655, mean_q: 4.609678\n",
            " 98840/100000: episode: 10061, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000092, mae: 2.782699, mean_q: 4.208429\n",
            " 98849/100000: episode: 10062, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000115, mae: 2.891943, mean_q: 4.328294\n",
            " 98858/100000: episode: 10063, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000112, mae: 2.978962, mean_q: 4.428098\n",
            " 98868/100000: episode: 10064, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000068, mae: 2.814879, mean_q: 4.235978\n",
            " 98877/100000: episode: 10065, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000049, mae: 2.894515, mean_q: 4.331869\n",
            " 98885/100000: episode: 10066, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000051, mae: 2.914581, mean_q: 4.355723\n",
            " 98893/100000: episode: 10067, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000025, mae: 2.897716, mean_q: 4.335002\n",
            " 98902/100000: episode: 10068, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000017, mae: 2.990043, mean_q: 4.439013\n",
            " 98911/100000: episode: 10069, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000026, mae: 3.079232, mean_q: 4.540138\n",
            " 98919/100000: episode: 10070, duration: 0.142s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 2.998460, mean_q: 4.448638\n",
            " 98927/100000: episode: 10071, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.982095, mean_q: 4.437158\n",
            " 98935/100000: episode: 10072, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000016, mae: 2.981028, mean_q: 4.437633\n",
            " 98944/100000: episode: 10073, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000038, mae: 3.068107, mean_q: 4.540869\n",
            " 98952/100000: episode: 10074, duration: 0.171s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000038, mae: 3.159158, mean_q: 4.643111\n",
            " 98960/100000: episode: 10075, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000041, mae: 2.864556, mean_q: 4.300200\n",
            " 98969/100000: episode: 10076, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000035, mae: 2.823230, mean_q: 4.254276\n",
            " 98977/100000: episode: 10077, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000026, mae: 2.892031, mean_q: 4.328744\n",
            " 98985/100000: episode: 10078, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.938406, mean_q: 4.391650\n",
            " 98994/100000: episode: 10079, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000032, mae: 2.980484, mean_q: 4.432941\n",
            " 99002/100000: episode: 10080, duration: 0.171s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000042, mae: 2.847351, mean_q: 4.275588\n",
            " 99010/100000: episode: 10081, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.919045, mean_q: 4.368469\n",
            " 99018/100000: episode: 10082, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000117, mae: 2.974404, mean_q: 4.423934\n",
            " 99028/100000: episode: 10083, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000247, mae: 3.004990, mean_q: 4.456633\n",
            " 99036/100000: episode: 10084, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000174, mae: 3.040019, mean_q: 4.497139\n",
            " 99045/100000: episode: 10085, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000140, mae: 3.042888, mean_q: 4.490316\n",
            " 99053/100000: episode: 10086, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000151, mae: 3.081093, mean_q: 4.537367\n",
            " 99061/100000: episode: 10087, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000229, mae: 2.762611, mean_q: 4.176265\n",
            " 99069/100000: episode: 10088, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000135, mae: 2.830981, mean_q: 4.268038\n",
            " 99077/100000: episode: 10089, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000178, mae: 2.899310, mean_q: 4.325466\n",
            " 99085/100000: episode: 10090, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000103, mae: 2.942276, mean_q: 4.386476\n",
            " 99094/100000: episode: 10091, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000142, mae: 2.879894, mean_q: 4.312275\n",
            " 99103/100000: episode: 10092, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000115, mae: 3.161573, mean_q: 4.636508\n",
            " 99111/100000: episode: 10093, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000124, mae: 2.916357, mean_q: 4.359843\n",
            " 99120/100000: episode: 10094, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000085, mae: 2.946968, mean_q: 4.392269\n",
            " 99129/100000: episode: 10095, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000044, mae: 3.139162, mean_q: 4.613003\n",
            " 99137/100000: episode: 10096, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 2.974810, mean_q: 4.437737\n",
            " 99146/100000: episode: 10097, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000022, mae: 2.978576, mean_q: 4.429642\n",
            " 99155/100000: episode: 10098, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000011, mae: 2.899093, mean_q: 4.338759\n",
            " 99164/100000: episode: 10099, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000020, mae: 2.991310, mean_q: 4.443010\n",
            " 99174/100000: episode: 10100, duration: 0.088s, episode steps:  10, steps per second: 113, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000007, mae: 2.858116, mean_q: 4.291589\n",
            " 99183/100000: episode: 10101, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000007, mae: 3.006995, mean_q: 4.459709\n",
            " 99193/100000: episode: 10102, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000008, mae: 2.910331, mean_q: 4.353803\n",
            " 99201/100000: episode: 10103, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000019, mae: 3.176507, mean_q: 4.662094\n",
            " 99210/100000: episode: 10104, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000014, mae: 2.679040, mean_q: 4.088576\n",
            " 99218/100000: episode: 10105, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.869784, mean_q: 4.306547\n",
            " 99227/100000: episode: 10106, duration: 0.079s, episode steps:   9, steps per second: 113, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000012, mae: 3.030153, mean_q: 4.491497\n",
            " 99236/100000: episode: 10107, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000005, mae: 2.976367, mean_q: 4.428841\n",
            " 99244/100000: episode: 10108, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.140296, mean_q: 4.618974\n",
            " 99253/100000: episode: 10109, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000003, mae: 3.105181, mean_q: 4.579470\n",
            " 99262/100000: episode: 10110, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 3.102339, mean_q: 4.575349\n",
            " 99270/100000: episode: 10111, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.929466, mean_q: 4.376025\n",
            " 99279/100000: episode: 10112, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000005, mae: 2.954444, mean_q: 4.401144\n",
            " 99287/100000: episode: 10113, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.041067, mean_q: 4.506218\n",
            " 99297/100000: episode: 10114, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.100 [0.000, 7.000],  loss: 0.000004, mae: 3.019789, mean_q: 4.481262\n",
            " 99306/100000: episode: 10115, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000004, mae: 2.854385, mean_q: 4.292127\n",
            " 99315/100000: episode: 10116, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000003, mae: 2.835811, mean_q: 4.270785\n",
            " 99323/100000: episode: 10117, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 3.011640, mean_q: 4.474954\n",
            " 99333/100000: episode: 10118, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000004, mae: 2.900233, mean_q: 4.339491\n",
            " 99342/100000: episode: 10119, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000001, mae: 3.160787, mean_q: 4.640968\n",
            " 99350/100000: episode: 10120, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.836306, mean_q: 4.268661\n",
            " 99360/100000: episode: 10121, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000003, mae: 2.933188, mean_q: 4.381414\n",
            " 99371/100000: episode: 10122, duration: 0.105s, episode steps:  11, steps per second: 105, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.636 [0.000, 7.000],  loss: 0.000001, mae: 3.148456, mean_q: 4.626020\n",
            " 99379/100000: episode: 10123, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.155810, mean_q: 4.632742\n",
            " 99387/100000: episode: 10124, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.775721, mean_q: 4.201082\n",
            " 99396/100000: episode: 10125, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000002, mae: 3.054075, mean_q: 4.526178\n",
            " 99405/100000: episode: 10126, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000003, mae: 3.091212, mean_q: 4.566037\n",
            " 99413/100000: episode: 10127, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.144753, mean_q: 4.621809\n",
            " 99426/100000: episode: 10128, duration: 0.115s, episode steps:  13, steps per second: 113, episode reward:  3.000, mean reward:  0.231 [-1.000,  1.000], mean action: 3.769 [0.000, 7.000],  loss: 0.000003, mae: 2.833271, mean_q: 4.266751\n",
            " 99434/100000: episode: 10129, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.834096, mean_q: 4.267308\n",
            " 99442/100000: episode: 10130, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.697773, mean_q: 4.106483\n",
            " 99450/100000: episode: 10131, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000001, mae: 2.811405, mean_q: 4.238854\n",
            " 99459/100000: episode: 10132, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000002, mae: 2.867254, mean_q: 4.299277\n",
            " 99467/100000: episode: 10133, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.737122, mean_q: 4.155804\n",
            " 99475/100000: episode: 10134, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 2.777885, mean_q: 4.200032\n",
            " 99483/100000: episode: 10135, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 3.019056, mean_q: 4.478920\n",
            " 99491/100000: episode: 10136, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.890805, mean_q: 4.329852\n",
            " 99501/100000: episode: 10137, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.900 [0.000, 7.000],  loss: 0.000007, mae: 3.032033, mean_q: 4.497827\n",
            " 99509/100000: episode: 10138, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000005, mae: 2.959333, mean_q: 4.407907\n",
            " 99519/100000: episode: 10139, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.300 [0.000, 7.000],  loss: 0.000002, mae: 2.977190, mean_q: 4.430348\n",
            " 99529/100000: episode: 10140, duration: 0.104s, episode steps:  10, steps per second:  97, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000004, mae: 2.849160, mean_q: 4.280221\n",
            " 99539/100000: episode: 10141, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.400 [0.000, 7.000],  loss: 0.000005, mae: 3.015626, mean_q: 4.478360\n",
            " 99547/100000: episode: 10142, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000006, mae: 3.148448, mean_q: 4.630683\n",
            " 99558/100000: episode: 10143, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.455 [0.000, 7.000],  loss: 0.000002, mae: 2.986720, mean_q: 4.441281\n",
            " 99566/100000: episode: 10144, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000002, mae: 2.928133, mean_q: 4.375962\n",
            " 99574/100000: episode: 10145, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.029142, mean_q: 4.487942\n",
            " 99582/100000: episode: 10146, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000003, mae: 3.027276, mean_q: 4.497199\n",
            " 99593/100000: episode: 10147, duration: 0.126s, episode steps:  11, steps per second:  87, episode reward:  5.000, mean reward:  0.455 [-1.000,  1.000], mean action: 3.545 [0.000, 7.000],  loss: 0.000009, mae: 3.064831, mean_q: 4.535504\n",
            " 99603/100000: episode: 10148, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.200 [0.000, 7.000],  loss: 0.000057, mae: 2.917246, mean_q: 4.365721\n",
            " 99612/100000: episode: 10149, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000103, mae: 2.833031, mean_q: 4.269547\n",
            " 99622/100000: episode: 10150, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000045, mae: 2.934358, mean_q: 4.371386\n",
            " 99630/100000: episode: 10151, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000044, mae: 2.934120, mean_q: 4.376156\n",
            " 99639/100000: episode: 10152, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000131, mae: 2.862745, mean_q: 4.294071\n",
            " 99648/100000: episode: 10153, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000111, mae: 2.958015, mean_q: 4.393777\n",
            " 99656/100000: episode: 10154, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000126, mae: 2.900987, mean_q: 4.325468\n",
            " 99664/100000: episode: 10155, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000211, mae: 2.896136, mean_q: 4.334361\n",
            " 99672/100000: episode: 10156, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000142, mae: 2.808248, mean_q: 4.239204\n",
            " 99680/100000: episode: 10157, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000157, mae: 3.052676, mean_q: 4.526684\n",
            " 99689/100000: episode: 10158, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000127, mae: 2.859106, mean_q: 4.295593\n",
            " 99698/100000: episode: 10159, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000117, mae: 3.155278, mean_q: 4.618983\n",
            " 99706/100000: episode: 10160, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000062, mae: 3.121300, mean_q: 4.591145\n",
            " 99715/100000: episode: 10161, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000071, mae: 2.952408, mean_q: 4.404353\n",
            " 99723/100000: episode: 10162, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000089, mae: 3.177575, mean_q: 4.656878\n",
            " 99732/100000: episode: 10163, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000073, mae: 2.862546, mean_q: 4.301667\n",
            " 99741/100000: episode: 10164, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.333 [0.000, 7.000],  loss: 0.000053, mae: 2.955219, mean_q: 4.402585\n",
            " 99750/100000: episode: 10165, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.667 [0.000, 7.000],  loss: 0.000032, mae: 2.811934, mean_q: 4.239090\n",
            " 99760/100000: episode: 10166, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.600 [0.000, 7.000],  loss: 0.000029, mae: 2.931831, mean_q: 4.368725\n",
            " 99768/100000: episode: 10167, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000021, mae: 2.981804, mean_q: 4.432510\n",
            " 99776/100000: episode: 10168, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000029, mae: 2.880110, mean_q: 4.311313\n",
            " 99785/100000: episode: 10169, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.444 [0.000, 7.000],  loss: 0.000053, mae: 2.788794, mean_q: 4.210709\n",
            " 99794/100000: episode: 10170, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000048, mae: 2.813251, mean_q: 4.239472\n",
            " 99802/100000: episode: 10171, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.858521, mean_q: 4.278133\n",
            " 99810/100000: episode: 10172, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000055, mae: 2.815342, mean_q: 4.239823\n",
            " 99818/100000: episode: 10173, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000020, mae: 3.045418, mean_q: 4.505958\n",
            " 99826/100000: episode: 10174, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000013, mae: 2.915826, mean_q: 4.354060\n",
            " 99835/100000: episode: 10175, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.556 [0.000, 7.000],  loss: 0.000027, mae: 2.816089, mean_q: 4.244347\n",
            " 99845/100000: episode: 10176, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000018, mae: 3.026730, mean_q: 4.483228\n",
            " 99854/100000: episode: 10177, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.889 [0.000, 7.000],  loss: 0.000015, mae: 2.992219, mean_q: 4.446133\n",
            " 99866/100000: episode: 10178, duration: 0.140s, episode steps:  12, steps per second:  85, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.250 [0.000, 7.000],  loss: 0.000017, mae: 3.077695, mean_q: 4.542400\n",
            " 99874/100000: episode: 10179, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000008, mae: 2.833632, mean_q: 4.260962\n",
            " 99883/100000: episode: 10180, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000015, mae: 2.905289, mean_q: 4.348833\n",
            " 99891/100000: episode: 10181, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000011, mae: 2.851629, mean_q: 4.293935\n",
            " 99903/100000: episode: 10182, duration: 0.117s, episode steps:  12, steps per second: 103, episode reward:  4.000, mean reward:  0.333 [-1.000,  1.000], mean action: 3.583 [0.000, 7.000],  loss: 0.000057, mae: 2.701333, mean_q: 4.112964\n",
            " 99911/100000: episode: 10183, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000066, mae: 3.003727, mean_q: 4.457631\n",
            " 99919/100000: episode: 10184, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000048, mae: 3.068637, mean_q: 4.537796\n",
            " 99928/100000: episode: 10185, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.222 [0.000, 7.000],  loss: 0.000035, mae: 2.963558, mean_q: 4.400385\n",
            " 99938/100000: episode: 10186, duration: 0.096s, episode steps:  10, steps per second: 104, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 2.900 [0.000, 7.000],  loss: 0.000026, mae: 3.082594, mean_q: 4.545600\n",
            " 99946/100000: episode: 10187, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000032, mae: 3.106437, mean_q: 4.581642\n",
            " 99955/100000: episode: 10188, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.778 [0.000, 7.000],  loss: 0.000029, mae: 2.914913, mean_q: 4.363180\n",
            " 99963/100000: episode: 10189, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000047, mae: 2.892407, mean_q: 4.330034\n",
            " 99973/100000: episode: 10190, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward:  6.000, mean reward:  0.600 [-1.000,  1.000], mean action: 3.700 [0.000, 7.000],  loss: 0.000029, mae: 2.926999, mean_q: 4.367795\n",
            " 99981/100000: episode: 10191, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000010, mae: 3.035159, mean_q: 4.498542\n",
            " 99989/100000: episode: 10192, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 3.500 [0.000, 7.000],  loss: 0.000014, mae: 2.919843, mean_q: 4.364588\n",
            " 99998/100000: episode: 10193, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  7.000, mean reward:  0.778 [-1.000,  1.000], mean action: 3.111 [0.000, 7.000],  loss: 0.000033, mae: 2.876574, mean_q: 4.306585\n",
            "done, took 1124.166 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play File"
      ],
      "metadata": {
        "id": "bQolIDUMbMMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from claim_processor_env import ClaimProcessorEnv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Define the steps in the health insurance claims process\n",
        "steps = [\n",
        "    \"Patient Verification\",\n",
        "    \"Consultation\",\n",
        "    \"Test Results\",\n",
        "    \"Medicine Dispensation\",\n",
        "    \"Claim Pre-Authorization\",\n",
        "    \"Claims Submission\",\n",
        "    \"Review\",\n",
        "    \"Approval\"\n",
        "]\n",
        "\n",
        "# Create environment\n",
        "env = ClaimProcessorEnv()\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Build a simple model with the same architecture used during training\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(24))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(24))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# Configure the agent\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "\n",
        "# Load the trained policy weights\n",
        "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "dqn.load_weights('dqn_claim_processor_weights.h5f')\n",
        "\n",
        "# Run multiple episodes\n",
        "num_episodes = 5\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(f\"\\nEpisode {episode + 1}\")\n",
        "    while not done:\n",
        "        action = dqn.forward(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        state_name = steps[state] if state < len(steps) else \"Unknown\"\n",
        "        print(f\"State: {state_name}, Reward: {reward}, Done: {done}\")\n",
        "        env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0bkfCVKN6ZB",
        "outputId": "54dad2b2-d5dc-4e4f-b0c5-497ca2cf1ac9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Episode 1\n",
            "State: Consultation, Reward: 1, Done: False\n",
            "State: Test Results, Reward: 1, Done: False\n",
            "State: Medicine Dispensation, Reward: 1, Done: False\n",
            "State: Claim Pre-Authorization, Reward: 1, Done: False\n",
            "State: Claims Submission, Reward: 1, Done: False\n",
            "State: Review, Reward: 1, Done: False\n",
            "State: Approval, Reward: 1, Done: False\n",
            "State: Unknown, Reward: 1, Done: True\n",
            "\n",
            "Episode 2\n",
            "State: Consultation, Reward: 1, Done: False\n",
            "State: Test Results, Reward: 1, Done: False\n",
            "State: Medicine Dispensation, Reward: 1, Done: False\n",
            "State: Claim Pre-Authorization, Reward: 1, Done: False\n",
            "State: Claims Submission, Reward: 1, Done: False\n",
            "State: Review, Reward: 1, Done: False\n",
            "State: Approval, Reward: 1, Done: False\n",
            "State: Unknown, Reward: 1, Done: True\n",
            "\n",
            "Episode 3\n",
            "State: Consultation, Reward: 1, Done: False\n",
            "State: Test Results, Reward: 1, Done: False\n",
            "State: Medicine Dispensation, Reward: 1, Done: False\n",
            "State: Claim Pre-Authorization, Reward: 1, Done: False\n",
            "State: Claims Submission, Reward: 1, Done: False\n",
            "State: Review, Reward: 1, Done: False\n",
            "State: Approval, Reward: 1, Done: False\n",
            "State: Unknown, Reward: 1, Done: True\n",
            "\n",
            "Episode 4\n",
            "State: Consultation, Reward: 1, Done: False\n",
            "State: Test Results, Reward: 1, Done: False\n",
            "State: Medicine Dispensation, Reward: 1, Done: False\n",
            "State: Claim Pre-Authorization, Reward: 1, Done: False\n",
            "State: Claims Submission, Reward: 1, Done: False\n",
            "State: Review, Reward: 1, Done: False\n",
            "State: Approval, Reward: 1, Done: False\n",
            "State: Unknown, Reward: 1, Done: True\n",
            "\n",
            "Episode 5\n",
            "State: Consultation, Reward: 1, Done: False\n",
            "State: Test Results, Reward: 1, Done: False\n",
            "State: Medicine Dispensation, Reward: 1, Done: False\n",
            "State: Claim Pre-Authorization, Reward: 1, Done: False\n",
            "State: Claims Submission, Reward: 1, Done: False\n",
            "State: Review, Reward: 1, Done: False\n",
            "State: Approval, Reward: 1, Done: False\n",
            "State: Unknown, Reward: 1, Done: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize file"
      ],
      "metadata": {
        "id": "5QxzXOS6c2VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import gym\n",
        "from claim_processor_env import ClaimProcessorEnv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Initialize Pygame\n",
        "pygame.init()\n",
        "\n",
        "# Create environment\n",
        "env = ClaimProcessorEnv()\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Build a simple model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# Configure the agent\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "\n",
        "# Load the trained policy weights\n",
        "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "dqn.load_weights('dqn_claim_processor_weights.h5f')\n",
        "\n",
        "# Pygame settings\n",
        "screen_width, screen_height = 500, 500\n",
        "screen = pygame.display.set_mode((screen_width, screen_height))\n",
        "pygame.display.set_caption(\"Health Insurance Claims Processor\")\n",
        "\n",
        "# Define colors\n",
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "RED = (255, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "BLUE = (0, 0, 255)\n",
        "\n",
        "# Define grid parameters\n",
        "grid_size = 8\n",
        "cell_size = screen_width // grid_size\n",
        "\n",
        "# Define steps names\n",
        "steps = [\"Patient Verification\", \"Consultation\", \"Test Results\", \"Medicine Dispensation\",\n",
        "         \"Claim Pre-Authorization\", \"Claims Submission\", \"Review\", \"Approval\"]\n",
        "\n",
        "# Main visualization loop\n",
        "running = True\n",
        "state = env.reset()\n",
        "done = False\n",
        "clock = pygame.time.Clock()\n",
        "\n",
        "while running and not done:\n",
        "    for event in pygame.event.get():\n",
        "        if event.type == pygame.QUIT:\n",
        "            running = False\n",
        "\n",
        "    action = dqn.forward(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "    screen.fill(WHITE)\n",
        "\n",
        "    # Draw grid\n",
        "    for x in range(0, screen_width, cell_size):\n",
        "        for y in range(0, screen_height, cell_size):\n",
        "            rect = pygame.Rect(x, y, cell_size, cell_size)\n",
        "            pygame.draw.rect(screen, BLACK, rect, 1)\n",
        "\n",
        "    # Highlight current state\n",
        "    x = (state % grid_size) * cell_size\n",
        "    y = (state // grid_size) * cell_size\n",
        "    pygame.draw.rect(screen, BLUE, (x, y, cell_size, cell_size))\n",
        "\n",
        "    # Display step names\n",
        "    for i, step_name in enumerate(steps):\n",
        "        font = pygame.font.Font(None, 36)\n",
        "        text = font.render(step_name, True, BLACK)\n",
        "        text_rect = text.get_rect(center=((i % grid_size) * cell_size + cell_size // 2,\n",
        "                                           (i // grid_size) * cell_size + cell_size // 2))\n",
        "        screen.blit(text, text_rect)\n",
        "\n",
        "    # Display reward\n",
        "    reward_text = font.render(f\"Reward: {reward}\", True, RED if reward < 0 else GREEN)\n",
        "    screen.blit(reward_text, (10, screen_height - 40))\n",
        "\n",
        "    pygame.display.flip()\n",
        "    clock.tick(5)  # Limit to 5 frames per second\n",
        "\n",
        "pygame.quit()\n"
      ],
      "metadata": {
        "id": "R4RL4FlVc027"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}